The Third Version of the [Generation, Evaluation & Metrics (GEM) Workshop](https://gem-benchmark.com/) will be held as part of [EMNLP](https://2023.emnlp.org/), December 6-10, 2023. 

### Overview
Many new NLP applications are cast through the lens of natural language generation. With the advent of these new approaches, many opportunities arise: generation in previously less studied languages, new evaluation paradigms, methods for corpus creation, more efficient architectures, strategies for safe deployments, among many others. At the same time, we can learn from the rich history of NLG research to further improve generation methods.
These developments require robust and sound NLG evaluation processes. To that end, the GEM workshop aims to encourage the development of model auditing and human evaluation strategies, and to popularize model evaluations in languages beyond English.

We welcome submissions related, but not limited to, the following topics:

- ðŸ’Ž Automatic evaluation of generation systems ([example](https://aclanthology.org/2021.gem-1.8/), [example](https://aclanthology.org/2021.gem-1.1/), [example](https://aclanthology.org/2022.gem-1.26/))
- ðŸ’Ž Creating NLG corpora and challenge sets ([example](https://aclanthology.org/2022.tacl-1.4/), [example](https://openreview.net/forum?id=CSi1eu_2q96), [example])https://aclanthology.org/2022.gem-1.6/))
- ðŸ’Ž Critiques of benchmarking efforts and responsibly measuring progress in NLG ([example](https://aclanthology.org/2020.emnlp-main.393/), [example](https://openreview.net/forum?id=j6NxpQbREA1))
- ðŸ’Ž Effective and/or efficient NLG methods that can be applied to a wide range of languages and/or scenarios ([example](https://aclanthology.org/2020.tacl-1.47/), [example](https://aclanthology.org/2021.gem-1.16/), [example](https://aclanthology.org/2022.gem-1.1/))
- ðŸ’Ž Application and evaluation of generation models interacting with external data and tools ([example](https://arxiv.org/abs/2302.04761), [example](https://arxiv.org/abs/2304.09842), [example](https://arxiv.org/abs/2302.07842))
- ðŸ’Ž Sociotechnical perspectives of employing large language models ([example](https://dl.acm.org/doi/abs/10.1145/3531146.3533088))
- ðŸ’Ž Standardizing human evaluation and making it more robust ([example](https://aclanthology.org/2021.tacl-1.87/), [example](https://aclanthology.org/2022.humeval-1.7/), [example](https://aclanthology.org/2022.gem-1.12/))

We further invite submissions that conduct in-depth analyses of outputs of existing systems, for example through error analyses, by applying new metrics, or by testing the system on new test sets. While we encourage the use of the infrastructure the organizing team has developed as part of the [GEM benchmark](https://arxiv.org/abs/2206.11249), its use is not required.

If you are interested, you can check out last year's workshop websites from [ACL 2021](/workshop/2021) and [EMNLP 2022](/workshop/2022).


### Industrial Track - Unleashing the Power of NLP: Bridging the Gap between Academia and Industry
GEM 2023 is proud to announce the launch of its Industrial Track, which aims to provide actionable insights to industry professionals and to foster collaborations between academia and industry. This track will address the unique challenges faced by non-academic colleagues, highlighting the differences in evaluation practices between academic and industrial research, and explore the challenges in evaluating generative models with real-world data.
The Industrial Track invites submissions covering the following topics, including (but not limited to):
ðŸ’Ž Breaking Barriers: Bridging the Gap between Academic and Industrial Research ([example](https://aclanthology.org/P17-2015))
ðŸ’Ž From Data Diversity to Model Robustness: Challenges in Evaluating Generative Models with Real-World Data ([example](https://aclanthology.org/2021.sigdial-1.8/))
ðŸ’Ž Beyond Metrics: Evaluating Generative Models for Real-World Business Impact ([example](https://arxiv.org/abs/1906.02243), [example](https://aclanthology.org/P16-2096), [example](https://arxiv.org/abs/2306.07402))


### How to submit?
Submissions can take either of the following forms:
- ðŸ’Ž Archival Papers Papers describing original and unpublished work can be submitted in a between 4 and 8 page format.
- ðŸ’Ž Non-Archival Abstracts To discuss work already presented or under review at a peer-reviewed venue, we allow the submission of 2-page abstracts.

All submissions are allowed unlimited space for references and appendices and should conform to EMNLP 2023 style guidelines. Archival paper submissions must be anonymized while abstract submissions may include author information.

You can submit directly through [SoftConf](https://softconf.com/emnlp2023/GEM2023). Please select the track you are submitting to during the submission. 

We additionally welcome presentations by authors of papers in the Findings of the EMNLP (details to be announced at a later date).

### Shared Task
We are organizing a shared task focused on multilingual summarization, including human and automatic evaluation. The Shared Task will be run "Backwards": the workshop will serve as a platform to pre-register your hypotheses. More info on how to participate to come!


### Important Dates

Note: For any questions, please email gem-benchmark-chairs@googlegroups.com.

Paper Submission Dates
- ðŸ“… 1 September 2023: Workshop paper submission deadline 
- ðŸ“… 6 October 2023:   Workshop paper notification deadline
- ðŸ“… 18 October 2023:  Workshop paper camera ready deadline

Workshop Dates
- ðŸ“… 6 December 2022: Workshop


### Organization

*Contact*:
gem-benchmark-chairs@googlegroups.com 

*General Chairs*

Khyathi Raghavi Chandu (AI2)

Elizabeth Clark (Google Deepmind)

Kaustubh Dhole (Emory University)

Sebastian Gehrmann (Bloomberg)

JoÃ£o Sedoc (NYU)

Alex Wang (Cohere)

*Industry Track Chairs*

Enrico Santus (Bloomberg)

Hooman Sedghamiz (Bayer)
