<!DOCTYPE html>
<html lang="en-US">
 <head>
  <meta content="width=device-width, initial-scale=1" name="viewport"/>
  <meta charset="utf-8"/>
  <title>
   Data Cards Labs
  </title>
  <base target="_blank"/>
  <link href="https://fonts.googleapis.com" rel="preconnect"/>
  <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500&amp;family=Rubik:wght@400;500&amp;family=Space+Grotesk:wght@400;500&amp;display=swap" rel="stylesheet"/>
  <style>
   a,abbr,acronym,address,applet,article,aside,audio,big,blockquote,body,canvas,caption,center,cite,code,dd,del,details,dfn,div,dl,dt,em,embed,fieldset,figcaption,figure,footer,form,h1,h2,h3,h4,h5,h6,header,hgroup,html,iframe,img,ins,kbd,label,legend,li,mark,menu,nav,object,ol,output,p,pre,q,ruby,s,samp,section,small,span,strike,strong,sub,summary,sup,table,tbody,td,tfoot,th,thead,time,tr,tt,u,ul,var,video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}*{-webkit-box-sizing:border-box;box-sizing:border-box}html{overflow:hidden}body{font-family:Inter, Helvetica, Arial, sans-serif;font-weight:400;font-size:16px;line-height:24px;color:#3c4f50;background:#f1f7f7;overflow-x:hidden}a{color:#ff675c;text-decoration:none}a:focus,a:hover{background:#ffecea}b,strong{font-weight:600}em{font-style:italic}img{max-width:100%}h1{font-family:Rubik, Helvetica, Arial, sans-serif;font-size:32px;line-height:48px;font-weight:400;margin-bottom:12px}h2{font-family:Rubik, Helvetica, Arial, sans-serif;font-size:18px;line-height:28px;display:block;margin-right:20px}h3{font-family:Rubik, Helvetica, Arial, sans-serif;margin-top:20px;padding-top:10px;font-size:16px;line-height:24px;margin-bottom:5px;color:#ff675c;border-top:1px solid #cddcdc}dt,h4{color:#7c9192;font-weight:500;font-size:12px;line-height:20px;text-transform:uppercase;letter-spacing:0.8px;margin-bottom:10px}h5{margin-top:10px;font-size:12px;font-weight:500;font-style:italic;color:#7c9192}p{margin-bottom:20px}p:last-of-type{margin-bottom:0}ol,ul{margin-left:20px;margin-bottom:20px}.header-wrapper{position:fixed;display:flex;align-items:center;justify-content:start;padding:0 60px;box-shadow:1px 2px 2px 0 rgba(0, 0, 0, 0.2);width:100%;background:#fff;font-family:Rubik, Helvetica, Arial, sans-serif;font-size:18px;line-height:20px;height:60px;z-index:3}.header-title{display:inline-block;font-family:Rubik, Helvetica, Arial, sans-serif;font-size:18px;line-height:20px}.main-wrapper{display:flex;justify-content:space-between}.panel-wrapper{min-width:60px;max-width:60px;min-height:100%;margin-top:60px;padding-top:20px;background:#fff;border-right:1px solid #cddcdc;box-shadow:1px 1px 1px 0 rgba(0, 0, 0, 0.2);z-index:2;display:flex;flex-direction:column;align-items:center}#datacard-outline-nav{position:fixed;left:-300px;min-width:300px;max-width:300px;height:100%;padding:120px 10px 20px 0;box-shadow:1px 1px 1px 0 rgba(0, 0, 0, 0.2);background:#f1f7f7;z-index:1;overflow-y:scroll;transition:all 0.5s ease-out}#datacard-outline-nav.active{transform:translateX(360px);transition:all 0.5s ease-out}#datacard-outline-nav ul{list-style-type:none;font-size:14px;line-height:22px;overflow-wrap:normal;margin-bottom:6px}#datacard-outline-nav li{margin-bottom:6px}#datacard-outline-nav li:focus,#datacard-outline-nav li:hover{cursor:pointer;color:#ff675c}#datacard-outline-nav li.active{color:#ff675c;list-style-type:disc}#datacard-outline-button{padding:10px;width:40px;height:40px;border-radius:20px;background:#f1f7f7;border:none;cursor:pointer}#datacard-outline-button svg{fill:#3c4f50}#datacard-outline-button.active,#datacard-outline-button:focus,#datacard-outline-button:hover{background:#cddcdc}.datacard-breadcrumbs{display:flex;align-items:center;height:40px;width:100%;position:fixed;padding-left:80px;top:60px;left:0;background:#fff;border-bottom:1px solid #cddcdc;font-size:14px;line-height:22px;z-index:1}.content-wrapper{width:100%;display:flex;justify-content:center;margin-top:60px;height:calc(100vh - 40px);overflow-x:hidden;overflow-y:scroll;scroll-behavior:smooth}.datacard-wrapper{max-width:1000px;width:100%;padding:20px}section{display:block;margin-bottom:40px}.quick-row,.section,.summary{background:#fff;display:block;width:100%;padding:20px;border:1px solid #cddcdc}.quick-row,.summary{border-radius:5px}.title-dataset-details,.title-quick-use{font-size:20px;line-height:28px;margin-top:10px;margin-bottom:0;padding-bottom:10px}.details-nav{display:flex;justify-content:space-between}.details-button{font-family:Inter, Helvetica, Arial, sans-serif;color:#152d2e;background:#fff;border:none;border-radius:5px;width:max-content;height:30px;font-size:12px;line-height:20px;padding:4px 8px;margin-left:10px;cursor:pointer;display:flex;justify-content:center;align-items:center}.details-button:focus,.details-button:hover{background:#cddcdc}.tags{display:none;margin-right:40px}.tag{font-family:Rubik, Helvetica, Arial, sans-serif;color:#3c4f50;background-color:#f1f7f7;font-size:14px;line-height:22px;padding:1px 5px;border-radius:5px;margin:2px 5px 2px 0;display:inline-block;cursor:pointer}.tag:focus,.tag:hover{background-color:#cddcdc}.tag:last-of-type{margin-right:0}@media screen and (min-width: 800px){.tags{max-width:500px;margin-left:20px}}@media screen and (min-width: 550px){.tags{display:inline-block}}.section{border-bottom:2px solid #cddcdc}.section:first-of-type,.section:nth-of-type(2){border-radius:5px 5px 0 0}.section:last-of-type{border-radius:0 0 5px 5px;border-bottom:1px solid #cddcdc}.section-preview{width:100%;display:flex;justify-content:space-between;align-items:center;min-height:60px}.section-preview-content{display:flex;justify-content:center;align-items:center}.section-open-button{min-width:24px;min-height:24px;height:100%;background:none;border:none;cursor:pointer;z-index:0}.section-open-button svg{transition:all 0.5s ease-out}.section-open-button:focus svg,.section-open-button:hover svg{fill:#ff675c}.subsection-wrapper{max-height:0;overflow:hidden;transition:all 0.25s ease-out;-webkit-transition:none;-moz-transition:all 0.25s ease-out;-o-transition:all 0.25s ease-out;webkit-backface-visibility:none}.open .subsection-wrapper{max-height:max-content;transition:all 0.5s ease-in;-webkit-transition:none;-moz-transition:all 0.5s ease-in;-o-transition:all 0.5s ease-in}.open .section-open-button svg{transform:rotate(180deg);transition:all 0.5s ease-out;-webkit-transition:all 0.5s ease-out;-moz-transition:all 0.5s ease-out;-o-transition:all 0.5s ease-out;webkit-backface-visibility:none}.summary-content{margin-bottom:20px}.summary-scopes{border-top:1px solid #f1f7f7;padding-top:20px}@media screen and (min-width: 550px){.summary-wrapper{display:grid;grid-template-columns:repeat(2, 1fr)}.summary-scopes{padding:0 20px;border:none}}@media screen and (min-width: 800px){.summary-scopes{display:grid;grid-template-columns:repeat(2, 1fr);gap:20px}}.row{margin-top:10px;display:block;width:100%;gap:20px}@media screen and (min-width: 550px){.row{display:grid;grid-template-columns:repeat(3, 1fr)}}.quick-row{display:block;width:100%;column-gap:20px;grid-template-columns:repeat(2, 1fr)}@media screen and (min-width: 550px){.quick-row{display:grid}}@media screen and (min-width: 800px){.quick-row{grid-template-columns:repeat(5, 1fr)}}.field{margin-bottom:20px;overflow-x:hidden}.field:last-of-type{margin-bottom:0}@media screen and (min-width: 550px){.field{max-width:300px}}.scope{padding:10px 0}.scope:last-of-type{border-bottom:none}@media screen and (min-width: 550px){.scope{border-bottom:none}}pre{background:#f1f7f7;display:inline-block;overflow-x:hidden;overflow-y:scroll;width:100%;max-height:300px;white-space:pre-wrap}code{font-family:'Space Grotesk', sans-serif;background:#f1f7f7;font-size:14px;line-height:20px;border-radius:6px;padding:1px 5px}.code-block-wrapper,.table-wrapper{background:#fff;border:#cddcdc 1px solid;border-radius:6px;padding:10px;margin:10px 0}.expand .code-block-wrapper,.expand .table-wrapper{box-shadow:1px 2px 2px 0 rgba(0, 0, 0, 0.2)}.code-block-wrapper{background:#f1f7f7}.summary-row .code-block-wrapper,.summary-row .table-wrapper{max-width:max-content}table{display:inline-block;overflow-x:scroll;overflow-y:hidden;max-height:300px;font-size:14px;line-height:20px}tr{overflow-x:scroll}tfoot,thead,tr{padding:10px}th{text-align:left;font-weight:500;padding:10px}td,tfoot{text-align:left;padding:10px;border-top:#f1f7f7 1px solid}thead{font-weight:500}.expand{position:fixed;padding:20px;margin:0;border-radius:0;top:60px;left:0;width:100%;max-width:100%;height:100%;max-height:100%;background:rgba(0, 0, 0, .4);display:flex;justify-content:center;align-items:center;flex-direction:column;z-index:3}.expand pre,.expand table{height:max-content;max-height:calc(100vh - 100px)}.expand .interactions{margin-bottom:10px}.interactions{display:flex;justify-content:end;align-items:center}.tooltip{margin-left:2px;width:0;height:0;border:none;background:none;display:inline}.tooltip::before{content:url("data:image/svg+xml, <svg xmlns='http://www.w3.org/2000/svg' height='16px' width='16px' fill='rgb(124, 145, 146)' viewBox='0 0 24 24'><path d='M11,7h2v2h-2V7z M11,11h2v6h-2V11z M12,2C6.48,2,2,6.48,2,12s4.48,10,10,10s10-4.48,10-10S17.52,2,12,2z M12,20 c-4.41,0-8-3.59-8-8s3.59-8,8-8s8,3.59,8,8S16.41,20,12,20z'/></svg>")}.tooltip-text{font-family:Inter, Helvetica, Arial, sans-serif;font-weight:400;font-size:12px;line-height:16px;padding:6px;text-transform:none;visibility:hidden;position:absolute;height:0;width:0;color:#fff;background:#3c4f50;border:none;border-radius:5px;box-shadow:1px 2px 2px 0 rgba(0,0,0,.2)}.tooltip:focus .tooltip-text,.tooltip:hover .tooltip-text{visibility:visible;height:auto;width:auto;max-width:240px;word-break:break-word}.expand-icon{content:url("data:image/svg+xml, <svg xmlns='http://www.w3.org/2000/svg' height='16px' width='16px' fill='rgb(124, 145, 146)' viewBox='0 0 24 24'><path d='M21,11 21,3 13,3 16.29,6.29 6.29,16.29 3,13 3,21 11,21 7.71,17.71 17.71,7.71z'/></svg>");border:none;background:none;cursor:pointer;margin:0;padding:2px}.collapse-icon{content:url("data:image/svg+xml, <svg xmlns='http://www.w3.org/2000/svg' height='16px' width='16px' fill='rgb(124, 145, 146)' viewBox='0 0 24 24'><path d='M22,3.41l-5.29,5.29L20,12h-8V4l3.29,3.29L20.59,2L22,3.41z M3.41,22l5.29-5.29L12,20v-8H4l3.29,3.29L2,20.59L3.41,22z'/></svg>");border:none;background:none;cursor:pointer;margin:0;padding:2px}.copy-icon{content:url("data:image/svg+xml, <svg xmlns='http://www.w3.org/2000/svg' height='16px' width='16px' fill='rgb(124, 145, 146)' viewBox='0 0 24 24'><path d='M16 1H4c-1.1 0-2 .9-2 2v14h2V3h12V1zm3 4H8c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h11c1.1 0 2-.9 2-2V7c0-1.1-.9-2-2-2zm0 16H8V7h11v14z'/></svg>");border:none;background:none;cursor:pointer;margin:0;padding:2px}.link-icon{content:url("data:image/svg+xml, <svg xmlns='http://www.w3.org/2000/svg' height='16px' width='16px' fill='rgb(124, 145, 146)' viewBox='0 0 24 24'><path d='M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z'/></svg>");border:none;background:none;cursor:pointer;margin:0;padding:2px}
  </style>
 </head>
 <body>
  <header class="header-wrapper">
   <div class="header-title">
    Data Cards Labs
   </div>
  </header>
  <main class="main-wrapper">
   <aside class="panel-wrapper">
    <button id="datacard-outline-button">
     <svg fill="none" height="16" viewbox="0 0 18 18" width="16" xmlns="http://www.w3.org/2000/svg">
      <path d="M16 2V16H2V2H16ZM16 0H2C0.9 0 0 0.9 0 2V16C0 17.1 0.9 18 2 18H16C17.1 18 18 17.1 18 16V2C18 0.9 17.1 0 16 0ZM11 14H4V12H11V14ZM14 10H4V8H14V10ZM14 6H4V4H14V6Z">
      </path>
     </svg>
    </button>
   </aside>
   <nav id="datacard-outline-nav">
    <ul>
     <li>
      Summary
     </li>
     <li>
      Quick-Use
     </li>
     <li>
      Dataset Details
     </li>
     <ul>
      <li>
       Dataset Overview
      </li>
      <ul>
       <li>
        Where to find the Data and its Documentation
       </li>
       <li>
        Languages and Intended Use
       </li>
       <li>
        Credit
       </li>
       <li>
        Dataset Structure
       </li>
      </ul>
      <li>
       Dataset in GEM
      </li>
      <ul>
       <li>
        Rationale for Inclusion in GEM
       </li>
       <li>
        GEM-Specific Curation
       </li>
       <li>
        Getting Started with the Task
       </li>
      </ul>
      <li>
       Previous Results
      </li>
      <ul>
       <li>
        Previous Results
       </li>
      </ul>
      <li>
       Dataset Curation
      </li>
      <ul>
       <li>
        Original Curation
       </li>
       <li>
        Language Data
       </li>
       <li>
        Structured Annotations
       </li>
       <li>
        Consent
       </li>
       <li>
        Private Identifying Information (PII)
       </li>
       <li>
        Maintenance
       </li>
      </ul>
      <li>
       Broader Social Context
      </li>
      <ul>
       <li>
        Previous Work on the Social Impact of the Dataset
       </li>
       <li>
        Impact on Under-Served Communities
       </li>
       <li>
        Discussion of Biases
       </li>
      </ul>
      <li>
       Considerations for Using the Data
      </li>
      <ul>
       <li>
        PII Risks and Liability
       </li>
       <li>
        Licenses
       </li>
       <li>
        Known Technical Limitations
       </li>
      </ul>
     </ul>
    </ul>
   </nav>
   <div class="content-wrapper">
    <article class="datacard-wrapper">
     <template id="table-template">
      <div class="table-wrapper">
       <div class="interactions">
        <div class="expand-icon" title="Click to expand table">
        </div>
       </div>
      </div>
     </template>
     <template id="code-block-template">
      <div class="code-block-wrapper">
       <div class="interactions">
        <div class="copy-icon" title="Click to copy code block">
        </div>
        <div class="expand-icon" title="Click to expand table">
        </div>
       </div>
      </div>
     </template>
     <section class="summary" id="summary">
      <h1>
       opusparcus
      </h1>
      <div class="summary-wrapper">
       <div class="summary-content">
        <p>
         Opusparcus is a paraphrase corpus for six European languages: German, English, Finnish, French, Russian, and Swedish. The paraphrases consist of subtitles from movies and TV shows.
        </p>
        <p>
         You can load the dataset via:
        </p>
        <pre><code>import datasets
data = datasets.load_dataset('GEM/opusparcus')
</code></pre>
        <p>
         The data loader can be found
         <a href="https://huggingface.co/datasets/GEM/opusparcus">
          here
         </a>
         .
        </p>
       </div>
       <div class="summary-scopes">
        <div class="field">
         <h4>
          website
         </h4>
         <p>
          <a href="http://urn.fi/urn:nbn:fi:lb-2018021221">
           Website
          </a>
         </p>
        </div>
        <div class="field">
         <h4>
          paper
         </h4>
         <p>
          <a href="http://www.lrec-conf.org/proceedings/lrec2018/pdf/131.pdf">
           LREC
          </a>
         </p>
        </div>
       </div>
      </div>
     </section>
     <section id="quick-use">
      <h2 class="title-quick-use">
       Quick-Use
      </h2>
      <div class="quick-row">
       <div class="field">
        <h4>
         Contact Name
         <div class="tooltip">
          <div class="tooltip-text">
           If known, provide the name of at least one person the reader can contact for questions about the dataset.
          </div>
         </div>
        </h4>
        <p>
         Mathias Creutz
        </p>
       </div>
       <div class="field">
        <h4>
         Multilingual?
         <div class="tooltip">
          <div class="tooltip-text">
           Is the dataset multilingual?
          </div>
         </div>
        </h4>
        <p>
         yes
        </p>
       </div>
       <div class="field">
        <h4>
         Covered Languages
         <div class="tooltip">
          <div class="tooltip-text">
           What languages/dialects are covered in the dataset?
          </div>
         </div>
        </h4>
        <p>
         <code>
          German
         </code>
         ,
         <code>
          English
         </code>
         ,
         <code>
          Finnish
         </code>
         ,
         <code>
          French
         </code>
         ,
         <code>
          Russian
         </code>
         ,
         <code>
          Swedish
         </code>
        </p>
       </div>
       <div class="field">
        <h4>
         License
         <div class="tooltip">
          <div class="tooltip-text">
           What is the license of the dataset?
          </div>
         </div>
        </h4>
        <p>
         cc-by-nc-4.0: Creative Commons Attribution Non Commercial 4.0 International
        </p>
       </div>
       <div class="field">
        <h4>
         Communicative Goal
         <div class="tooltip">
          <div class="tooltip-text">
           Provide a short description of the communicative goal of a model trained for this task on this dataset.
          </div>
         </div>
        </h4>
        <p>
         Models can be trained, e.g., for paraphrase detection and generation, that is, determining whether two given sentences mean the same thing or generating new paraphrases for a given sentence.
        </p>
       </div>
       <div class="field">
        <h4>
         Additional Annotations?
         <div class="tooltip">
          <div class="tooltip-text">
           Does the dataset have additional annotations for each instance?
          </div>
         </div>
        </h4>
        <p>
         expert created
        </p>
       </div>
       <div class="field">
        <h4>
         Contains PII?
         <div class="tooltip">
          <div class="tooltip-text">
           Does the source language data likely contain Personal Identifying Information about the data creators or subjects?
          </div>
         </div>
        </h4>
        <p>
         yes/very likely
        </p>
       </div>
      </div>
     </section>
     <section id="dataset-details">
      <div class="details-nav">
       <p class="title-dataset-details">
        Dataset Details
       </p>
       <div class="interactions">
        <button class="details-button" id="expand-all">
         Expand All
         <svg height="16" viewbox="0 0 48 48" width="16" xmlns="http://www.w3.org/2000/svg">
          <path d="M17.2 17.4 15 15.2 24 6.2 33 15.2 30.8 17.4 24 10.6ZM24 42 15 33 17.2 30.8 24 37.6 30.8 30.8 33 33Z">
          </path>
         </svg>
        </button>
        <button class="details-button" id="collapse-all">
         Collapse All
         <svg height="16" viewbox="0 0 48 48" width="16" xmlns="http://www.w3.org/2000/svg">
          <path d="M24 19.15 15 10.15 17.15 8 24 14.85 30.85 8 33 10.15ZM17.15 40 15 37.85 24 28.85 33 37.85 30.85 40 24 33.15Z">
          </path>
         </svg>
        </button>
       </div>
      </div>
      <div class="section">
       <div class="section-preview">
        <div class="section-preview-content">
         <h2 id="dataset-overview">
          Dataset Overview
         </h2>
         <div class="tags">
          <div class="tag">
           Where to find the Data and its Documentation
          </div>
          <div class="tag">
           Languages and Intended Use
          </div>
          <div class="tag">
           Credit
          </div>
          <div class="tag">
           Dataset Structure
          </div>
         </div>
        </div>
        <button class="section-open-button">
         <svg fill="#3c4f50" height="24px" viewbox="0 0 24 24" width="24px" xmlns="http://www.w3.org/2000/svg">
          <path d="M0 0h24v24H0z" fill="none">
          </path>
          <path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z">
          </path>
         </svg>
        </button>
       </div>
       <div class="subsection-wrapper">
        <h3 id="where-to-find-the-data-and-its-documentation">
         Where to find the Data and its Documentation
        </h3>
        <div class="row">
         <div class="scope">
          <div class="field">
           <h4>
            Webpage
            <div class="tooltip">
             <div class="tooltip-text">
              What is the webpage for the dataset (if it exists)?
             </div>
            </div>
           </h4>
           <p>
            <a href="http://urn.fi/urn:nbn:fi:lb-2018021221">
             Website
            </a>
           </p>
          </div>
          <div class="field">
           <h4>
            Download
            <div class="tooltip">
             <div class="tooltip-text">
              What is the link to where the original dataset is hosted?
             </div>
            </div>
           </h4>
           <p>
            <a href="http://urn.fi/urn:nbn:fi:lb-2018021221">
             Website
            </a>
           </p>
          </div>
          <div class="field">
           <h4>
            Paper
            <div class="tooltip">
             <div class="tooltip-text">
              What is the link to the paper describing the dataset (open access preferred)?
             </div>
            </div>
           </h4>
           <p>
            <a href="http://www.lrec-conf.org/proceedings/lrec2018/pdf/131.pdf">
             LREC
            </a>
           </p>
          </div>
          <div class="field">
           <h4>
            Has a Leaderboard?
            <div class="tooltip">
             <div class="tooltip-text">
              Does the dataset have an active leaderboard?
             </div>
            </div>
           </h4>
           <p>
            no
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Contact Name
            <div class="tooltip">
             <div class="tooltip-text">
              If known, provide the name of at least one person the reader can contact for questions about the dataset.
             </div>
            </div>
           </h4>
           <p>
            Mathias Creutz
           </p>
          </div>
          <div class="field">
           <h4>
            Contact Email
            <div class="tooltip">
             <div class="tooltip-text">
              If known, provide the email of at least one person the reader can contact for questions about the dataset.
             </div>
            </div>
           </h4>
           <p>
            firstname dot lastname at helsinki dot fi
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            BibTex
            <div class="tooltip">
             <div class="tooltip-text">
              Provide the BibTex-formatted reference for the dataset. Please use the correct published version (ACL anthology, etc.) instead of google scholar created Bibtex.
             </div>
            </div>
           </h4>
           <pre><code>@InProceedings{creutz:lrec2018,
  title = {Open Subtitles Paraphrase Corpus for Six Languages},
  author={Mathias Creutz},
  booktitle={Proceedings of the 11th edition of the Language Resources and Evaluation Conference (LREC 2018)},
  year={2018},
  month = {May 7-12},
  address = {Miyazaki, Japan},
  editor = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and Hélène Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {979-10-95546-00-9},
  language = {english},
  url={http://www.lrec-conf.org/proceedings/lrec2018/pdf/131.pdf}
</code></pre>
          </div>
         </div>
        </div>
        <h3 id="languages-and-intended-use">
         Languages and Intended Use
        </h3>
        <div class="row">
         <div class="scope">
          <div class="field">
           <h4>
            Multilingual?
            <div class="tooltip">
             <div class="tooltip-text">
              Is the dataset multilingual?
             </div>
            </div>
           </h4>
           <p>
            yes
           </p>
          </div>
          <div class="field">
           <h4>
            Covered Languages
            <div class="tooltip">
             <div class="tooltip-text">
              What languages/dialects are covered in the dataset?
             </div>
            </div>
           </h4>
           <p>
            <code>
             German
            </code>
            ,
            <code>
             English
            </code>
            ,
            <code>
             Finnish
            </code>
            ,
            <code>
             French
            </code>
            ,
            <code>
             Russian
            </code>
            ,
            <code>
             Swedish
            </code>
           </p>
          </div>
          <div class="field">
           <h4>
            License
            <div class="tooltip">
             <div class="tooltip-text">
              What is the license of the dataset?
             </div>
            </div>
           </h4>
           <p>
            cc-by-nc-4.0: Creative Commons Attribution Non Commercial 4.0 International
           </p>
          </div>
          <div class="field">
           <h4>
            Primary Task
            <div class="tooltip">
             <div class="tooltip-text">
              What primary task does the dataset support?
             </div>
            </div>
           </h4>
           <p>
            Paraphrasing
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Whose Language?
            <div class="tooltip">
             <div class="tooltip-text">
              Whose language is in the dataset?
             </div>
            </div>
           </h4>
           <p>
            Opusparcus is a paraphrase corpus for six European languages: German, English, Finnish, French, Russian, and Swedish. The paraphrases consist of subtitles from movies and TV shows.
           </p>
           <p>
            The data in Opusparcus has been extracted from
            <a href="http://opus.nlpl.eu/OpenSubtitles2016.php">
             OpenSubtitles2016
            </a>
            , which is in turn based on data from http://www.opensubtitles.org/.
           </p>
          </div>
          <div class="field">
           <h4>
            Communicative Goal
            <div class="tooltip">
             <div class="tooltip-text">
              Provide a short description of the communicative goal of a model trained for this task on this dataset.
             </div>
            </div>
           </h4>
           <p>
            Models can be trained, e.g., for paraphrase detection and generation, that is, determining whether two given sentences mean the same thing or generating new paraphrases for a given sentence.
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Intended Use
            <div class="tooltip">
             <div class="tooltip-text">
              What is the intended use of the dataset?
             </div>
            </div>
           </h4>
           <p>
            Opusparcus is a sentential paraphrase corpus for multiple languages containing colloquial language.
           </p>
          </div>
         </div>
        </div>
        <h3 id="credit">
         Credit
        </h3>
        <div class="row">
         <div class="scope">
         </div>
         <div class="scope">
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Who added the Dataset to GEM?
            <div class="tooltip">
             <div class="tooltip-text">
              Who contributed to the data card and adding the dataset to GEM? List the people+affiliations involved in creating this data card and who helped integrate this dataset into GEM.
             </div>
            </div>
           </h4>
           <p>
            Mathias Creutz (University of Helsinki)
           </p>
          </div>
         </div>
        </div>
        <h3 id="dataset-structure">
         Dataset Structure
        </h3>
        <div class="row">
         <div class="scope">
          <div class="field">
           <h4>
            Data Fields
            <div class="tooltip">
             <div class="tooltip-text">
              List and describe the fields present in the dataset.
             </div>
            </div>
           </h4>
           <ul>
            <li>
             <code>
              sent1
             </code>
             : a tokenized sentence
            </li>
            <li>
             <code>
              sent2
             </code>
             : another tokenized sentence, which is potentially a paraphrase of
             <code>
              sent1
             </code>
             .
            </li>
            <li>
             <code>
              annot_score
             </code>
             : a value between 1.0 and 4.0 indicating how good an example of paraphrases
             <code>
              sent1
             </code>
             and
             <code>
              sent2
             </code>
             are. (For the training sets, the value is 0.0, which indicates that no manual annotation has taken place.)
            </li>
            <li>
             <code>
              lang
             </code>
             : language of this dataset
            </li>
            <li>
             <code>
              gem_id
             </code>
             : unique identifier of this entry
            </li>
           </ul>
           <p>
            All fields are strings except
            <code>
             annot_score
            </code>
            , which is a float.
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Example Instance
            <div class="tooltip">
             <div class="tooltip-text">
              Provide a JSON formatted example of a typical instance in the dataset.
             </div>
            </div>
           </h4>
           <pre><code>{'annot_score': 4.0, 'gem_id': 'gem-opusparcus-test-1587', 'lang': 'en', 'sent1': "I haven 't been contacted by anybody .", 'sent2': "Nobody 's contacted me ."}
</code></pre>
          </div>
          <div class="field">
           <h4>
            Data Splits
            <div class="tooltip">
             <div class="tooltip-text">
              Describe and name the splits in the dataset if there are more than one.
             </div>
            </div>
           </h4>
           <p>
            The data is split into training, validation and test sets. The validation and test sets come in two versions, the regular validation and test sets and the full sets, called validation.full and test.full. The full sets contain all sentence pairs successfully annotated by the annotators, including the sentence pairs that were rejected as paraphrases. The annotation scores of the full sets thus range between 1.0 and 4.0. The regular validation and test sets only contain sentence pairs that qualify as paraphrases, scored between 3.0 and 4.0 by the annotators.
           </p>
           <p>
            The number of sentence pairs in the data splits are as follows for each of the languages. The range between the smallest (
            <code>
             quality=95
            </code>
            ) and largest (
            <code>
             quality=60
            </code>
            ) train configuration have been shown.
           </p>
           <table>
            <thead>
             <tr>
              <th>
              </th>
              <th>
               train
              </th>
              <th>
               valid
              </th>
              <th>
               test
              </th>
              <th>
               valid.full
              </th>
              <th>
               test.full
              </th>
             </tr>
            </thead>
            <tbody>
             <tr>
              <td>
               de
              </td>
              <td>
               0.59M .. 13M
              </td>
              <td>
               1013
              </td>
              <td>
               1047
              </td>
              <td>
               1582
              </td>
              <td>
               1586
              </td>
             </tr>
             <tr>
              <td>
               en
              </td>
              <td>
               1.0M .. 35M
              </td>
              <td>
               1015
              </td>
              <td>
               982
              </td>
              <td>
               1455
              </td>
              <td>
               1445
              </td>
             </tr>
             <tr>
              <td>
               fi
              </td>
              <td>
               0.48M .. 8.9M
              </td>
              <td>
               963
              </td>
              <td>
               958
              </td>
              <td>
               1760
              </td>
              <td>
               1749
              </td>
             </tr>
             <tr>
              <td>
               fr
              </td>
              <td>
               0.94M .. 22M
              </td>
              <td>
               997
              </td>
              <td>
               1007
              </td>
              <td>
               1630
              </td>
              <td>
               1674
              </td>
             </tr>
             <tr>
              <td>
               ru
              </td>
              <td>
               0.15M .. 15M
              </td>
              <td>
               1020
              </td>
              <td>
               1068
              </td>
              <td>
               1854
              </td>
              <td>
               1855
              </td>
             </tr>
             <tr>
              <td>
               sv
              </td>
              <td>
               0.24M .. 4.5M
              </td>
              <td>
               984
              </td>
              <td>
               947
              </td>
              <td>
               1887
              </td>
              <td>
               1901
              </td>
             </tr>
            </tbody>
           </table>
           <p>
            As a concrete example, loading the English data requesting 95% quality of the train split produces the following:
           </p>
           <p>
            ```
           </p>
           <blockquote>
            <blockquote>
             <blockquote>
              <p>
               data = load_dataset("GEM/opusparcus", lang="en", quality=95)
              </p>
              <p>
               data
DatasetDict({
    test: Dataset({
        features: ['lang', 'sent1', 'sent2', 'annot_score', 'gem_id'],
        num_rows: 982
    })
    validation: Dataset({
        features: ['lang', 'sent1', 'sent2', 'annot_score', 'gem_id'],
        num_rows: 1015
    })
    test.full: Dataset({
        features: ['lang', 'sent1', 'sent2', 'annot_score', 'gem_id'],
        num_rows: 1445
    })
    validation.full: Dataset({
        features: ['lang', 'sent1', 'sent2', 'annot_score', 'gem_id'],
        num_rows: 1455
    })
    train: Dataset({
        features: ['lang', 'sent1', 'sent2', 'annot_score', 'gem_id'],
        num_rows: 1000000
    })
})
              </p>
              <p 4.0_="4.0," _annot_score_:="'annot_score':" _en_="'en'," _gem-opusparcus-test-1587_="'gem-opusparcus-test-1587'," _gem_id_:="'gem_id':" _i='"I' _lang_:="'lang':" _nobody='"Nobody' _s="'s" _sent1_:="'sent1':" _sent2_:="'sent2':" _t="'t" anybody="anybody" been="been" by="by" class='", "' contacted="contacted" haven="haven" me="me">
               data["test"][0]
              </p>
              <p 3.0_="3.0," _="?'," _annot_score_:="'annot_score':" _en_="'en'," _gem-opusparcus-validation-1586_="'gem-opusparcus-validation-1586'," _gem_id_:="'gem_id':" _i='"I' _lang_:="'lang':" _m="'m" _no="'No" _sent1_:="'sent1':" _sent2_:="'sent2':" anything="anything" class='"' not="not" okay="okay" promises="promises" promising="promising">
               data["validation"][2]
              </p>
              <p 0.0_="0.0," _="?'" _am="'Am" _annot_score_:="'annot_score':" _en_="'en'," _gem-opusparcus-train-12501001_="'gem-opusparcus-train-12501001'," _gem_id_:="'gem_id':" _lang_:="'lang':" _sent1_:="'sent1':" _sent2_:="'sent2':" beautiful="beautiful" i="I" pretty="pretty">
               data["train"][1000]
              </p>
             </blockquote>
            </blockquote>
           </blockquote>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Reason for Structure
            <div class="tooltip">
             <div class="tooltip-text">
              How was the dataset structure determined?
             </div>
            </div>
           </h4>
           <p>
            For each target language, the Opusparcus data have been partitioned into three types of data sets: training, validation and test sets. The training sets are large, consisting of millions of sentence pairs, and have been compiled automatically, with the help of probabilistic ranking functions. The development and test sets consist of sentence pairs that have been annotated manually; each set contains approximately 1000 sentence pairs that have been verified to be acceptable paraphrases by two independent annotators.
           </p>
           <p>
            When you download Opusparcus, you must always indicate the language you want to retrieve, for instance:
           </p>
           <pre><code>data = load_dataset("GEM/opusparcus", lang="de")
</code></pre>
           <p>
            The above command will download the validation and test sets for German. If additionally, you want to retrieve training data, you need to specify the level of quality you desire, such as "French, with 90% quality of the training data":
           </p>
           <pre><code>data = load_dataset("GEM/opusparcus", lang="fr", quality=90)
</code></pre>
           <p>
            The entries in the training sets have been ranked automatically by how likely they are paraphrases, best first, worst last. The quality parameter indicates the estimated proportion (in percent) of true
paraphrases in the training set. Allowed quality values range between 60 and 100, in increments of 5 (60, 65, 70, ..., 100). A value of 60 means that 60% of the sentence pairs in the training set are estimated to be true paraphrases (and the remaining 40% are not). A higher value produces a smaller but cleaner set. The smaller sets are subsets of the larger sets, such that the
            <code>
             quality=95
            </code>
            set is a subset of
            <code>
             quality=90
            </code>
            , which is a subset of
            <code>
             quality=85
            </code>
            , and so on.
           </p>
           <p>
            The default
            <code>
             quality
            </code>
            value, if omitted, is 100. This matches no training data at all, which can be convenient, if you are only interested in the validation and test sets, which are considerably
smaller, but manually annotated.
           </p>
           <p>
            Note that an alternative to typing the parameter values explicitly, you can use configuration names instead. The following commands are equivalent to the ones above:
           </p>
           <pre><code>data = load_dataset("GEM/opusparcus", "de.100")
data = load_dataset("GEM/opusparcus", "fr.90")
</code></pre>
          </div>
          <div class="field">
           <h4>
            How were labels chosen?
            <div class="tooltip">
             <div class="tooltip-text">
              How were the labels chosen?
             </div>
            </div>
           </h4>
           <p>
            Annotators have used the following scores to label sentence pairs in the test and validation sets:
           </p>
           <p>
            4: Good example of paraphrases (Dark green button in the annotation tool): The two sentences can be used in the same situation and essentially "mean the same thing".
           </p>
           <p>
            3: Mostly good example of paraphrases (Light green button in the annotation tool): It is acceptable to think that the two sentences refer to the same thing, although one sentence might be more specific
than the other one, or there are differences in style, such as polite form versus familiar form.
           </p>
           <p>
            2: Mostly bad example of paraphrases (Yellow button in the annotation tool): There is some connection between the sentences that explains why they occur together, but one would not really consider them to mean the same thing.
           </p>
           <p>
            1: Bad example of paraphrases (Red button in the annotation tool): There is no obvious connection. The sentences mean different things.
           </p>
           <p>
            If the two annotators fully agreed on the category, the value in the
            <code>
             annot_score
            </code>
            field is 4.0, 3.0, 2.0 or 1.0.  If the two annotators chose adjacent categories, the value in this field will be 3.5, 2.5 or
1.5.  For instance, a value of 2.5 means that one annotator gave a score of 3 ("mostly good"), indicating a possible paraphrase pair, whereas the other annotator scored this as a 2 ("mostly bad"), that is, unlikely to be a paraphrase pair.  If the annotators disagreed by more than one category, the sentence pair was discarded and won't show up in the datasets.
           </p>
           <p>
            The training sets were not annotated manually. This is indicated by
the value 0.0 in the
            <code>
             annot_score
            </code>
            field.
           </p>
           <p>
            For an assessment of of inter-annotator agreement, see Aulamo et al. (2019).
            <a href="http://ceur-ws.org/Vol-2364/3_paper.pdf">
             Annotation of subtitle paraphrases using a new web tool.
            </a>
            In
            <em>
             Proceedings of the
Digital Humanities in the Nordic Countries 4th Conference
            </em>
            , Copenhagen, Denmark.
           </p>
          </div>
          <div class="field">
           <h4>
            Splitting Criteria
            <div class="tooltip">
             <div class="tooltip-text">
              Describe any criteria for splitting the data, if used. If there are differences between the splits (e.g., if the training annotations are machine-generated and the dev and test ones are created by humans, or if different numbers of annotators contributed to each example), describe them here.
             </div>
            </div>
           </h4>
           <p>
            The validation and test sets have been annotated manually, but the training sets have been produced using automatic scoring and come in different size configurations depending on the desired quality level. (See above descriptions and examples for more details.)
           </p>
           <p>
            Please note that previous work suggests that a larger and noisier training set is better than a
smaller and clean set. See Sjöblom et al. (2018).
            <a href="http://noisy-text.github.io/2018/pdf/W-NUT20189.pdf">
             Paraphrase Detection on Noisy Subtitles in Six
Languages
            </a>
            . In
            <em>
             Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text
            </em>
            , and Vahtola et al. (2021).
            <a href="https://aclanthology.org/2021.wnut-1.32/">
             Coping with Noisy Training Data Labels in Paraphrase Detection
            </a>
            . In
            <em>
             Proceedings of the 7th Workshop on Noisy User-generated Text
            </em>
            .
           </p>
          </div>
         </div>
        </div>
       </div>
      </div>
      <div class="section">
       <div class="section-preview">
        <div class="section-preview-content">
         <h2 id="dataset-in-gem">
          Dataset in GEM
         </h2>
         <div class="tags">
          <div class="tag">
           Rationale for Inclusion in GEM
          </div>
          <div class="tag">
           GEM-Specific Curation
          </div>
          <div class="tag">
           Getting Started with the Task
          </div>
         </div>
        </div>
        <button class="section-open-button">
         <svg fill="#3c4f50" height="24px" viewbox="0 0 24 24" width="24px" xmlns="http://www.w3.org/2000/svg">
          <path d="M0 0h24v24H0z" fill="none">
          </path>
          <path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z">
          </path>
         </svg>
        </button>
       </div>
       <div class="subsection-wrapper">
        <h3 id="rationale-for-inclusion-in-gem">
         Rationale for Inclusion in GEM
        </h3>
        <div class="row">
         <div class="scope">
          <div class="field">
           <h4>
            Similar Datasets
            <div class="tooltip">
             <div class="tooltip-text">
              Do other datasets for the high level task exist?
             </div>
            </div>
           </h4>
           <p>
            yes
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Unique Language Coverage
            <div class="tooltip">
             <div class="tooltip-text">
              Does this dataset cover other languages than other datasets for the same task?
             </div>
            </div>
           </h4>
           <p>
            yes
           </p>
          </div>
          <div class="field">
           <h4>
            Ability that the Dataset measures
            <div class="tooltip">
             <div class="tooltip-text">
              What aspect of model ability can be measured with this dataset?
             </div>
            </div>
           </h4>
           <p>
            Sentence meaning
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Why is the Dataset in GEM?
            <div class="tooltip">
             <div class="tooltip-text">
              What does this dataset contribute toward better generation evaluation and why is it part of GEM?
             </div>
            </div>
           </h4>
           <p>
            Opusparcus provides examples of sentences that mean the same thing or have very similar meaning. Sentences are available in six languages and the style is colloquial language.
           </p>
          </div>
          <div class="field">
           <h4>
            Difference from other GEM datasets
            <div class="tooltip">
             <div class="tooltip-text">
              What else sets this dataset apart from other similar datasets in GEM?
             </div>
            </div>
           </h4>
           <p>
            There is another data set containing manually labeled Finnish paraphrases.
           </p>
          </div>
         </div>
        </div>
        <h3 id="gem-specific-curation">
         GEM-Specific Curation
        </h3>
        <div class="row">
         <div class="scope">
          <div class="field">
           <h4>
            Modificatied for GEM?
            <div class="tooltip">
             <div class="tooltip-text">
              Has the GEM version of the dataset been modified in any way (data, processing, splits) from the original curated data?
             </div>
            </div>
           </h4>
           <p>
            yes
           </p>
          </div>
          <div class="field">
           <h4>
            Additional Splits?
            <div class="tooltip">
             <div class="tooltip-text">
              Does GEM provide additional splits to the dataset?
             </div>
            </div>
           </h4>
           <p>
            yes
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            GEM Modifications
            <div class="tooltip">
             <div class="tooltip-text">
              What changes have been made to he original dataset?
             </div>
            </div>
           </h4>
           <p>
            <code>
             other
            </code>
           </p>
          </div>
          <div class="field">
           <h4>
            Split Information
            <div class="tooltip">
             <div class="tooltip-text">
              Describe how the new splits were created
             </div>
            </div>
           </h4>
           <p>
            There are two versions of the validations and test sets: the regular sets which only contain positive examples of paraphrases and the full sets containing all examples.
           </p>
          </div>
          <div class="field">
           <h4>
            Split Motivation
            <div class="tooltip">
             <div class="tooltip-text">
              What aspects of the model's generation capacities were the splits created to test?
             </div>
            </div>
           </h4>
           <p>
            In the original release, only the full validation and test sets were supplied. The "regular sets" have been added in order to make it easier to test on true parapahrases only.
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Modification Details
            <div class="tooltip">
             <div class="tooltip-text">
              For each of these changes, described them in more details and provided the intended purpose of the modification
             </div>
            </div>
           </h4>
           <p>
            Training sets have been prepared for each the "quality levels" 60% – 95%.
           </p>
           <p>
            In the original release, this task was left to the user of the data.
           </p>
          </div>
         </div>
        </div>
        <h3 id="getting-started-with-the-task">
         Getting Started with the Task
        </h3>
        <div class="row">
         <div class="scope">
         </div>
         <div class="scope">
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Pointers to Resources
            <div class="tooltip">
             <div class="tooltip-text">
              Getting started with in-depth research on the task. Add relevant pointers to resources that researchers can consult when they want to get started digging deeper into the task.
             </div>
            </div>
           </h4>
           <p>
            Creutz (2018).
            <a href="http://www.lrec-conf.org/proceedings/lrec2018/pdf/131.pdf">
             Open Subtitles Paraphrase Corpus for Six Languages
            </a>
            , Proceedings of the 11th edition of the Language Resources and Evaluation Conference (LREC 2018).
           </p>
           <p>
            Sjöblom et al. (2018).
            <a href="http://noisy-text.github.io/2018/pdf/W-NUT20189.pdf">
             Paraphrase Detection on Noisy Subtitles in Six Languages
            </a>
            . In Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text.
           </p>
           <p>
            Aulamo et al. (2019).
            <a href="http://ceur-ws.org/Vol-2364/3_paper.pdf">
             Annotation of subtitle paraphrases using a new web tool.
            </a>
            In Proceedings of the Digital Humanities in the Nordic Countries 4th Conference.
           </p>
           <p>
            Sjöblom et al. (2020).
            <a href="https://aclanthology.org/2020.lrec-1.224/">
             Paraphrase Generation and Evaluation on Colloquial-Style Sentences
            </a>
            , Proceedings of the 12th Language Resources and Evaluation Conference (LREC).
           </p>
           <p>
            Vahtola et al. (2021).
            <a href="https://aclanthology.org/2021.wnut-1.32/">
             Coping with Noisy Training Data Labels in Paraphrase Detection
            </a>
            . In Proceedings of the 7th Workshop on Noisy User-generated Text.
           </p>
          </div>
         </div>
        </div>
       </div>
      </div>
      <div class="section">
       <div class="section-preview">
        <div class="section-preview-content">
         <h2 id="previous-results">
          Previous Results
         </h2>
         <div class="tags">
          <div class="tag">
           Previous Results
          </div>
         </div>
        </div>
        <button class="section-open-button">
         <svg fill="#3c4f50" height="24px" viewbox="0 0 24 24" width="24px" xmlns="http://www.w3.org/2000/svg">
          <path d="M0 0h24v24H0z" fill="none">
          </path>
          <path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z">
          </path>
         </svg>
        </button>
       </div>
       <div class="subsection-wrapper">
        <h3 id="previous-results">
         Previous Results
        </h3>
        <div class="row">
         <div class="scope">
          <div class="field">
           <h4>
            Measured Model Abilities
            <div class="tooltip">
             <div class="tooltip-text">
              What aspect of model ability can be measured with this dataset?
             </div>
            </div>
           </h4>
           <p>
            Sentence meaning
           </p>
           <p>
            In a scenario of paraphrase detection, the model determines whether two given sentences carry approximately the same meaning.
           </p>
           <p>
            In a scenario of paraphrase generation, the model generates a potential paraphrase of a given sentence.
           </p>
          </div>
          <div class="field">
           <h4>
            Previous results available?
            <div class="tooltip">
             <div class="tooltip-text">
              Are previous results available?
             </div>
            </div>
           </h4>
           <p>
            yes
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Metrics
            <div class="tooltip">
             <div class="tooltip-text">
              What metrics are typically used for this task?
             </div>
            </div>
           </h4>
           <p>
            <code>
             BLEU
            </code>
            ,
            <code>
             BERT-Score
            </code>
            ,
            <code>
             Other: Other Metrics
            </code>
           </p>
          </div>
          <div class="field">
           <h4>
            Other Metrics
            <div class="tooltip">
             <div class="tooltip-text">
              Definitions of other metrics
             </div>
            </div>
           </h4>
           <p>
            PINC
           </p>
          </div>
          <div class="field">
           <h4>
            Other Evaluation Approaches
            <div class="tooltip">
             <div class="tooltip-text">
              What evaluation approaches have others used?
             </div>
            </div>
           </h4>
           <p>
            See publications on using Opusparcus
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Proposed Evaluation
            <div class="tooltip">
             <div class="tooltip-text">
              List and describe the purpose of the metrics and evaluation methodology (including human evaluation) that the dataset creators used when introducing this task.
             </div>
            </div>
           </h4>
           <p>
            The metrics mentioned above can be used to assess how well a generated paraphrase corresponds to a given reference sentence. The PINC score additionally assesses how different the surface forms are.
           </p>
          </div>
          <div class="field">
           <h4>
            Relevant Previous Results
            <div class="tooltip">
             <div class="tooltip-text">
              What are the most relevant previous results for this task/dataset?
             </div>
            </div>
           </h4>
           <p>
            Sjöblom et al. (2020).
            <a href="https://aclanthology.org/2020.lrec-1.224/">
             Paraphrase Generation and Evaluation on Colloquial-Style Sentences
            </a>
            , Proceedings of the 12th Language Resources and Evaluation Conference (LREC).
           </p>
          </div>
         </div>
        </div>
       </div>
      </div>
      <div class="section">
       <div class="section-preview">
        <div class="section-preview-content">
         <h2 id="dataset-curation">
          Dataset Curation
         </h2>
         <div class="tags">
          <div class="tag">
           Original Curation
          </div>
          <div class="tag">
           Language Data
          </div>
          <div class="tag">
           Structured Annotations
          </div>
          <div class="tag">
           Consent
          </div>
          <div class="tag">
           Private Identifying Information (PII)
          </div>
          <div class="tag">
           Maintenance
          </div>
         </div>
        </div>
        <button class="section-open-button">
         <svg fill="#3c4f50" height="24px" viewbox="0 0 24 24" width="24px" xmlns="http://www.w3.org/2000/svg">
          <path d="M0 0h24v24H0z" fill="none">
          </path>
          <path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z">
          </path>
         </svg>
        </button>
       </div>
       <div class="subsection-wrapper">
        <h3 id="original-curation">
         Original Curation
        </h3>
        <div class="row">
         <div class="scope">
          <div class="field">
           <h4>
            Original Curation Rationale
            <div class="tooltip">
             <div class="tooltip-text">
              Original curation rationale
             </div>
            </div>
           </h4>
           <p>
            Opusparcus was created in order to produce a
            <em>
             sentential
            </em>
            paraphrase corpus for multiple languages containing
            <em>
             colloquial
            </em>
            language (as opposed to news or religious text, for instance).
           </p>
          </div>
          <div class="field">
           <h4>
            Sourced from Different Sources
            <div class="tooltip">
             <div class="tooltip-text">
              Is the dataset aggregated from different data sources?
             </div>
            </div>
           </h4>
           <p>
            no
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Communicative Goal
            <div class="tooltip">
             <div class="tooltip-text">
              What was the communicative goal?
             </div>
            </div>
           </h4>
           <p>
            Opusparcus provides labeled examples of pairs of sentences that have similar (or dissimilar) meanings.
           </p>
          </div>
         </div>
         <div class="scope">
         </div>
        </div>
        <h3 id="language-data">
         Language Data
        </h3>
        <div class="row">
         <div class="scope">
          <div class="field">
           <h4>
            How was Language Data Obtained?
            <div class="tooltip">
             <div class="tooltip-text">
              How was the language data obtained?
             </div>
            </div>
           </h4>
           <p>
            <code>
             Crowdsourced
            </code>
           </p>
          </div>
          <div class="field">
           <h4>
            Data Validation
            <div class="tooltip">
             <div class="tooltip-text">
              Was the text validated by a different worker or a data curator?
             </div>
            </div>
           </h4>
           <p>
            validated by data curator
           </p>
          </div>
          <div class="field">
           <h4>
            Was Data Filtered?
            <div class="tooltip">
             <div class="tooltip-text">
              Were text instances selected or filtered?
             </div>
            </div>
           </h4>
           <p>
            algorithmically
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Where was it crowdsourced?
            <div class="tooltip">
             <div class="tooltip-text">
              If crowdsourced, where from?
             </div>
            </div>
           </h4>
           <p>
            <code>
             Other crowdworker platform
            </code>
           </p>
          </div>
          <div class="field">
           <h4>
            Topics Covered
            <div class="tooltip">
             <div class="tooltip-text">
              Does the language in the dataset focus on specific topics? How would you describe them?
             </div>
            </div>
           </h4>
           <p>
            The language is representative of movies and TV shows. Domains covered include comedy, drama, relationships, suspense, etc.
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Language Producers
            <div class="tooltip">
             <div class="tooltip-text">
              What further information do we have on the language producers?
             </div>
            </div>
           </h4>
           <p>
            The data in Opusparcus has been extracted from
            <a href="http://opus.nlpl.eu/OpenSubtitles2016.php">
             OpenSubtitles2016
            </a>
            , which is in turn based on data from http://www.opensubtitles.org/.
           </p>
           <p>
            The texts consists of subtitles that have been produced using crowdsourcing.
           </p>
          </div>
          <div class="field">
           <h4>
            Data Preprocessing
            <div class="tooltip">
             <div class="tooltip-text">
              How was the text data pre-processed? (Enter N/A if the text was not pre-processed)
             </div>
            </div>
           </h4>
           <p>
            Sentence and word tokenization was performed.
           </p>
          </div>
          <div class="field">
           <h4>
            Filter Criteria
            <div class="tooltip">
             <div class="tooltip-text">
              What were the selection criteria?
             </div>
            </div>
           </h4>
           <p>
            The sentence pairs in the training sets were ordered automatically based on the estimated likelihood that the sentences were paraphrases, most likely paraphrases on the top, and least likely paraphrases on the bottom.
           </p>
           <p>
            The validation and test sets were checked and annotated manually, but the sentence pairs selected for annotation had to be different enough in terms of minimum edit distance (Levenshtein distance). This ensured that annotators would not spend their time annotating pairs of more or less identical sentences.
           </p>
          </div>
         </div>
        </div>
        <h3 id="structured-annotations">
         Structured Annotations
        </h3>
        <div class="row">
         <div class="scope">
          <div class="field">
           <h4>
            Additional Annotations?
            <div class="tooltip">
             <div class="tooltip-text">
              Does the dataset have additional annotations for each instance?
             </div>
            </div>
           </h4>
           <p>
            expert created
           </p>
          </div>
          <div class="field">
           <h4>
            Number of Raters
            <div class="tooltip">
             <div class="tooltip-text">
              What is the number of raters
             </div>
            </div>
           </h4>
           <p>
            11&lt;n&lt;50
           </p>
          </div>
          <div class="field">
           <h4>
            Annotation Service?
            <div class="tooltip">
             <div class="tooltip-text">
              Was an annotation service used?
             </div>
            </div>
           </h4>
           <p>
            no
           </p>
          </div>
          <div class="field">
           <h4>
            Any Quality Control?
            <div class="tooltip">
             <div class="tooltip-text">
              Quality control measures?
             </div>
            </div>
           </h4>
           <p>
            validated by another rater
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Rater Qualifications
            <div class="tooltip">
             <div class="tooltip-text">
              Describe the qualifications required of an annotator.
             </div>
            </div>
           </h4>
           <p>
            Students and staff at the University of Helsinki (native or very proficient speakers of the target languages)
           </p>
          </div>
          <div class="field">
           <h4>
            Raters per Training Example
            <div class="tooltip">
             <div class="tooltip-text">
              How many annotators saw each training example?
             </div>
            </div>
           </h4>
           <p>
            0
           </p>
          </div>
          <div class="field">
           <h4>
            Raters per Test Example
            <div class="tooltip">
             <div class="tooltip-text">
              How many annotators saw each test example?
             </div>
            </div>
           </h4>
           <p>
            2
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Annotation Values
            <div class="tooltip">
             <div class="tooltip-text">
              Purpose and values for each annotation
             </div>
            </div>
           </h4>
           <p>
            The development and test sets consist of sentence pairs that have been annotated manually; each set contains approximately 1000 sentence pairs that have been verified to be acceptable paraphrases by two independent annotators.
           </p>
           <p>
            The
            <code>
             annot_score
            </code>
            field reflects the judgments made by the annotators. If the annnotators fully agreed on the category (4.0: dark green, 3.0: light green, 2.0: yellow, 1.0: red), the value of
            <code>
             annot_score
            </code>
            is 4.0, 3.0, 2.0 or 1.0.  If the annotators chose adjacent categories, the value in this field will be 3.5, 2.5 or 1.5.  For instance, a value of 2.5 means that one annotator gave a score of 3 ("mostly good"), indicating a possible paraphrase pair, whereas the other annotator scored this as a 2 ("mostly bad"), that is, unlikely to be a paraphrase pair.  If the annotators disagreed by more than one category, the sentence pair was discarded and won't show up in the datasets.
           </p>
           <p>
            Annotators could also reject a sentence pair as being corrupted data.
           </p>
          </div>
          <div class="field">
           <h4>
            Quality Control Details
            <div class="tooltip">
             <div class="tooltip-text">
              Describe the quality control measures that were taken.
             </div>
            </div>
           </h4>
           <p>
            If the annotators disagreed by more than one category, the sentence pair was discarded and is not part of the final dataset.
           </p>
          </div>
         </div>
        </div>
        <h3 id="consent">
         Consent
        </h3>
        <div class="row">
         <div class="scope">
          <div class="field">
           <h4>
            Any Consent Policy?
            <div class="tooltip">
             <div class="tooltip-text">
              Was there a consent policy involved when gathering the data?
             </div>
            </div>
           </h4>
           <p>
            no
           </p>
          </div>
         </div>
         <div class="scope">
         </div>
         <div class="scope">
         </div>
        </div>
        <h3 id="private-identifying-information-(pii)">
         Private Identifying Information (PII)
        </h3>
        <div class="row">
         <div class="scope">
          <div class="field">
           <h4>
            Contains PII?
            <div class="tooltip">
             <div class="tooltip-text">
              Does the source language data likely contain Personal Identifying Information about the data creators or subjects?
             </div>
            </div>
           </h4>
           <p>
            yes/very likely
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Any PII Identification?
            <div class="tooltip">
             <div class="tooltip-text">
              Did the curators use any automatic/manual method to identify PII in the dataset?
             </div>
            </div>
           </h4>
           <p>
            no identification
           </p>
          </div>
         </div>
         <div class="scope">
         </div>
        </div>
        <h3 id="maintenance">
         Maintenance
        </h3>
        <div class="row">
         <div class="scope">
          <div class="field">
           <h4>
            Any Maintenance Plan?
            <div class="tooltip">
             <div class="tooltip-text">
              Does the original dataset have a maintenance plan?
             </div>
            </div>
           </h4>
           <p>
            no
           </p>
          </div>
         </div>
         <div class="scope">
         </div>
         <div class="scope">
         </div>
        </div>
       </div>
      </div>
      <div class="section">
       <div class="section-preview">
        <div class="section-preview-content">
         <h2 id="broader-social-context">
          Broader Social Context
         </h2>
         <div class="tags">
          <div class="tag">
           Previous Work on the Social Impact of the Dataset
          </div>
          <div class="tag">
           Impact on Under-Served Communities
          </div>
          <div class="tag">
           Discussion of Biases
          </div>
         </div>
        </div>
        <button class="section-open-button">
         <svg fill="#3c4f50" height="24px" viewbox="0 0 24 24" width="24px" xmlns="http://www.w3.org/2000/svg">
          <path d="M0 0h24v24H0z" fill="none">
          </path>
          <path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z">
          </path>
         </svg>
        </button>
       </div>
       <div class="subsection-wrapper">
        <h3 id="previous-work-on-the-social-impact-of-the-dataset">
         Previous Work on the Social Impact of the Dataset
        </h3>
        <div class="row">
         <div class="scope">
          <div class="field">
           <h4>
            Usage of Models based on the Data
            <div class="tooltip">
             <div class="tooltip-text">
              Are you aware of cases where models trained on the task featured in this dataset ore related tasks have been used in automated systems?
             </div>
            </div>
           </h4>
           <p>
            no
           </p>
          </div>
         </div>
         <div class="scope">
         </div>
         <div class="scope">
         </div>
        </div>
        <h3 id="impact-on-under-served-communities">
         Impact on Under-Served Communities
        </h3>
        <div class="row">
         <div class="scope">
          <div class="field">
           <h4>
            Addresses needs of underserved Communities?
            <div class="tooltip">
             <div class="tooltip-text">
              Does this dataset address the needs of communities that are traditionally underserved in language technology, and particularly language generation technology? Communities may be underserved for exemple because their language, language variety, or social or geographical context is underepresented in NLP and NLG resources (datasets and models).
             </div>
            </div>
           </h4>
           <p>
            no
           </p>
          </div>
         </div>
         <div class="scope">
         </div>
         <div class="scope">
         </div>
        </div>
        <h3 id="discussion-of-biases">
         Discussion of Biases
        </h3>
        <div class="row">
         <div class="scope">
          <div class="field">
           <h4>
            Any Documented Social Biases?
            <div class="tooltip">
             <div class="tooltip-text">
              Are there documented social biases in the dataset? Biases in this context are variations in the ways members of different social categories are represented that can have harmful downstream consequences for members of the more disadvantaged group.
             </div>
            </div>
           </h4>
           <p>
            no
           </p>
          </div>
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Are the Language Producers Representative of the Language?
            <div class="tooltip">
             <div class="tooltip-text">
              Does the distribution of language producers in the dataset accurately represent the full distribution of speakers of the language world-wide? If not, how does it differ?
             </div>
            </div>
           </h4>
           <p>
            What social bias there may be in the subtitles in this dataset has not been studied.
           </p>
          </div>
         </div>
         <div class="scope">
         </div>
        </div>
       </div>
      </div>
      <div class="section">
       <div class="section-preview">
        <div class="section-preview-content">
         <h2 id="considerations-for-using-the-data">
          Considerations for Using the Data
         </h2>
         <div class="tags">
          <div class="tag">
           PII Risks and Liability
          </div>
          <div class="tag">
           Licenses
          </div>
          <div class="tag">
           Known Technical Limitations
          </div>
         </div>
        </div>
        <button class="section-open-button">
         <svg fill="#3c4f50" height="24px" viewbox="0 0 24 24" width="24px" xmlns="http://www.w3.org/2000/svg">
          <path d="M0 0h24v24H0z" fill="none">
          </path>
          <path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z">
          </path>
         </svg>
        </button>
       </div>
       <div class="subsection-wrapper">
        <h3 id="pii-risks-and-liability">
         PII Risks and Liability
        </h3>
        <div class="row">
         <div class="scope">
         </div>
         <div class="scope">
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Potential PII Risk
            <div class="tooltip">
             <div class="tooltip-text">
              Considering your answers to the PII part of the Data Curation Section, describe any potential privacy to the data subjects and creators risks when using the dataset.
             </div>
            </div>
           </h4>
           <p>
            The data only contains subtitles of publicly available movies and TV shows.
           </p>
          </div>
         </div>
        </div>
        <h3 id="licenses">
         Licenses
        </h3>
        <div class="row">
         <div class="scope">
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Copyright Restrictions on the Dataset
            <div class="tooltip">
             <div class="tooltip-text">
              Based on your answers in the Intended Use part of the Data Overview Section, which of the following best describe the copyright and licensing status of the dataset?
             </div>
            </div>
           </h4>
           <p>
            <code>
             non-commercial use only
            </code>
           </p>
          </div>
          <div class="field">
           <h4>
            Copyright Restrictions on the Language Data
            <div class="tooltip">
             <div class="tooltip-text">
              Based on your answers in the Language part of the Data Curation Section, which of the following best describe the copyright and licensing status of the underlying language data?
             </div>
            </div>
           </h4>
           <p>
            <code>
             non-commercial use only
            </code>
           </p>
          </div>
         </div>
         <div class="scope">
         </div>
        </div>
        <h3 id="known-technical-limitations">
         Known Technical Limitations
        </h3>
        <div class="row">
         <div class="scope">
         </div>
         <div class="scope">
         </div>
         <div class="scope">
          <div class="field">
           <h4>
            Technical Limitations
            <div class="tooltip">
             <div class="tooltip-text">
              Describe any known technical limitations, such as spurrious correlations, train/test overlap, annotation biases, or mis-annotations, and cite the works that first identified these limitations when possible.
             </div>
            </div>
           </h4>
           <p>
            Some subtitles contain typos that are caused by inaccurate OCR.
           </p>
          </div>
          <div class="field">
           <h4>
            Unsuited Applications
            <div class="tooltip">
             <div class="tooltip-text">
              When using a model trained on this dataset in a setting where users or the public may interact with its predictions, what are some pitfalls to look out for? In particular, describe some applications of the general task featured in this dataset that its curation or properties make it less suitable for.
             </div>
            </div>
           </h4>
           <p>
            The models might memorize individual subtitles of existing movies and TV shows, but there is no context across sentence boundaries in the data.
           </p>
          </div>
          <div class="field">
           <h4>
            Discouraged Use Cases
            <div class="tooltip">
             <div class="tooltip-text">
              What are some discouraged use cases of a model trained to maximize the proposed metrics on this dataset? In particular, think about settings where decisions made by a model that performs reasonably well on the metric my still have strong negative consequences for user or members of the public.
             </div>
            </div>
           </h4>
           <p>
            A general issue with paraphrasing is that very small modifications in the surface form might produce valid paraphrases, which are however rather uninteresting. It is more valuable to produce paraphrases with clearly different surface realizations (e.g., measured using minimum edit distance).
           </p>
          </div>
         </div>
        </div>
       </div>
      </div>
     </section>
    </article>
   </div>
  </main>
 </body>
 <script>
  initializeSections();initializeTables();initializeCodeBlocks();initializeTags();initializeNav();initializeEscapes();function initializeSections(){const sections=document.getElementsByClassName('section');const sectionButtons=document.getElementsByClassName('section-open-button');sections[0].classList.add('open');for(let i=0;i<sections.length;i+=1){sectionButtons[i].addEventListener('click',(e)=>{if(sections[i].classList.contains('open')){closeSection(sections[i])}else{openSection(sections[i])}})}document.getElementById('expand-all').addEventListener('click',(e)=>{for(section of sections){openSection(section)}});document.getElementById('collapse-all').addEventListener('click',(e)=>{for(section of sections){closeSection(section)}})}function openSection(section){const collapsible=section.children[1];collapsible.style.maxHeight=collapsible.scrollHeight+"px";section.classList.add('open');setTimeout(()=>{collapsible.removeAttribute('style')},500)}function closeSection(section){if(section.classList.contains('open')){const collapsible=section.children[1];collapsible.style.maxHeight=collapsible.scrollHeight+"px";section.classList.remove('open');setTimeout(()=>{collapsible.removeAttribute('style')},200)}}function initializeTables(){const elements=document.getElementsByTagName('table');const wrapperTemplate=document.getElementById('table-template');for(let i=0;i<elements.length;i+=1){const element=elements[i];const table=wrapperTemplate.content.cloneNode(true).children[0];const wrapper=document.createElement('div');wrapper.appendChild(table);wrapper.addEventListener('click',(e)=>escapeModal);element.parentElement.replaceChild(wrapper,element);table.getElementsByClassName('expand-icon')[0].addEventListener('click',toggleModal);table.appendChild(element)}}function initializeCodeBlocks(){const elements=document.getElementsByTagName('pre');const wrapperTemplate=document.getElementById('code-block-template');for(let i=0;i<elements.length;i+=1){const element=elements[i];const code=wrapperTemplate.content.cloneNode(true).children[0];const wrapper=document.createElement('div');wrapper.appendChild(code);wrapper.addEventListener('click',(e)=>escapeModal);element.parentElement.replaceChild(wrapper,element);code.getElementsByClassName('expand-icon')[0].addEventListener('click',toggleModal);code.getElementsByClassName('copy-icon')[0].addEventListener('click',copy);code.appendChild(element)}}function initializeEscapes(){document.addEventListener('click',(e)=>{if(e.target.classList.contains('expand')){escapeModal(e.target)}});document.addEventListener('keyup',(e)=>{if(e.key==="Escape"){const expanded=document.getElementsByClassName('expand');if(expanded.length>0){escapeModal(expanded[0])}}})}function initializeTags(){for(tag of document.getElementsByClassName('tag')){const s=tag.parentElement.parentElement.parentElement.parentElement;tag.addEventListener('click',(e)=>jumpToHeading(titleToId(e.currentTarget.textContent)))}}function initializeNav(){document.getElementById('datacard-outline-button').addEventListener('click',toggleNav);for(heading of document.getElementById('datacard-outline-nav').getElementsByTagName('li')){heading.addEventListener('click',(e)=>{jumpToHeading(titleToId(e.currentTarget.textContent))})}}function toggleNav(){const button=document.getElementById('datacard-outline-button');const nav=document.getElementById('datacard-outline-nav');if(button.classList.contains('active')){button.classList.remove('active');nav.classList.remove('active')}else{button.classList.add('active');nav.classList.add('active')}}function jumpToHeading(id){const heading=document.getElementById(id);if(heading.parentElement.parentElement.classList.contains('section')){openSection(heading.parentElement.parentElement)}else if(heading.parentElement.parentElement.parentElement.classList.contains('section')){openSection(heading.parentElement.parentElement.parentElement)}const cardWindow=document.getElementsByClassName('content-wrapper')[0];setTimeout(()=>{cardWindow.scrollTop=heading.offsetTop-90},500)}function titleToId(title){return title.trim().toLowerCase().replaceAll(' ','-')}function toggleModal(e){const icon=e.currentTarget;const wrapper=icon.parentElement.parentElement.parentElement;if(icon.className==='expand-icon'){document.body.style.overflow='hidden';wrapper.classList.add('expand');icon.className='collapse-icon'}else{wrapper.classList.remove('expand');icon.className='expand-icon';document.body.style.overflow='scroll'}}function escapeModal(wrapper){wrapper.classList.remove('expand');for(icon of wrapper.getElementsByClassName('collapse-icon')){icon.className='expand-icon'}document.body.style.overflow='scroll'}function copy(e){const icon=e.currentTarget;const wrapper=icon.parentElement.parentElement.parentElement;const content=wrapper.getElementsByTagName('code')[0].textContent;navigator.clipboard.writeText(content);}
 </script>
 <script src="./resizer.js">
 </script>
</html>
