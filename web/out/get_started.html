<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="Benchmark natural language generation systems with GEM."/><meta property="og:image" content="https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"/><meta name="og:title" content="GEM"/><meta name="twitter:card" content="summary_large_image"/><title>GEM Getting Started</title><link rel="preload" href="/_next/static/css/2786522978a02f025205.css" as="style"/><link rel="stylesheet" href="/_next/static/css/2786522978a02f025205.css" data-n-g=""/><link rel="preload" href="/_next/static/css/f2fce7b83fe6ca04479b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f2fce7b83fe6ca04479b.css" data-n-p=""/><noscript data-n-css="true"></noscript><link rel="preload" href="/_next/static/chunks/main-47bc8f80085b54a800da.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.baa41d4dbf5d52db897c.js" as="script"/><link rel="preload" href="/_next/static/chunks/7be4c7f9a57475915b89ef778f50e2cd16d9d4fa.4a36a385313236c59b19.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-a9ae7a6d1de4e51a7ab6.js" as="script"/><link rel="preload" href="/_next/static/chunks/cb1608f2.a574dc0b5846fc81ad3b.js" as="script"/><link rel="preload" href="/_next/static/chunks/5fc4db2d47954213393d82dafadd7645f428589f.bebe18d4314e3419f9cc.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/get_started-765efda2c2825d3f73c0.js" as="script"/></head><body><div id="__next"><div class="layout_background__1AVEa undefined"><header class="layout_header__2rhWq"><div class="navbar_navwrapper__15zia"><div class="navbar_gradbar__1Xi5u"></div><nav class="navbar_navbar__3gnco"><span class="utils_headingLg__de7p0 navbar_navbarlogo__PLEwr"><a href="/">GEM BENCHMARK</a></span><div class="navbar_menutoggle__358pJ" id="mobile-menu"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars fa-w-14 navbar_bar__QVPSR" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"></path></svg></div><ul><li class="navbar_navitem__3ICSG navbar_pushright__3G2DM"><a href="/resources">Resources</a></li><li class="navbar_navitem__3ICSG"><a href="/data_cards">Data Cards</a></li><li class="navbar_navitem__3ICSG"><a href="/model_cards">Model Cards</a></li><li class="navbar_navitem__3ICSG"><a href="/get_started">How To</a></li><li class="navbar_navitem__3ICSG"><a href="/results">Results</a></li><li class="navbar_navitem__3ICSG"><a href="/papers">Papers</a></li><li class="navbar_navitem__3ICSG"><a href="/team">Team</a></li><li class="navbar_navitem__3ICSG"><a href="/nl_augmenter">NL-Augmenter</a></li><li class="navbar_navitem__3ICSG"><a href="/workshop">Workshop</a></li></ul></nav></div></header><div class="layout_container__2t4v2"><main><article><span class="utils_headingXl__1XecN">Getting Started <code> coding</code>.</span><span class="utils_smallSpace__375iy"></span><div><p>This tutorial presents a full walk-through how to get started with GEM, how to load and inspect data, how to finetune a baseline model, and how to generate predictions.
Throughout this tutorial, we will focus on the CommonGen task, but we will note
what changes to make to use another of the GEM datasets.</p>
<p><strong>SUBMITTING</strong> Our <a href="https://forms.gle/vbTZDMCuqzok8tTA9">submission form</a> is permanently open! Please account for some extra time to write your model card.</p>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#preliminaries">Preliminaries</a></li>
<li><a href="#loading-the-data">Loading the Data</a>
<ul>
<li><a href="#loading-a-single-example">Loading a single example</a></li>
</ul>
</li>
<li><a href="#finetuning-a-pretrained-model">Finetuning a pretrained model</a></li>
<li><a href="#generating-and-evaluating-predictions">Generating and evaluating Predictions</a></li>
<li><a href="#generating-and-submitting-test-predictions">Generating and Submitting Test Predictions</a>
<ul>
<li><a href="#format-description">Format Description</a></li>
<li><a href="#formatting-your-predictions">Formatting your predictions</a></li>
</ul>
</li>
<li><a href="#evaluating-your-submission-file-with-the-gem-evaluation-framework">Evaluating your submission file with the GEM evaluation framework.</a></li>
</ul>
<h2 id="preliminaries">Preliminaries</h2>
<p>This tutorial uses PyTorch and the HuggingFace infrastructure to finetune models. You need to install the following dependencies:</p>
<pre><code class="hljs language-bash">pip install git+https://github.com/huggingface/datasets.git
pip install rouge_score
pip install sentencepiece
pip install transformers</code></pre>
<p>We further assume access to a GPU in this tutorial. You should be able to run all the code inside of a <a href="https://colab.research.google.com/">colab notebook for free GPU access</a>.</p>
<h2 id="loading-the-data">Loading the Data</h2>
<p>We will be using <a href="https://huggingface.co/docs/datasets/gem">HuggingFace datasets</a>, but the GEM datasets are available in <a href="https://www.tensorflow.org/datasets">TFDS</a> as well.</p>
<p>You can load and inspect datasets like this:</p>
<pre><code class="hljs language-python">>> <span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
>> data = load_dataset(<span class="hljs-string">"gem"</span>, <span class="hljs-string">"common_gen"</span>)
>> data

DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">'gem_id'</span>, <span class="hljs-string">'gem_parent_id'</span>, <span class="hljs-string">'concept_set_id'</span>, <span class="hljs-string">'concepts'</span>, <span class="hljs-string">'target'</span>, <span class="hljs-string">'references'</span>],
        num_rows: <span class="hljs-number">67389</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">'gem_id'</span>, <span class="hljs-string">'gem_parent_id'</span>, <span class="hljs-string">'concept_set_id'</span>, <span class="hljs-string">'concepts'</span>, <span class="hljs-string">'target'</span>, <span class="hljs-string">'references'</span>],
        num_rows: <span class="hljs-number">993</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">'gem_id'</span>, <span class="hljs-string">'gem_parent_id'</span>, <span class="hljs-string">'concept_set_id'</span>, <span class="hljs-string">'concepts'</span>, <span class="hljs-string">'target'</span>, <span class="hljs-string">'references'</span>],
        num_rows: <span class="hljs-number">1497</span>
    })
    challenge_train_sample: Dataset({
        features: [<span class="hljs-string">'gem_id'</span>, <span class="hljs-string">'gem_parent_id'</span>, <span class="hljs-string">'concept_set_id'</span>, <span class="hljs-string">'concepts'</span>, <span class="hljs-string">'target'</span>, <span class="hljs-string">'references'</span>],
        num_rows: <span class="hljs-number">500</span>
    })
    challenge_validation_sample: Dataset({
        features: [<span class="hljs-string">'gem_id'</span>, <span class="hljs-string">'gem_parent_id'</span>, <span class="hljs-string">'concept_set_id'</span>, <span class="hljs-string">'concepts'</span>, <span class="hljs-string">'target'</span>, <span class="hljs-string">'references'</span>],
        num_rows: <span class="hljs-number">500</span>
    })
    challenge_test_scramble: Dataset({
        features: [<span class="hljs-string">'gem_id'</span>, <span class="hljs-string">'gem_parent_id'</span>, <span class="hljs-string">'concept_set_id'</span>, <span class="hljs-string">'concepts'</span>, <span class="hljs-string">'target'</span>, <span class="hljs-string">'references'</span>],
        num_rows: <span class="hljs-number">500</span>
    })
})</code></pre>
<p>You can notice that challenge sets created as part of GEM act just like any other data split, which means that you can use them with exactly the same code!</p>
<p>GEM supports many other datasets, simply pick one from this list and check out the corresponding <a href="/data_cards">data cards</a>.</p>
<pre><code class="hljs language-py">[<span class="hljs-string">'common_gen'</span>, <span class="hljs-string">'cs_restaurants'</span>, <span class="hljs-string">'dart'</span>, <span class="hljs-string">'mlsum_de'</span>, <span class="hljs-string">'mlsum_es'</span>, <span class="hljs-string">'xsum'</span>,  
 <span class="hljs-string">'e2e_nlg'</span>, <span class="hljs-string">'schema_guided_dialog'</span>, <span class="hljs-string">'totto'</span>, <span class="hljs-string">'web_nlg_en'</span>, <span class="hljs-string">'web_nlg_ru'</span>, 
  <span class="hljs-string">'wiki_auto_asset_turk'</span>, <span class="hljs-string">'wiki_lingua_arabic_ar'</span>, <span class="hljs-string">'wiki_lingua_chinese_zh'</span>, 
  <span class="hljs-string">'wiki_lingua_czech_cs'</span>, <span class="hljs-string">'wiki_lingua_dutch_nl'</span>, <span class="hljs-string">'wiki_lingua_english_en'</span>, 
  <span class="hljs-string">'wiki_lingua_french_fr'</span>, <span class="hljs-string">'wiki_lingua_german_de'</span>, <span class="hljs-string">'wiki_lingua_hindi_hi'</span>, 
  <span class="hljs-string">'wiki_lingua_indonesian_id'</span>, <span class="hljs-string">'wiki_lingua_italian_it'</span>, 
  <span class="hljs-string">'wiki_lingua_japanese_ja'</span>, <span class="hljs-string">'wiki_lingua_korean_ko'</span>, 
  <span class="hljs-string">'wiki_lingua_portuguese_pt'</span>, <span class="hljs-string">'wiki_lingua_russian_ru'</span>, 
  <span class="hljs-string">'wiki_lingua_spanish_es'</span>, <span class="hljs-string">'wiki_lingua_thai_th'</span>, 
  <span class="hljs-string">'wiki_lingua_turkish_tr'</span>, <span class="hljs-string">'wiki_lingua_vietnamese_vi'</span>]</code></pre>
<h3 id="loading-a-single-example">Loading a single example</h3>
<p>Now let's look at a single example:</p>
<pre><code class="hljs language-python">>> data[<span class="hljs-string">'train'</span>][<span class="hljs-number">0</span>]

{<span class="hljs-string">'concept_set_id'</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">'concepts'</span>: [<span class="hljs-string">'mountain'</span>, <span class="hljs-string">'ski'</span>, <span class="hljs-string">'skier'</span>],
 <span class="hljs-string">'gem_id'</span>: <span class="hljs-string">'common_gen-train-0'</span>,
 <span class="hljs-string">'references'</span>: [],
 <span class="hljs-string">'target'</span>: <span class="hljs-string">'Skier skis down the mountain'</span>}</code></pre>
<p>CommonGen is a task that asks for the production of a sentence (<code>target</code>) from a set of concepts (<code>concepts</code>). Since one concept set can generate multiple meaningful sentences, the example also includes a unique identifier (<code>concept_set_idx</code>) so that multiple references can be linked to an input.</p>
<p>Next, let's define utility functions that can generate batches of (tokenized) examples which we can use during training.</p>
<pre><code class="hljs language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">construct_input_for_batch</span>(<span class="hljs-params">batch</span>):</span>
  <span class="hljs-string">"""
  Function that takes a batch from a dataset and constructs the corresponding
  input string.
  """</span>
  source = [<span class="hljs-string">' '</span>.join(concepts) <span class="hljs-keyword">for</span> concepts <span class="hljs-keyword">in</span> batch [<span class="hljs-string">"concepts"</span>]]
  target = batch[<span class="hljs-string">"target"</span>]
  <span class="hljs-keyword">return</span> source, target

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">batch_tokenize</span>(<span class="hljs-params">dataset_batch, tokenizer, decoder_max_length=<span class="hljs-number">32</span></span>):</span>
  <span class="hljs-string">"""
  Construct the batch (source, target) and run them through a tokenizer.
  """</span>
  source, target = construct_input_for_batch(dataset_batch)
  res = {
      <span class="hljs-string">"input_ids"</span>: tokenizer(source)[<span class="hljs-string">"input_ids"</span>],
      <span class="hljs-string">"labels"</span>: tokenizer(
          target,
          padding=<span class="hljs-string">'max_length'</span>,
          truncation=<span class="hljs-literal">True</span>,
          max_length=decoder_max_length
      )[<span class="hljs-string">"input_ids"</span>],
  }
  <span class="hljs-keyword">return</span> res</code></pre>
<p>All we need to do now to preprocess the dataset is to call <code>batch_tokenize</code> on it. For our example, we are using BART-base as a model and we need to load the corresponding tokenizer:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">'facebook/bart-base'</span>)

train_data_tokenized = data[<span class="hljs-string">'train'</span>].<span class="hljs-built_in">map</span>(
  <span class="hljs-keyword">lambda</span> batch: batch_tokenize(batch, tokenizer, DATASET_NAME, decoder_max_length=DECODER_MAX_LENGTH),
  batched=<span class="hljs-literal">True</span>
)
valid_data_tokenized = data[<span class="hljs-string">'validation'</span>].<span class="hljs-built_in">map</span>(
  <span class="hljs-keyword">lambda</span> batch: batch_tokenize(batch, tokenizer, DATASET_NAME, decoder_max_length=DECODER_MAX_LENGTH),
  batched=<span class="hljs-literal">True</span>
)</code></pre>
<h2 id="finetuning-a-pretrained-model">Finetuning a pretrained model</h2>
<p>We can now utilize the preprocessed data to finetune a model. To do so, we will utilize the <a href="https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainingarguments">Trainer API</a> which handles gradient updates, model selection, and evaluation for us.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments</code></pre>
<p>To improve model selection, let's pick the model that has the best test performance on ROUGE-2, a metric that is typically associated with higher fluency. We can do this by constructing a function that returns a function that computes the score and we only have to pass it to our trainer.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric

rouge_scorer = load_metric(<span class="hljs-string">"rouge"</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">rouge_metric_builder</span>(<span class="hljs-params">tokenizer</span>):</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_rouge_metrics</span>(<span class="hljs-params">pred</span>):</span>
    <span class="hljs-string">"""utility to compute ROUGE during training."""</span>
    labels_ids = pred.label_ids
    pred_ids = pred.predictions
    <span class="hljs-comment"># All special tokens are removed.</span>
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
    labels_ids[labels_ids == -<span class="hljs-number">100</span>] = tokenizer.pad_token_id
    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># Compute the metric.</span>
    rouge_results = rouge_scorer.compute(
        predictions=pred_str,
        references=label_str,
        rouge_types=[<span class="hljs-string">"rouge2"</span>, <span class="hljs-string">"rougeL"</span>],
        use_agregator=<span class="hljs-literal">True</span>,
        use_stemmer=<span class="hljs-literal">False</span>,
    )
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">"rouge2"</span>: <span class="hljs-built_in">round</span>(rouge_results[<span class="hljs-string">'rouge2'</span>].mid.fmeasure, <span class="hljs-number">4</span>),
        <span class="hljs-string">"rougeL"</span>: <span class="hljs-built_in">round</span>(rouge_results[<span class="hljs-string">'rougeL'</span>].mid.fmeasure, <span class="hljs-number">4</span>),
    }
  <span class="hljs-keyword">return</span> compute_rouge_metrics

rouge_metric_fn = rouge_metric_builder(tokenizer)</code></pre>
<p>Fantastic, now all we have to do is set up our trainer class with everything we defined so far and train it!</p>
<pre><code class="hljs language-python">model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">'facebook/bart-base'</span>)
model = model.to(<span class="hljs-string">'cuda:0'</span>)

train_args = Seq2SeqTrainingArguments(
    output_dir=<span class="hljs-string">"BART-commongen"</span>,
    do_train=<span class="hljs-literal">True</span>,
    do_eval=<span class="hljs-literal">True</span>,
    evaluation_strategy=<span class="hljs-string">"epoch"</span>,
    logging_steps=<span class="hljs-number">100</span>,
    <span class="hljs-comment"># optimization args, the trainer uses the Adam optimizer</span>
    <span class="hljs-comment"># and has a linear warmup for the learning rate</span>
    per_device_train_batch_size=<span class="hljs-number">32</span>,
    per_device_eval_batch_size=<span class="hljs-number">32</span>,
    gradient_accumulation_steps=<span class="hljs-number">1</span>,
    learning_rate=<span class="hljs-number">1e-04</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    warmup_steps=<span class="hljs-number">1000</span>,
    <span class="hljs-comment"># misc args</span>
    seed=<span class="hljs-number">42</span>,
    disable_tqdm=<span class="hljs-literal">False</span>,
    load_best_model_at_end=<span class="hljs-literal">True</span>,
    metric_for_best_model=<span class="hljs-string">"rouge2"</span>,
    <span class="hljs-comment"># generation</span>
    predict_with_generate=<span class="hljs-literal">True</span>,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=train_args,
    train_dataset=train_data_tokenized,
    eval_dataset=valid_data_tokenized,
    tokenizer=tokenizer,
    compute_metrics=rouge_metric_fn,
)

trainer._max_length = DECODER_MAX_LENGTH
trainer._num_beams = BEAM_SIZE</code></pre>
<p>And finally:</p>
<pre><code class="hljs language-python">>> trainer.train()

Epoch	Training Loss	Validation Loss	Rouge2	    Rougel
<span class="hljs-number">1</span>	<span class="hljs-number">1.081300</span>	<span class="hljs-number">1.063452</span>	<span class="hljs-number">0.121900</span>	<span class="hljs-number">0.319900</span>
<span class="hljs-number">2</span>	<span class="hljs-number">0.948100</span>	<span class="hljs-number">1.086376</span>	<span class="hljs-number">0.134000</span>	<span class="hljs-number">0.329800</span>
<span class="hljs-number">3</span>	<span class="hljs-number">0.820100</span>	<span class="hljs-number">1.077763</span>	<span class="hljs-number">0.133900</span>	<span class="hljs-number">0.328000</span></code></pre>
<p>We now have a model that achieves 13.4 ROUGE-2 which can obviously still be tuned, but it is a great starting point.</p>
<h2 id="generating-and-evaluating-predictions">Generating and evaluating Predictions</h2>
<p>Given that we now have a model, we also want to generate model outputs now. For this, let's build another two utility functions that generate a batch with only model inputs and which generate and detokenize text with a model.</p>
<pre><code class="hljs language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">make_batch_inputs</span>(<span class="hljs-params">batch, tokenizer, device=<span class="hljs-string">'cuda:0'</span></span>):</span>
  <span class="hljs-string">"""
  Function that takes a batch from a dataset and formats it as input to model.
  """</span>
  <span class="hljs-comment"># Concatenate the concept names for each example in the batch.</span>
  input_lists, _ = construct_input_for_batch(batch)
  <span class="hljs-comment"># Use the model's tokenizer to create the batch input_ids.</span>
  batch_features = tokenizer(input_lists, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">'pt'</span>)
  <span class="hljs-comment"># Move all inputs to the device.</span>
  batch_features = <span class="hljs-built_in">dict</span>([(k, v.to(device)) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch_features.items()])
  <span class="hljs-keyword">return</span> batch_features

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">beam_generate_sentences</span>(<span class="hljs-params">batch,
                            model,
                            tokenizer,
                            num_beams=<span class="hljs-number">4</span>,
                            max_length=<span class="hljs-number">32</span>,
                            device=<span class="hljs-string">'cuda:0'</span></span>):</span>
  <span class="hljs-string">"""
  Function to generate outputs from a model with beam search decoding.
  """</span>
  <span class="hljs-comment"># Create batch inputs.</span>
  features = make_batch_inputs(
      batch=batch,
      tokenizer=tokenizer,
      device=device)
  <span class="hljs-comment"># Generate with beam search.</span>
  generated_ids = model.generate(
      input_ids=features[<span class="hljs-string">'input_ids'</span>],
      attention_mask=features[<span class="hljs-string">'attention_mask'</span>],
      num_beams=num_beams,
      max_length=max_length,
  )
  <span class="hljs-comment"># Use model tokenizer to decode to text.</span>
  generated_sentences = [
      tokenizer.decode(gen_ids.tolist(), skip_special_tokens=<span class="hljs-literal">True</span>)
      <span class="hljs-keyword">for</span> gen_ids <span class="hljs-keyword">in</span> generated_ids
  ]
  <span class="hljs-keyword">return</span> generated_sentences</code></pre>
<p>We can quickly apply this function across our validation set as a sanity check.</p>
<pre><code class="hljs language-python">valid_output = data[<span class="hljs-string">'validation'</span>].<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> batch: {<span class="hljs-string">'generated'</span>: beam_generate_sentences(
        batch,
        model,
        tokenizer,
        num_beams=BEAM_SIZE,
        max_length=MAX_GENERATION_LENGTH)
    },
    batched=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">128</span>,
)

rouge_scorer = load_metric(<span class="hljs-string">"rouge"</span>)
<span class="hljs-comment"># Evaluate for ROUGE-2/L</span>
rouge_results = rouge_scorer.compute(
    predictions=valid_output[<span class="hljs-string">"generated"</span>],
    references=valid_output[<span class="hljs-string">"target"</span>],
    rouge_types=[<span class="hljs-string">"rouge2"</span>, <span class="hljs-string">"rougeL"</span>],
    use_agregator=<span class="hljs-literal">True</span>, use_stemmer=<span class="hljs-literal">False</span>,
)

<span class="hljs-string">f"R-2: <span class="hljs-subst">{rouge_results[<span class="hljs-string">'rouge2'</span>].mid.fmeasure:<span class="hljs-number">.3</span>f}</span> R-L: <span class="hljs-subst">{rouge_results[<span class="hljs-string">'rougeL'</span>].mid.fmeasure:<span class="hljs-number">.3</span>f}</span>"</span></code></pre>
<p>As expected, this yields the following output:</p>
<pre><code class="hljs language-python"><span class="hljs-string">'R-2: 0.134 R-L: 0.329'</span></code></pre>
<h2 id="generating-and-submitting-test-predictions">Generating and Submitting Test Predictions</h2>
<h3 id="format-description">Format Description</h3>
<p>Please format submissions in the following format</p>
<pre><code class="hljs language-json">{
  <span class="hljs-attr">"submission_name"</span>: <span class="hljs-string">"An identifying name of your system"</span>,
  <span class="hljs-attr">"param_count"</span>: <span class="hljs-number">123</span>, # the number of parameters your system has.
  <span class="hljs-attr">"description"</span>: <span class="hljs-string">"An optional brief description of the system that will be shown on the website"</span>,
  <span class="hljs-attr">"tasks"</span>:
    {
      <span class="hljs-attr">"dataset_identifier"</span>: {
        <span class="hljs-attr">"values"</span>: [<span class="hljs-string">"output1"</span>, <span class="hljs-string">"output2"</span>, <span class="hljs-string">"..."</span>], # A list of system outputs
        # Optionally, you can add the keys which are part of an example to ensure that there is no shuffling mistakes.
        <span class="hljs-attr">"keys"</span>: [<span class="hljs-string">"schema_guided_dialog-test-9585"</span>, <span class="hljs-string">"schema_guided_dialog-test-9585"</span>, ...] 
        }
    }
}
</code></pre>
<p>In this case, <code>dataset_identifier</code> is the identifier of the dataset followed by an identifier of the set the outputs were created from, for example <code>_validation</code> or <code>_test</code>. That means, the common_gen validation set would have the identifier <code>common_gen_validation</code>.</p>
<p>The <code>keys</code> field can be set to avoid accidental shuffling to impact your metrics. Simply add a list of the <code>gem_id</code> for each output example in the same order as your values.</p>
<h3 id="formatting-your-predictions">Formatting your predictions</h3>
<p>To format your model outputs for GEM, let's first assume that we have the test and challenge set outputs similar to our validation outputs above. The code is adding a <code>generated</code> field into the dataset which makes analysis much easier.
However, in our submission file we only want the actual values and corresponding IDs. Thus, we filter:</p>
<pre><code class="hljs language-python">valid_formatted = [o[<span class="hljs-string">'generated'</span>] <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> valid_output]
valid_keys = [o[<span class="hljs-string">'gem_id'</span>] <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> data[<span class="hljs-string">'validation'</span>]]

test_formatted = [o[<span class="hljs-string">'generated'</span>] <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> test_output]
test_keys = [o[<span class="hljs-string">'gem_id'</span>] <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> data[<span class="hljs-string">'test'</span>]]

challenge_train_sample_formatted = [o[<span class="hljs-string">'generated'</span>] <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> challenge_train_sample_output]
challenge_train_sample_keys = [o[<span class="hljs-string">'gem_id'</span>] <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> data[<span class="hljs-string">'challenge_train_sample'</span>]]</code></pre>
<p>In our final step, we only have to add the outputs to our larger submission construct.</p>
<pre><code class="hljs language-python">submission_dict = {
    <span class="hljs-string">"submission_name"</span>: <span class="hljs-string">"BART-base"</span>,
    <span class="hljs-string">"param_count"</span>: <span class="hljs-built_in">sum</span>(p.numel() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters()),
    <span class="hljs-string">"description"</span>: <span class="hljs-string">"Baseline for the task based on BART-base."</span>,
    <span class="hljs-string">"tasks"</span>: {
      <span class="hljs-string">"common_gen_validation"</span>: {<span class="hljs-string">"values"</span>: valid_formatted, <span class="hljs-string">"keys"</span>: valid_keys},
      <span class="hljs-string">"common_gen_test"</span>: {<span class="hljs-string">"values"</span>: test_formatted, <span class="hljs-string">"keys"</span>: test_keys},
      <span class="hljs-string">"common_gen_challenge_train_sample"</span>: {<span class="hljs-string">"values"</span>: challenge_train_sample_formatted, 
                                            <span class="hljs-string">"keys"</span>: challenge_train_sample_keys}
    }
}</code></pre>
<p>This format is scalable to more tasks, you simply need to add more outputs to the <code>tasks</code> subfield.
The last step is to write our submission dictionary to a file.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> json
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'gem_submission.json'</span>, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> f:
  f.write(json.dumps(submission_dict, indent=<span class="hljs-number">2</span>))</code></pre>
<h2 id="evaluating-your-submission-file-with-the-gem-evaluation-framework">Evaluating your submission file with the GEM evaluation framework.</h2>
<p>Obviously, we do not want to rely only on ROUGE scores. For this, we developed the GEM evaluation framework. You can download it by running</p>
<pre><code class="hljs language-bash">git <span class="hljs-built_in">clone</span> git@github.com:GEM-benchmark/GEM-metrics.git</code></pre>
<p>Assuming that you formatted your outputs correctly, you can now run</p>
<pre><code class="hljs language-bash">python run_metrics.py [-r references.json] [-o outputs.scores.json] outputs.json </code></pre>
<p>which will create a json file with your scores per task and challenge set. Please follow the <a href="https://github.com/GEM-benchmark/GEM-metrics">README</a> for more detailed usage information.</p>
</div></article></main><div class="layout_push__1J9g0"></div></div><footer class="layout_footer__127N0 utils_eggshell__Njxsh"><span class="layout_backToHome__1vZsp"><a href="/">‚Üê Home</a></span><span>If you have any questions, please join our <a href="https://groups.google.com/g/gem-benchmark" target="_blank" class="utils_accentUnderline__k083p">google group</a> for support.</span></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"tutorialData":{"contentHtml":"\u003cp\u003eThis tutorial presents a full walk-through how to get started with GEM, how to load and inspect data, how to finetune a baseline model, and how to generate predictions.\nThroughout this tutorial, we will focus on the CommonGen task, but we will note\nwhat changes to make to use another of the GEM datasets.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSUBMITTING\u003c/strong\u003e Our \u003ca href=\"https://forms.gle/vbTZDMCuqzok8tTA9\"\u003esubmission form\u003c/a\u003e is permanently open! Please account for some extra time to write your model card.\u003c/p\u003e\n\u003ch2 id=\"table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#preliminaries\"\u003ePreliminaries\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#loading-the-data\"\u003eLoading the Data\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#loading-a-single-example\"\u003eLoading a single example\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#finetuning-a-pretrained-model\"\u003eFinetuning a pretrained model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#generating-and-evaluating-predictions\"\u003eGenerating and evaluating Predictions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#generating-and-submitting-test-predictions\"\u003eGenerating and Submitting Test Predictions\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#format-description\"\u003eFormat Description\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#formatting-your-predictions\"\u003eFormatting your predictions\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#evaluating-your-submission-file-with-the-gem-evaluation-framework\"\u003eEvaluating your submission file with the GEM evaluation framework.\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"preliminaries\"\u003ePreliminaries\u003c/h2\u003e\n\u003cp\u003eThis tutorial uses PyTorch and the HuggingFace infrastructure to finetune models. You need to install the following dependencies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003epip install git+https://github.com/huggingface/datasets.git\npip install rouge_score\npip install sentencepiece\npip install transformers\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe further assume access to a GPU in this tutorial. You should be able to run all the code inside of a \u003ca href=\"https://colab.research.google.com/\"\u003ecolab notebook for free GPU access\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"loading-the-data\"\u003eLoading the Data\u003c/h2\u003e\n\u003cp\u003eWe will be using \u003ca href=\"https://huggingface.co/docs/datasets/gem\"\u003eHuggingFace datasets\u003c/a\u003e, but the GEM datasets are available in \u003ca href=\"https://www.tensorflow.org/datasets\"\u003eTFDS\u003c/a\u003e as well.\u003c/p\u003e\n\u003cp\u003eYou can load and inspect datasets like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003e\u003e \u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_dataset\n\u003e\u003e data = load_dataset(\u003cspan class=\"hljs-string\"\u003e\"gem\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"common_gen\"\u003c/span\u003e)\n\u003e\u003e data\n\nDatasetDict({\n    train: Dataset({\n        features: [\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'gem_parent_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concept_set_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concepts'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'references'\u003c/span\u003e],\n        num_rows: \u003cspan class=\"hljs-number\"\u003e67389\u003c/span\u003e\n    })\n    validation: Dataset({\n        features: [\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'gem_parent_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concept_set_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concepts'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'references'\u003c/span\u003e],\n        num_rows: \u003cspan class=\"hljs-number\"\u003e993\u003c/span\u003e\n    })\n    test: Dataset({\n        features: [\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'gem_parent_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concept_set_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concepts'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'references'\u003c/span\u003e],\n        num_rows: \u003cspan class=\"hljs-number\"\u003e1497\u003c/span\u003e\n    })\n    challenge_train_sample: Dataset({\n        features: [\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'gem_parent_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concept_set_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concepts'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'references'\u003c/span\u003e],\n        num_rows: \u003cspan class=\"hljs-number\"\u003e500\u003c/span\u003e\n    })\n    challenge_validation_sample: Dataset({\n        features: [\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'gem_parent_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concept_set_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concepts'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'references'\u003c/span\u003e],\n        num_rows: \u003cspan class=\"hljs-number\"\u003e500\u003c/span\u003e\n    })\n    challenge_test_scramble: Dataset({\n        features: [\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'gem_parent_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concept_set_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concepts'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'references'\u003c/span\u003e],\n        num_rows: \u003cspan class=\"hljs-number\"\u003e500\u003c/span\u003e\n    })\n})\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can notice that challenge sets created as part of GEM act just like any other data split, which means that you can use them with exactly the same code!\u003c/p\u003e\n\u003cp\u003eGEM supports many other datasets, simply pick one from this list and check out the corresponding \u003ca href=\"/data_cards\"\u003edata cards\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-py\"\u003e[\u003cspan class=\"hljs-string\"\u003e'common_gen'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'cs_restaurants'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'dart'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'mlsum_de'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'mlsum_es'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'xsum'\u003c/span\u003e,  \n \u003cspan class=\"hljs-string\"\u003e'e2e_nlg'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'schema_guided_dialog'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'totto'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'web_nlg_en'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'web_nlg_ru'\u003c/span\u003e, \n  \u003cspan class=\"hljs-string\"\u003e'wiki_auto_asset_turk'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_arabic_ar'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_chinese_zh'\u003c/span\u003e, \n  \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_czech_cs'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_dutch_nl'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_english_en'\u003c/span\u003e, \n  \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_french_fr'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_german_de'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_hindi_hi'\u003c/span\u003e, \n  \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_indonesian_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_italian_it'\u003c/span\u003e, \n  \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_japanese_ja'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_korean_ko'\u003c/span\u003e, \n  \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_portuguese_pt'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_russian_ru'\u003c/span\u003e, \n  \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_spanish_es'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_thai_th'\u003c/span\u003e, \n  \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_turkish_tr'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'wiki_lingua_vietnamese_vi'\u003c/span\u003e]\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"loading-a-single-example\"\u003eLoading a single example\u003c/h3\u003e\n\u003cp\u003eNow let's look at a single example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003e\u003e data[\u003cspan class=\"hljs-string\"\u003e'train'\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n\n{\u003cspan class=\"hljs-string\"\u003e'concept_set_id'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,\n \u003cspan class=\"hljs-string\"\u003e'concepts'\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'mountain'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'ski'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'skier'\u003c/span\u003e],\n \u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'common_gen-train-0'\u003c/span\u003e,\n \u003cspan class=\"hljs-string\"\u003e'references'\u003c/span\u003e: [],\n \u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'Skier skis down the mountain'\u003c/span\u003e}\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCommonGen is a task that asks for the production of a sentence (\u003ccode\u003etarget\u003c/code\u003e) from a set of concepts (\u003ccode\u003econcepts\u003c/code\u003e). Since one concept set can generate multiple meaningful sentences, the example also includes a unique identifier (\u003ccode\u003econcept_set_idx\u003c/code\u003e) so that multiple references can be linked to an input.\u003c/p\u003e\n\u003cp\u003eNext, let's define utility functions that can generate batches of (tokenized) examples which we can use during training.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003econstruct_input_for_batch\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ebatch\u003c/span\u003e):\u003c/span\u003e\n  \u003cspan class=\"hljs-string\"\u003e\"\"\"\n  Function that takes a batch from a dataset and constructs the corresponding\n  input string.\n  \"\"\"\u003c/span\u003e\n  source = [\u003cspan class=\"hljs-string\"\u003e' '\u003c/span\u003e.join(concepts) \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e concepts \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e batch [\u003cspan class=\"hljs-string\"\u003e\"concepts\"\u003c/span\u003e]]\n  target = batch[\u003cspan class=\"hljs-string\"\u003e\"target\"\u003c/span\u003e]\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e source, target\n\n\u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003ebatch_tokenize\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003edataset_batch, tokenizer, decoder_max_length=\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e\u003c/span\u003e):\u003c/span\u003e\n  \u003cspan class=\"hljs-string\"\u003e\"\"\"\n  Construct the batch (source, target) and run them through a tokenizer.\n  \"\"\"\u003c/span\u003e\n  source, target = construct_input_for_batch(dataset_batch)\n  res = {\n      \u003cspan class=\"hljs-string\"\u003e\"input_ids\"\u003c/span\u003e: tokenizer(source)[\u003cspan class=\"hljs-string\"\u003e\"input_ids\"\u003c/span\u003e],\n      \u003cspan class=\"hljs-string\"\u003e\"labels\"\u003c/span\u003e: tokenizer(\n          target,\n          padding=\u003cspan class=\"hljs-string\"\u003e'max_length'\u003c/span\u003e,\n          truncation=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n          max_length=decoder_max_length\n      )[\u003cspan class=\"hljs-string\"\u003e\"input_ids\"\u003c/span\u003e],\n  }\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e res\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAll we need to do now to preprocess the dataset is to call \u003ccode\u003ebatch_tokenize\u003c/code\u003e on it. For our example, we are using BART-base as a model and we need to load the corresponding tokenizer:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\u003cspan class=\"hljs-string\"\u003e'facebook/bart-base'\u003c/span\u003e)\n\ntrain_data_tokenized = data[\u003cspan class=\"hljs-string\"\u003e'train'\u003c/span\u003e].\u003cspan class=\"hljs-built_in\"\u003emap\u003c/span\u003e(\n  \u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e batch: batch_tokenize(batch, tokenizer, DATASET_NAME, decoder_max_length=DECODER_MAX_LENGTH),\n  batched=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e\n)\nvalid_data_tokenized = data[\u003cspan class=\"hljs-string\"\u003e'validation'\u003c/span\u003e].\u003cspan class=\"hljs-built_in\"\u003emap\u003c/span\u003e(\n  \u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e batch: batch_tokenize(batch, tokenizer, DATASET_NAME, decoder_max_length=DECODER_MAX_LENGTH),\n  batched=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e\n)\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"finetuning-a-pretrained-model\"\u003eFinetuning a pretrained model\u003c/h2\u003e\n\u003cp\u003eWe can now utilize the preprocessed data to finetune a model. To do so, we will utilize the \u003ca href=\"https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainingarguments\"\u003eTrainer API\u003c/a\u003e which handles gradient updates, model selection, and evaluation for us.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo improve model selection, let's pick the model that has the best test performance on ROUGE-2, a metric that is typically associated with higher fluency. We can do this by constructing a function that returns a function that computes the score and we only have to pass it to our trainer.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_metric\n\nrouge_scorer = load_metric(\u003cspan class=\"hljs-string\"\u003e\"rouge\"\u003c/span\u003e)\n\n\u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003erouge_metric_builder\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003etokenizer\u003c/span\u003e):\u003c/span\u003e\n  \u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003ecompute_rouge_metrics\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003epred\u003c/span\u003e):\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"\"\"utility to compute ROUGE during training.\"\"\"\u003c/span\u003e\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n    \u003cspan class=\"hljs-comment\"\u003e# All special tokens are removed.\u003c/span\u003e\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n    labels_ids[labels_ids == -\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e] = tokenizer.pad_token_id\n    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n    \u003cspan class=\"hljs-comment\"\u003e# Compute the metric.\u003c/span\u003e\n    rouge_results = rouge_scorer.compute(\n        predictions=pred_str,\n        references=label_str,\n        rouge_types=[\u003cspan class=\"hljs-string\"\u003e\"rouge2\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"rougeL\"\u003c/span\u003e],\n        use_agregator=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n        use_stemmer=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e,\n    )\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e {\n        \u003cspan class=\"hljs-string\"\u003e\"rouge2\"\u003c/span\u003e: \u003cspan class=\"hljs-built_in\"\u003eround\u003c/span\u003e(rouge_results[\u003cspan class=\"hljs-string\"\u003e'rouge2'\u003c/span\u003e].mid.fmeasure, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e),\n        \u003cspan class=\"hljs-string\"\u003e\"rougeL\"\u003c/span\u003e: \u003cspan class=\"hljs-built_in\"\u003eround\u003c/span\u003e(rouge_results[\u003cspan class=\"hljs-string\"\u003e'rougeL'\u003c/span\u003e].mid.fmeasure, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e),\n    }\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e compute_rouge_metrics\n\nrouge_metric_fn = rouge_metric_builder(tokenizer)\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFantastic, now all we have to do is set up our trainer class with everything we defined so far and train it!\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003emodel = AutoModelForSeq2SeqLM.from_pretrained(\u003cspan class=\"hljs-string\"\u003e'facebook/bart-base'\u003c/span\u003e)\nmodel = model.to(\u003cspan class=\"hljs-string\"\u003e'cuda:0'\u003c/span\u003e)\n\ntrain_args = Seq2SeqTrainingArguments(\n    output_dir=\u003cspan class=\"hljs-string\"\u003e\"BART-commongen\"\u003c/span\u003e,\n    do_train=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n    do_eval=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n    evaluation_strategy=\u003cspan class=\"hljs-string\"\u003e\"epoch\"\u003c/span\u003e,\n    logging_steps=\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e,\n    \u003cspan class=\"hljs-comment\"\u003e# optimization args, the trainer uses the Adam optimizer\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e# and has a linear warmup for the learning rate\u003c/span\u003e\n    per_device_train_batch_size=\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e,\n    per_device_eval_batch_size=\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e,\n    gradient_accumulation_steps=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\n    learning_rate=\u003cspan class=\"hljs-number\"\u003e1e-04\u003c/span\u003e,\n    num_train_epochs=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e,\n    warmup_steps=\u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e,\n    \u003cspan class=\"hljs-comment\"\u003e# misc args\u003c/span\u003e\n    seed=\u003cspan class=\"hljs-number\"\u003e42\u003c/span\u003e,\n    disable_tqdm=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e,\n    load_best_model_at_end=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n    metric_for_best_model=\u003cspan class=\"hljs-string\"\u003e\"rouge2\"\u003c/span\u003e,\n    \u003cspan class=\"hljs-comment\"\u003e# generation\u003c/span\u003e\n    predict_with_generate=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=train_args,\n    train_dataset=train_data_tokenized,\n    eval_dataset=valid_data_tokenized,\n    tokenizer=tokenizer,\n    compute_metrics=rouge_metric_fn,\n)\n\ntrainer._max_length = DECODER_MAX_LENGTH\ntrainer._num_beams = BEAM_SIZE\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd finally:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003e\u003e trainer.train()\n\nEpoch\tTraining Loss\tValidation Loss\tRouge2\t    Rougel\n\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e1.081300\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e1.063452\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.121900\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.319900\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.948100\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e1.086376\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.134000\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.329800\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.820100\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e1.077763\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.133900\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.328000\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe now have a model that achieves 13.4 ROUGE-2 which can obviously still be tuned, but it is a great starting point.\u003c/p\u003e\n\u003ch2 id=\"generating-and-evaluating-predictions\"\u003eGenerating and evaluating Predictions\u003c/h2\u003e\n\u003cp\u003eGiven that we now have a model, we also want to generate model outputs now. For this, let's build another two utility functions that generate a batch with only model inputs and which generate and detokenize text with a model.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003emake_batch_inputs\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ebatch, tokenizer, device=\u003cspan class=\"hljs-string\"\u003e'cuda:0'\u003c/span\u003e\u003c/span\u003e):\u003c/span\u003e\n  \u003cspan class=\"hljs-string\"\u003e\"\"\"\n  Function that takes a batch from a dataset and formats it as input to model.\n  \"\"\"\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e# Concatenate the concept names for each example in the batch.\u003c/span\u003e\n  input_lists, _ = construct_input_for_batch(batch)\n  \u003cspan class=\"hljs-comment\"\u003e# Use the model's tokenizer to create the batch input_ids.\u003c/span\u003e\n  batch_features = tokenizer(input_lists, padding=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e, return_tensors=\u003cspan class=\"hljs-string\"\u003e'pt'\u003c/span\u003e)\n  \u003cspan class=\"hljs-comment\"\u003e# Move all inputs to the device.\u003c/span\u003e\n  batch_features = \u003cspan class=\"hljs-built_in\"\u003edict\u003c/span\u003e([(k, v.to(device)) \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e k, v \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e batch_features.items()])\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e batch_features\n\n\u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003ebeam_generate_sentences\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ebatch,\n                            model,\n                            tokenizer,\n                            num_beams=\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e,\n                            max_length=\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e,\n                            device=\u003cspan class=\"hljs-string\"\u003e'cuda:0'\u003c/span\u003e\u003c/span\u003e):\u003c/span\u003e\n  \u003cspan class=\"hljs-string\"\u003e\"\"\"\n  Function to generate outputs from a model with beam search decoding.\n  \"\"\"\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e# Create batch inputs.\u003c/span\u003e\n  features = make_batch_inputs(\n      batch=batch,\n      tokenizer=tokenizer,\n      device=device)\n  \u003cspan class=\"hljs-comment\"\u003e# Generate with beam search.\u003c/span\u003e\n  generated_ids = model.generate(\n      input_ids=features[\u003cspan class=\"hljs-string\"\u003e'input_ids'\u003c/span\u003e],\n      attention_mask=features[\u003cspan class=\"hljs-string\"\u003e'attention_mask'\u003c/span\u003e],\n      num_beams=num_beams,\n      max_length=max_length,\n  )\n  \u003cspan class=\"hljs-comment\"\u003e# Use model tokenizer to decode to text.\u003c/span\u003e\n  generated_sentences = [\n      tokenizer.decode(gen_ids.tolist(), skip_special_tokens=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n      \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e gen_ids \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e generated_ids\n  ]\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e generated_sentences\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe can quickly apply this function across our validation set as a sanity check.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003evalid_output = data[\u003cspan class=\"hljs-string\"\u003e'validation'\u003c/span\u003e].\u003cspan class=\"hljs-built_in\"\u003emap\u003c/span\u003e(\n    \u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e batch: {\u003cspan class=\"hljs-string\"\u003e'generated'\u003c/span\u003e: beam_generate_sentences(\n        batch,\n        model,\n        tokenizer,\n        num_beams=BEAM_SIZE,\n        max_length=MAX_GENERATION_LENGTH)\n    },\n    batched=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n    batch_size=\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e,\n)\n\nrouge_scorer = load_metric(\u003cspan class=\"hljs-string\"\u003e\"rouge\"\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# Evaluate for ROUGE-2/L\u003c/span\u003e\nrouge_results = rouge_scorer.compute(\n    predictions=valid_output[\u003cspan class=\"hljs-string\"\u003e\"generated\"\u003c/span\u003e],\n    references=valid_output[\u003cspan class=\"hljs-string\"\u003e\"target\"\u003c/span\u003e],\n    rouge_types=[\u003cspan class=\"hljs-string\"\u003e\"rouge2\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"rougeL\"\u003c/span\u003e],\n    use_agregator=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e, use_stemmer=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e,\n)\n\n\u003cspan class=\"hljs-string\"\u003ef\"R-2: \u003cspan class=\"hljs-subst\"\u003e{rouge_results[\u003cspan class=\"hljs-string\"\u003e'rouge2'\u003c/span\u003e].mid.fmeasure:\u003cspan class=\"hljs-number\"\u003e.3\u003c/span\u003ef}\u003c/span\u003e R-L: \u003cspan class=\"hljs-subst\"\u003e{rouge_results[\u003cspan class=\"hljs-string\"\u003e'rougeL'\u003c/span\u003e].mid.fmeasure:\u003cspan class=\"hljs-number\"\u003e.3\u003c/span\u003ef}\u003c/span\u003e\"\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAs expected, this yields the following output:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-string\"\u003e'R-2: 0.134 R-L: 0.329'\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"generating-and-submitting-test-predictions\"\u003eGenerating and Submitting Test Predictions\u003c/h2\u003e\n\u003ch3 id=\"format-description\"\u003eFormat Description\u003c/h3\u003e\n\u003cp\u003ePlease format submissions in the following format\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-json\"\u003e{\n  \u003cspan class=\"hljs-attr\"\u003e\"submission_name\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"An identifying name of your system\"\u003c/span\u003e,\n  \u003cspan class=\"hljs-attr\"\u003e\"param_count\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e123\u003c/span\u003e, # the number of parameters your system has.\n  \u003cspan class=\"hljs-attr\"\u003e\"description\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"An optional brief description of the system that will be shown on the website\"\u003c/span\u003e,\n  \u003cspan class=\"hljs-attr\"\u003e\"tasks\"\u003c/span\u003e:\n    {\n      \u003cspan class=\"hljs-attr\"\u003e\"dataset_identifier\"\u003c/span\u003e: {\n        \u003cspan class=\"hljs-attr\"\u003e\"values\"\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e\"output1\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"output2\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"...\"\u003c/span\u003e], # A list of system outputs\n        # Optionally, you can add the keys which are part of an example to ensure that there is no shuffling mistakes.\n        \u003cspan class=\"hljs-attr\"\u003e\"keys\"\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e\"schema_guided_dialog-test-9585\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"schema_guided_dialog-test-9585\"\u003c/span\u003e, ...] \n        }\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn this case, \u003ccode\u003edataset_identifier\u003c/code\u003e is the identifier of the dataset followed by an identifier of the set the outputs were created from, for example \u003ccode\u003e_validation\u003c/code\u003e or \u003ccode\u003e_test\u003c/code\u003e. That means, the common_gen validation set would have the identifier \u003ccode\u003ecommon_gen_validation\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003ekeys\u003c/code\u003e field can be set to avoid accidental shuffling to impact your metrics. Simply add a list of the \u003ccode\u003egem_id\u003c/code\u003e for each output example in the same order as your values.\u003c/p\u003e\n\u003ch3 id=\"formatting-your-predictions\"\u003eFormatting your predictions\u003c/h3\u003e\n\u003cp\u003eTo format your model outputs for GEM, let's first assume that we have the test and challenge set outputs similar to our validation outputs above. The code is adding a \u003ccode\u003egenerated\u003c/code\u003e field into the dataset which makes analysis much easier.\nHowever, in our submission file we only want the actual values and corresponding IDs. Thus, we filter:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003evalid_formatted = [o[\u003cspan class=\"hljs-string\"\u003e'generated'\u003c/span\u003e] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e o \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e valid_output]\nvalid_keys = [o[\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e o \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e data[\u003cspan class=\"hljs-string\"\u003e'validation'\u003c/span\u003e]]\n\ntest_formatted = [o[\u003cspan class=\"hljs-string\"\u003e'generated'\u003c/span\u003e] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e o \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e test_output]\ntest_keys = [o[\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e o \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e data[\u003cspan class=\"hljs-string\"\u003e'test'\u003c/span\u003e]]\n\nchallenge_train_sample_formatted = [o[\u003cspan class=\"hljs-string\"\u003e'generated'\u003c/span\u003e] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e o \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e challenge_train_sample_output]\nchallenge_train_sample_keys = [o[\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e o \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e data[\u003cspan class=\"hljs-string\"\u003e'challenge_train_sample'\u003c/span\u003e]]\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn our final step, we only have to add the outputs to our larger submission construct.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003esubmission_dict = {\n    \u003cspan class=\"hljs-string\"\u003e\"submission_name\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"BART-base\"\u003c/span\u003e,\n    \u003cspan class=\"hljs-string\"\u003e\"param_count\"\u003c/span\u003e: \u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(p.numel() \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e p \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e model.parameters()),\n    \u003cspan class=\"hljs-string\"\u003e\"description\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"Baseline for the task based on BART-base.\"\u003c/span\u003e,\n    \u003cspan class=\"hljs-string\"\u003e\"tasks\"\u003c/span\u003e: {\n      \u003cspan class=\"hljs-string\"\u003e\"common_gen_validation\"\u003c/span\u003e: {\u003cspan class=\"hljs-string\"\u003e\"values\"\u003c/span\u003e: valid_formatted, \u003cspan class=\"hljs-string\"\u003e\"keys\"\u003c/span\u003e: valid_keys},\n      \u003cspan class=\"hljs-string\"\u003e\"common_gen_test\"\u003c/span\u003e: {\u003cspan class=\"hljs-string\"\u003e\"values\"\u003c/span\u003e: test_formatted, \u003cspan class=\"hljs-string\"\u003e\"keys\"\u003c/span\u003e: test_keys},\n      \u003cspan class=\"hljs-string\"\u003e\"common_gen_challenge_train_sample\"\u003c/span\u003e: {\u003cspan class=\"hljs-string\"\u003e\"values\"\u003c/span\u003e: challenge_train_sample_formatted, \n                                            \u003cspan class=\"hljs-string\"\u003e\"keys\"\u003c/span\u003e: challenge_train_sample_keys}\n    }\n}\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis format is scalable to more tasks, you simply need to add more outputs to the \u003ccode\u003etasks\u003c/code\u003e subfield.\nThe last step is to write our submission dictionary to a file.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e json\n\u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eopen\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'gem_submission.json'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'w'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e f:\n  f.write(json.dumps(submission_dict, indent=\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e))\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"evaluating-your-submission-file-with-the-gem-evaluation-framework\"\u003eEvaluating your submission file with the GEM evaluation framework.\u003c/h2\u003e\n\u003cp\u003eObviously, we do not want to rely only on ROUGE scores. For this, we developed the GEM evaluation framework. You can download it by running\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003egit \u003cspan class=\"hljs-built_in\"\u003eclone\u003c/span\u003e git@github.com:GEM-benchmark/GEM-metrics.git\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAssuming that you formatted your outputs correctly, you can now run\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003epython run_metrics.py [-r references.json] [-o outputs.scores.json] outputs.json \u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhich will create a json file with your scores per task and challenge set. Please follow the \u003ca href=\"https://github.com/GEM-benchmark/GEM-metrics\"\u003eREADME\u003c/a\u003e for more detailed usage information.\u003c/p\u003e\n","title":"Getting Started with GEM"}},"__N_SSG":true},"page":"/get_started","query":{},"buildId":"PUpFviqIdrXs-KvlmN0xY","nextExport":false,"isFallback":false,"gsp":true,"head":[["meta",{"name":"viewport","content":"width=device-width"}],["meta",{"charSet":"utf-8"}],["link",{"rel":"icon","href":"/favicon.ico"}],["meta",{"name":"description","content":"Benchmark natural language generation systems with GEM."}],["meta",{"property":"og:image","content":"https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light\u0026md=1\u0026fontSize=100px\u0026images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"}],["meta",{"name":"og:title","content":"GEM"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["title",{"children":"GEM Getting Started"}]]}</script><script nomodule="" src="/_next/static/chunks/polyfills-1f8e22dc7857e48a2d36.js"></script><script src="/_next/static/chunks/main-47bc8f80085b54a800da.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.baa41d4dbf5d52db897c.js" async=""></script><script src="/_next/static/chunks/7be4c7f9a57475915b89ef778f50e2cd16d9d4fa.4a36a385313236c59b19.js" async=""></script><script src="/_next/static/chunks/pages/_app-a9ae7a6d1de4e51a7ab6.js" async=""></script><script src="/_next/static/chunks/cb1608f2.a574dc0b5846fc81ad3b.js" async=""></script><script src="/_next/static/chunks/5fc4db2d47954213393d82dafadd7645f428589f.bebe18d4314e3419f9cc.js" async=""></script><script src="/_next/static/chunks/pages/get_started-765efda2c2825d3f73c0.js" async=""></script><script src="/_next/static/PUpFviqIdrXs-KvlmN0xY/_buildManifest.js" async=""></script><script src="/_next/static/PUpFviqIdrXs-KvlmN0xY/_ssgManifest.js" async=""></script></body></html>