<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="Benchmark natural language generation systems with GEM."/><meta property="og:image" content="https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"/><meta name="og:title" content="GEM"/><meta name="twitter:card" content="summary_large_image"/><title>GEM Self-Training, Acceptability Classifiers and Context-Conditioning</title><meta name="next-head-count" content="8"/><link rel="preload" href="/_next/static/css/a3e0e142709ba87c.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a3e0e142709ba87c.css" data-n-g=""/><link rel="preload" href="/_next/static/css/9c6cbdf6de0d7f7f.css" as="style"/><link rel="stylesheet" href="/_next/static/css/9c6cbdf6de0d7f7f.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-0ab145c6df6e7728.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6dfdacc79861396c.js" defer=""></script><script src="/_next/static/chunks/pages/_app-f4e7aa7b46503ccf.js" defer=""></script><script src="/_next/static/chunks/cb1608f2-4ba41bd4c311e4ed.js" defer=""></script><script src="/_next/static/chunks/50-1fbacdd54a295188.js" defer=""></script><script src="/_next/static/chunks/pages/model_cards/%5Bid%5D-308b3e0cae025e18.js" defer=""></script><script src="/_next/static/7CoIxUQBnvC3dSsR6Rk6K/_buildManifest.js" defer=""></script><script src="/_next/static/7CoIxUQBnvC3dSsR6Rk6K/_ssgManifest.js" defer=""></script><script src="/_next/static/7CoIxUQBnvC3dSsR6Rk6K/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="layout_background__qqb_S undefined"><header class="layout_header__kY0Lt"><div class="navbar_navwrapper__PQ35R"><div class="navbar_gradbar__2_FPI"></div><nav class="navbar_navbar__9q1fQ"><span class="utils_headingLg__5535D navbar_navbarlogo__R_s98"><a href="/">GEM BENCHMARK</a></span><div class="navbar_menutoggle__XS8Qx" id="mobile-menu"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars fa-w-14 navbar_bar___PSVO" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"></path></svg></div><ul><li class="navbar_navitem__wFPZL navbar_pushright__cG1uq"><a href="/resources">Resources</a></li><li class="navbar_navitem__wFPZL"><a href="/data_cards">Data Cards</a></li><li class="navbar_navitem__wFPZL"><a href="/model_cards">Model Cards</a></li><li class="navbar_navitem__wFPZL"><a href="/tutorials">tutorials</a></li><li class="navbar_navitem__wFPZL"><a href="/results">Results</a></li><li class="navbar_navitem__wFPZL"><a href="/papers">Papers</a></li><li class="navbar_navitem__wFPZL"><a href="/team">Team</a></li><li class="navbar_navitem__wFPZL"><a href="/workshop">Workshop</a></li></ul></nav></div></header><div class="layout_container__fbLkO undefined"><main><article><span class="utils_headingXl__u25Y2">Self-Training, Acceptability Classifiers and Context-Conditioning</span><span class="utils_smallSpace__n1Oyc"></span><span class="utils_lightText__eUzGY">Shared Task 2021</span><div><h2 id="user-content-table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#model-description">Model Description</a></li>
<li><a href="#social-impact">Social Impact</a>
<ul>
<li><a href="#additional-data">Additional Data</a></li>
<li><a href="#training-process">Training Process</a></li>
<li><a href="#real-world-use">Real-World Use</a></li>
<li><a href="#measuring-impact">Measuring Impact</a></li>
</ul>
</li>
<li><a href="#reproducibility">Reproducibility</a>
<ul>
<li><a href="#model-description-1">Model Description</a></li>
<li><a href="#model-hyperparameters">Model Hyperparameters</a></li>
<li><a href="#the-hyperparameter-specifications-for-best-performing-models">The Hyperparameter specifications for best performing models</a></li>
<li><a href="#dataset-details">Dataset Details</a></li>
<li><a href="#dependencies-and-external-libraries">Dependencies and External Libraries</a></li>
<li><a href="#link-to-downloadable-source-code">Link to downloadable source code</a></li>
<li><a href="#computing-infrastructure-used">Computing Infrastructure Used</a></li>
<li><a href="#evaluation-details">Evaluation Details</a></li>
</ul>
</li>
</ul>
<h2 id="user-content-model-description">Model Description</h2>
<ul>
<li><strong>Paper:</strong> <a href="https://aclanthology.org/2021.gem-1.12">Structure-to-Text Generation with Self-Training, Acceptability Classifiers and Context-Conditioning for the GEM Shared Task</a></li>
<li><strong>Creators:</strong> Shreyan Bakshi, Soumya Batra, Peyman Heidari, Ankit Arun, Shashank Jain, Michael White</li>
<li><strong>Point of Contact:</strong> Shreyan Bakshi (<a href="mailto:shreyanb@fb.com">shreyanb@fb.com</a>)</li>
</ul>
<h2 id="user-content-social-impact">Social Impact</h2>
<p>In this section, we ask you to provide information on all of the steps that went into obtaining your models, especially in regards to how they would affect a user’s interactions with the technology if it was deployed in a live system.</p>
<p>Then, please choose one of these steps to analyze in terms of possible negative impacts on potential direct and indirect users, and propose a test to evaluate the existence and magnitude of that impact. We provide examples of such analyses in the following two paragraphs.</p>
<p>Consider for example a model pre-trained on English Wikipedia and fine-tuned on a summarization dataset, and imagine that such a model is deployed in a news website to provide automatic summaries. Given the gender gap on Wikipedia, we can imagine two possible effects:</p>
<ol>
<li>the model could systematically produce summaries with lower ROUGE scores when compared to a reference for articles describing women than it would for articles describing men.</li>
<li>the model may be less likely to name the subject of an article if the subject is a woman than if the subject is a man.</li>
</ol>
<p>We can measure either of these effects by running an entity linking system on the articles, for example, and comparing subsets of the test set where the gender of the actors is known.</p>
<p>Alternatively, imagine that a system that is fine-tuned on WikiAuto, which aligns English Wikipedia text to its Simple English Wikipedia version, is used in an attempt to make a blog where the writer talks about their personal experience more accessible. How well does the model handle the shift from a third person to a first person point of view? One way we can check whether the behavior is roughly the same is by comparing the copy rate (number of words from the input that are re-used in the output) between the Wikipedia and blog setting.</p>
<p>We ask you to take a similar approach to analyzing your model:</p>
<h3 id="user-content-additional-data">Additional Data</h3>
<p><em>If you used a model that was pre-trained on additional data or used additional data, please describe it. Provide a link to a datasheet or data statement if there is one available, otherwise, provide as much relevant information as possible on the source of the data, the people represented in it, its languages, licensing, pre-processing, etc.</em></p>
<p>For generation (including self-training), a Pre-trained BART-Large Model as described/published (and available) here: <a href="https://github.com/pytorch/fairseq/blob/master/examples/bart/README.md">https://github.com/pytorch/fairseq/blob/master/examples/bart/README.md</a> was used.</p>
<p>This was published (<a href="https://arxiv.org/pdf/1910.13461.pdf">https://arxiv.org/pdf/1910.13461.pdf</a>) in 2019 by Facebook AI, under the MIT License (<a href="https://github.com/pytorch/fairseq)">https://github.com/pytorch/fairseq)</a>.</p>
<p>We also used the templates that Kale &#x26; Rastogi used in their EMNLP-20 paper, available here: <a href="https://github.com/google-research/schema-guided-dialogue/tree/main/generation">https://github.com/google-research/schema-guided-dialogue/tree/main/generation</a></p>
<p>For the Acceptability Classification, we used a Pre-trained RoBERTa-Base Model as described/published (and available) here:
<a href="https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md">https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md</a></p>
<p>This was published (<a href="https://arxiv.org/pdf/1907.11692.pdf">https://arxiv.org/pdf/1907.11692.pdf</a>) in 2019 by Facebook AI, under the MIT License (<a href="https://github.com/pytorch/fairseq)">https://github.com/pytorch/fairseq)</a>.</p>
<h3 id="user-content-training-process">Training Process</h3>
<p><em>Describe the training or fine tuning setup, including whether the final model was trained on a single task or in a multi-task setting. If a data augmentation technique was used, describe the technique</em></p>
<p>We passed the SGD through the Google templates and augmented with the service name and five turns of preceding context.</p>
<p>For WebNLG, we preprocessed the dataset based on the implicit tree structure ordering from Yang et al. (2020). We additionally sorted siblings by increasing subtree depth, breaking ties by sorting alphabetically on predicate names.</p>
<p>For E2E, no additional pre-processing of that data was performed.</p>
<p>We then fine-tuned a BART-Large model to generate the target responses using these inputs.  We also trained a BART-Large model to reconstruct the input from the generated response.  For SGD and WebNLG, this model included the service/domain name in the input as well and was trained to reconstruct the templated input (without context for SGD).</p>
<p>Using these models, we ran 2 iterations of self-training, where we created unpaired inputs by deleting subsets of the input dialog acts, randomly choosing up to 20 unpaired inputs per original input.  In each self-training iteration, we ran the generator on the unpaired inputs and then the reconstruction model on the resulting responses, keeping those that yielded an exact match with the original input and adding them to the training data for retraining the generator and reconstruction models.</p>
<p>We noted that for the case of SGD, the self-trained model was susceptible to stuttering, i.e., repeating the same phrase over and over again. This was not observed in the BART-Large generation model. Hence, to control for stuttering, for each response generated by the self-trained model, we used the heuristic that if any word (excluding stop words such as articles, conjunctions, etc.) was repeated in the generated response more than 5 times; we preferred the response generated by the BART-Large model instead.</p>
<p>Finally, we passed the generated responses for a dataset to an acceptability classifier that selected the first generated response in a 5-best list that passed the acceptability test for output (or the 1-best output if no responses passed the test). We filtered out the responses from the 5-best list that had an unacceptability score > threshold, and the top response was selected from the remaining. In the case of all responses getting filtered out, the original 1-best response was selected.</p>
<p>We trained an acceptability classifier for each dataset using the training data provided for that dataset and further augmenting it by generating synthetic positive and negative responses to augment those in the training data. The synthetic data was created by mask filling using the BART-Large generator (prior to self-training) fine-tuned on the original training data, with the resulting synthesized responses passed through a RoBERTa-large-mnli entailment model to determine whether they were acceptable (i.e., paraphrases of the seed response) or unacceptable (semantically inaccurate compared to the seed response).  Finally, the resulting data was used to train a RoBERTa-base classifier for predicting response acceptability.</p>
<h3 id="user-content-real-world-use">Real-World Use</h3>
<p><em>Describe a possible real-use application of your model, then choose one of the steps (e.g. choice of the pre-trained model, data source, data augmentation, training loss, etc.) above and describe a negative impact it may have on the user experience</em></p>
<p>BART is a pre-trained model whose training data is perhaps not as well-studied as it could be (as is apparently true of all pre-trained models).   Our SGD model conditions on the context and uses BART, and thus would be susceptible to being triggered into generating unsafe (malicious, racist, sexist, etc.) language by certain words that the user provides if it were to be deployed with no safety rails. We could estimate how susceptible the model might be to adversarial attacks by examining how often it copied a word from the context that was outside the training vocabulary. More sophisticated analyses could follow the methods of Dinan et al. (<a href="https://arxiv.org/abs/1908.06083)">https://arxiv.org/abs/1908.06083)</a>.  As noted above, this could lead to unsafe responses getting generated in real-word use-cases such as Dialog systems for Assistants.</p>
<h3 id="user-content-measuring-impact">Measuring Impact</h3>
<p><em>Propose a method to test the magnitude of the impact identified in your previous answer</em></p>
<p>As noted earlier, Dinan et al. have proposed a method (<a href="https://arxiv.org/abs/1908.06083">https://arxiv.org/abs/1908.06083</a>) for adversarial analysis that could be pursued to investigate this issue.  Subsequent work (e.g., Xu et al., <a href="https://arxiv.org/abs/2010.07079">https://arxiv.org/abs/2010.07079</a>) has investigated ways of improving safety, but this remains an open problem.</p>
<h2 id="user-content-reproducibility">Reproducibility</h2>
<p><em>In this section, we ask you to provide any information that would be required for someone to reproduce your model and experimental results. These questions are derived from the suggested model card in [1] and the reproducibility checklist in [2].</em></p>
<p><em>[1] Dodge, Jesse, et al. "Show Your Work: Improved Reporting of Experimental Results." EMNLP. 2019.</em></p>
<p><em>[1] Mitchell et al. "Model cards for model reporting." Proceedings of the conference on fairness, accountability, and transparency. 2019.</em></p>
<h3 id="user-content-model-description-1">Model Description</h3>
<p><em>Provide basic information about your model, including (1) the model type (e.g., BART, Pointer Network), (2) model version/date if multiple versions are available, (3) training algorithms used. Please cite papers or other resources where further information about the model can be found. Also include the model license and citation details.</em></p>
<p>As noted earlier, we use BART-Large and RoBERTa-Base models.</p>
<p>For generation (including self-training), a Pre-trained BART-Large Model as described/published (and available) here: <a href="https://github.com/pytorch/fairseq/blob/master/examples/bart/README.md">https://github.com/pytorch/fairseq/blob/master/examples/bart/README.md</a> was used.</p>
<p>This was published (<a href="https://arxiv.org/pdf/1910.13461.pdf">https://arxiv.org/pdf/1910.13461.pdf</a>) in 2019 by Facebook AI, under the MIT License (<a href="https://github.com/pytorch/fairseq)">https://github.com/pytorch/fairseq)</a>.</p>
<p>We also used the templates that Kale &#x26; Rastogi used in their EMNLP-20 paper, available here: <a href="https://github.com/google-research/schema-guided-dialogue/tree/main/generation">https://github.com/google-research/schema-guided-dialogue/tree/main/generation</a></p>
<p>For the Acceptability Classification, we used a Pre-trained RoBERTa-Base Model as described/published (and available) here:
<a href="https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md">https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md</a></p>
<p>This was published (<a href="https://arxiv.org/pdf/1907.11692.pdf">https://arxiv.org/pdf/1907.11692.pdf</a>) in 2019 by Facebook AI, under the MIT License (<a href="https://github.com/pytorch/fairseq)">https://github.com/pytorch/fairseq)</a>.</p>
<p>Our self-training and acceptability classifier methods are drawn from two papers under submission; further details will eventually be available in these papers.</p>
<h3 id="user-content-model-hyperparameters">Model Hyperparameters</h3>
<p><em>Provide the range of hyperparameters that would be required to reproduce your final model (e.g., optimizer used, number of epochs, learning rate, etc.). If hyperparameter search was used, please describe (1) the bounds for each hyperparameter, (2) the number of hyperparameter search trials, (3) the method for choosing hyperparameter values (e.g., uniform sampling, manual tuning, evolutionary optimization, etc.).</em></p>
<p>BART-Large Generation/Reconstruction Hyperparameters</p>
<p>Tokenizer: BPE
Tokenizer Max Length: 256
Dropout: 0.3
Encoder/Decoder Embedding Dim: 1024
Optimizer: Adam
LR: 0.000005
Weight Decay: 0.00001
Number of Model Params: 514484225</p>
<p>Acceptability Classifier Roberta-Base Hyperparameters</p>
<p>Tokenizer: BPE
Tokenizer Max Length: 1024
Encoder output dropout: 0.1
Encoder embedding dim: 768
#encoder layers: 12
#encoder attention heads: 12
Decoder dropout: 0
Decoder activation: relu
Optimizer: Adam
Learning rate: 0.000001
Adam betas: [0.9, 0.999]
Weight Decay: 0
#Model Params: 124055810</p>
<p>Acceptability Classifier Data Generation Model Hyperparameters</p>
<p>Beam Size: 5
topk: 3
Mask normal: 0.5
Mask insert: 0.3</p>
<h3 id="user-content-the-hyperparameter-specifications-for-best-performing-models">The Hyperparameter specifications for best performing models</h3>
<p>Acceptability Classification RoBERTa</p>
<p>In addition to the above hyperparameters, best performing model on the val set had the following unacceptability confidence thresholds at which filtering out took place:</p>
<pre><code>        Dataset                    |     Unacceptability Threshold

        webnlg                     |            0.7

          e2e                         |            0.6
</code></pre>
<p>Schema guided dialog      |             0.6</p>
<p>Bounds used to calculate this were: [0.1 - 0.9] with 0.1 step size.</p>
<h3 id="user-content-dataset-details">Dataset Details</h3>
<p><em>Include relevant training data statistics (e.g., number of samples used, whether some subsets of the dataset were discarded), the training/validation/test splits for the number of samples and any pre-processing steps if used.</em></p>
<p>All the GEM End to End, WebNLG and Schema Guided Dialog train set samples were (transformed and then) used for finetuning the (pre-trained) BART-Large generation model.  The data transforms we performed for WebNLG were:   Pre-processed the dataset based on the implicit tree structure ordering from Yang et al. (2020). We additionally sorted siblings by increasing subtree depth, breaking ties by sorting alphabetically on predicate names.  The data transforms we performed for SGD were:   Create inputs with the service name, templatized inputs and five preceding turns of dialog context separated by a separator token Using the Google templates required additionally retrieving the service method call from the original SGD dataset Preceding turns were listed in order, prefixed by “user:” or “sys:”</p>
<h3 id="user-content-dependencies-and-external-libraries">Dependencies and External Libraries</h3>
<p><em>Include a specification of library dependencies</em></p>
<p>The following Python Libraries:</p>
<p>PyText, FairSeq, Pandas, Numpy, Json, csv, math, sacrebleu, nltk</p>
<h3 id="user-content-link-to-downloadable-source-code">Link to downloadable source code</h3>
<p>N/A</p>
<h3 id="user-content-computing-infrastructure-used">Computing Infrastructure Used</h3>
<p><em>Describe the computing infrastructure used to train your model (e.g., number of GPUs, GPU type and vRAM) and the time taken to train your final model.</em></p>
<p>For dataset transforms: single CPU</p>
<p>For training each generation BART-Large model: 8 GPUs, about 3.5 hours for larger datasets like SGD</p>
<p>For training accuracy classifier RoBERTa-base model: 8 GPUs, data prep + training time is a function of the dataset size, taking upto 2 days on larger datasets like SGD</p>
<p>All experiments were conducted on 32GB Quadro GV100 GPUs. The GPUs are part of a shared distributed cluster, which adds its own time overheads.</p>
<h3 id="user-content-evaluation-details">Evaluation Details</h3>
<p><em>How were your models evaluated? Please include evaluation metric details (including links to code), train/validation/test splits, and model performance on both test and validation sets. If more than one model was trained and evaluated, what was the number of training and evaluation runs, and the variance in scores? If human evaluation was used, please describe the experimental setup.</em></p>
<p>We used the BLEU score from the GEM-metrics script to verify that a BART-Base model with no context was slightly better than the T5 baseline on the validation data.  The small increase could be explained by the inclusion of the service name in the input.  Subsequently, we used a different version of BLEU to compare models as the GEM-metrics were not easy to use in our standard computing environment.  We compared BART-Base and BART-Large models with and without templatizing the inputs and with 0, 1 or 5 turns of preceding context, finding that BART-Large with 5 turns of preceding context and templatized inputs worked the best. We then ran 2 rounds of self-training (each round having alternating generation and reconstruction), and observed improvements in the reconstruction match accuracy during this process, even though BLEU scores were either slightly decreased or stayed the same.</p>
<p>When selecting responses using acceptability classifier, we further analyzed the cases where the selected response was different than the one outputted by the generation model, using a 2-way entailment (to establish paraphrases) via Roberta-large-mnli model between target and selected responses as well as target and original responses. We verified that there were more paraphrases in the former compared to the latter, as well as manually looked at a random sample in each case. We noted that BLEU scores were slightly changed (either increased or decreased) when adding the Acceptability Classifier, but the number of paraphrases (generated response w.r.t target response) were increased.</p>
</div></article></main><div class="layout_push__2FFZa"></div></div><footer class="layout_footer__dka_2 utils_eggshell__LF2UE"><span class="layout_backToHome__9sjx_"><a href="/">← Home</a></span><span>If you have any questions, please join our <!-- --><a href="https://groups.google.com/g/gem-benchmark" target="_blank" class="utils_accentUnderline__FmNQy">google group</a> for support.<!-- --></span></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"taskData":{"contentHtml":"\u003ch2 id=\"user-content-table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#model-description\"\u003eModel Description\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#social-impact\"\u003eSocial Impact\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#additional-data\"\u003eAdditional Data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#training-process\"\u003eTraining Process\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#real-world-use\"\u003eReal-World Use\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#measuring-impact\"\u003eMeasuring Impact\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#reproducibility\"\u003eReproducibility\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#model-description-1\"\u003eModel Description\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#model-hyperparameters\"\u003eModel Hyperparameters\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#the-hyperparameter-specifications-for-best-performing-models\"\u003eThe Hyperparameter specifications for best performing models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dataset-details\"\u003eDataset Details\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dependencies-and-external-libraries\"\u003eDependencies and External Libraries\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#link-to-downloadable-source-code\"\u003eLink to downloadable source code\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#computing-infrastructure-used\"\u003eComputing Infrastructure Used\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#evaluation-details\"\u003eEvaluation Details\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"user-content-model-description\"\u003eModel Description\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePaper:\u003c/strong\u003e \u003ca href=\"https://aclanthology.org/2021.gem-1.12\"\u003eStructure-to-Text Generation with Self-Training, Acceptability Classifiers and Context-Conditioning for the GEM Shared Task\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCreators:\u003c/strong\u003e Shreyan Bakshi, Soumya Batra, Peyman Heidari, Ankit Arun, Shashank Jain, Michael White\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoint of Contact:\u003c/strong\u003e Shreyan Bakshi (\u003ca href=\"mailto:shreyanb@fb.com\"\u003eshreyanb@fb.com\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"user-content-social-impact\"\u003eSocial Impact\u003c/h2\u003e\n\u003cp\u003eIn this section, we ask you to provide information on all of the steps that went into obtaining your models, especially in regards to how they would affect a user’s interactions with the technology if it was deployed in a live system.\u003c/p\u003e\n\u003cp\u003eThen, please choose one of these steps to analyze in terms of possible negative impacts on potential direct and indirect users, and propose a test to evaluate the existence and magnitude of that impact. We provide examples of such analyses in the following two paragraphs.\u003c/p\u003e\n\u003cp\u003eConsider for example a model pre-trained on English Wikipedia and fine-tuned on a summarization dataset, and imagine that such a model is deployed in a news website to provide automatic summaries. Given the gender gap on Wikipedia, we can imagine two possible effects:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ethe model could systematically produce summaries with lower ROUGE scores when compared to a reference for articles describing women than it would for articles describing men.\u003c/li\u003e\n\u003cli\u003ethe model may be less likely to name the subject of an article if the subject is a woman than if the subject is a man.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWe can measure either of these effects by running an entity linking system on the articles, for example, and comparing subsets of the test set where the gender of the actors is known.\u003c/p\u003e\n\u003cp\u003eAlternatively, imagine that a system that is fine-tuned on WikiAuto, which aligns English Wikipedia text to its Simple English Wikipedia version, is used in an attempt to make a blog where the writer talks about their personal experience more accessible. How well does the model handle the shift from a third person to a first person point of view? One way we can check whether the behavior is roughly the same is by comparing the copy rate (number of words from the input that are re-used in the output) between the Wikipedia and blog setting.\u003c/p\u003e\n\u003cp\u003eWe ask you to take a similar approach to analyzing your model:\u003c/p\u003e\n\u003ch3 id=\"user-content-additional-data\"\u003eAdditional Data\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eIf you used a model that was pre-trained on additional data or used additional data, please describe it. Provide a link to a datasheet or data statement if there is one available, otherwise, provide as much relevant information as possible on the source of the data, the people represented in it, its languages, licensing, pre-processing, etc.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eFor generation (including self-training), a Pre-trained BART-Large Model as described/published (and available) here: \u003ca href=\"https://github.com/pytorch/fairseq/blob/master/examples/bart/README.md\"\u003ehttps://github.com/pytorch/fairseq/blob/master/examples/bart/README.md\u003c/a\u003e was used.\u003c/p\u003e\n\u003cp\u003eThis was published (\u003ca href=\"https://arxiv.org/pdf/1910.13461.pdf\"\u003ehttps://arxiv.org/pdf/1910.13461.pdf\u003c/a\u003e) in 2019 by Facebook AI, under the MIT License (\u003ca href=\"https://github.com/pytorch/fairseq)\"\u003ehttps://github.com/pytorch/fairseq)\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWe also used the templates that Kale \u0026#x26; Rastogi used in their EMNLP-20 paper, available here: \u003ca href=\"https://github.com/google-research/schema-guided-dialogue/tree/main/generation\"\u003ehttps://github.com/google-research/schema-guided-dialogue/tree/main/generation\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFor the Acceptability Classification, we used a Pre-trained RoBERTa-Base Model as described/published (and available) here:\n\u003ca href=\"https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md\"\u003ehttps://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis was published (\u003ca href=\"https://arxiv.org/pdf/1907.11692.pdf\"\u003ehttps://arxiv.org/pdf/1907.11692.pdf\u003c/a\u003e) in 2019 by Facebook AI, under the MIT License (\u003ca href=\"https://github.com/pytorch/fairseq)\"\u003ehttps://github.com/pytorch/fairseq)\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"user-content-training-process\"\u003eTraining Process\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eDescribe the training or fine tuning setup, including whether the final model was trained on a single task or in a multi-task setting. If a data augmentation technique was used, describe the technique\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe passed the SGD through the Google templates and augmented with the service name and five turns of preceding context.\u003c/p\u003e\n\u003cp\u003eFor WebNLG, we preprocessed the dataset based on the implicit tree structure ordering from Yang et al. (2020). We additionally sorted siblings by increasing subtree depth, breaking ties by sorting alphabetically on predicate names.\u003c/p\u003e\n\u003cp\u003eFor E2E, no additional pre-processing of that data was performed.\u003c/p\u003e\n\u003cp\u003eWe then fine-tuned a BART-Large model to generate the target responses using these inputs.  We also trained a BART-Large model to reconstruct the input from the generated response.  For SGD and WebNLG, this model included the service/domain name in the input as well and was trained to reconstruct the templated input (without context for SGD).\u003c/p\u003e\n\u003cp\u003eUsing these models, we ran 2 iterations of self-training, where we created unpaired inputs by deleting subsets of the input dialog acts, randomly choosing up to 20 unpaired inputs per original input.  In each self-training iteration, we ran the generator on the unpaired inputs and then the reconstruction model on the resulting responses, keeping those that yielded an exact match with the original input and adding them to the training data for retraining the generator and reconstruction models.\u003c/p\u003e\n\u003cp\u003eWe noted that for the case of SGD, the self-trained model was susceptible to stuttering, i.e., repeating the same phrase over and over again. This was not observed in the BART-Large generation model. Hence, to control for stuttering, for each response generated by the self-trained model, we used the heuristic that if any word (excluding stop words such as articles, conjunctions, etc.) was repeated in the generated response more than 5 times; we preferred the response generated by the BART-Large model instead.\u003c/p\u003e\n\u003cp\u003eFinally, we passed the generated responses for a dataset to an acceptability classifier that selected the first generated response in a 5-best list that passed the acceptability test for output (or the 1-best output if no responses passed the test). We filtered out the responses from the 5-best list that had an unacceptability score \u003e threshold, and the top response was selected from the remaining. In the case of all responses getting filtered out, the original 1-best response was selected.\u003c/p\u003e\n\u003cp\u003eWe trained an acceptability classifier for each dataset using the training data provided for that dataset and further augmenting it by generating synthetic positive and negative responses to augment those in the training data. The synthetic data was created by mask filling using the BART-Large generator (prior to self-training) fine-tuned on the original training data, with the resulting synthesized responses passed through a RoBERTa-large-mnli entailment model to determine whether they were acceptable (i.e., paraphrases of the seed response) or unacceptable (semantically inaccurate compared to the seed response).  Finally, the resulting data was used to train a RoBERTa-base classifier for predicting response acceptability.\u003c/p\u003e\n\u003ch3 id=\"user-content-real-world-use\"\u003eReal-World Use\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eDescribe a possible real-use application of your model, then choose one of the steps (e.g. choice of the pre-trained model, data source, data augmentation, training loss, etc.) above and describe a negative impact it may have on the user experience\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eBART is a pre-trained model whose training data is perhaps not as well-studied as it could be (as is apparently true of all pre-trained models).   Our SGD model conditions on the context and uses BART, and thus would be susceptible to being triggered into generating unsafe (malicious, racist, sexist, etc.) language by certain words that the user provides if it were to be deployed with no safety rails. We could estimate how susceptible the model might be to adversarial attacks by examining how often it copied a word from the context that was outside the training vocabulary. More sophisticated analyses could follow the methods of Dinan et al. (\u003ca href=\"https://arxiv.org/abs/1908.06083)\"\u003ehttps://arxiv.org/abs/1908.06083)\u003c/a\u003e.  As noted above, this could lead to unsafe responses getting generated in real-word use-cases such as Dialog systems for Assistants.\u003c/p\u003e\n\u003ch3 id=\"user-content-measuring-impact\"\u003eMeasuring Impact\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003ePropose a method to test the magnitude of the impact identified in your previous answer\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAs noted earlier, Dinan et al. have proposed a method (\u003ca href=\"https://arxiv.org/abs/1908.06083\"\u003ehttps://arxiv.org/abs/1908.06083\u003c/a\u003e) for adversarial analysis that could be pursued to investigate this issue.  Subsequent work (e.g., Xu et al., \u003ca href=\"https://arxiv.org/abs/2010.07079\"\u003ehttps://arxiv.org/abs/2010.07079\u003c/a\u003e) has investigated ways of improving safety, but this remains an open problem.\u003c/p\u003e\n\u003ch2 id=\"user-content-reproducibility\"\u003eReproducibility\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eIn this section, we ask you to provide any information that would be required for someone to reproduce your model and experimental results. These questions are derived from the suggested model card in [1] and the reproducibility checklist in [2].\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e[1] Dodge, Jesse, et al. \"Show Your Work: Improved Reporting of Experimental Results.\" EMNLP. 2019.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e[1] Mitchell et al. \"Model cards for model reporting.\" Proceedings of the conference on fairness, accountability, and transparency. 2019.\u003c/em\u003e\u003c/p\u003e\n\u003ch3 id=\"user-content-model-description-1\"\u003eModel Description\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eProvide basic information about your model, including (1) the model type (e.g., BART, Pointer Network), (2) model version/date if multiple versions are available, (3) training algorithms used. Please cite papers or other resources where further information about the model can be found. Also include the model license and citation details.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAs noted earlier, we use BART-Large and RoBERTa-Base models.\u003c/p\u003e\n\u003cp\u003eFor generation (including self-training), a Pre-trained BART-Large Model as described/published (and available) here: \u003ca href=\"https://github.com/pytorch/fairseq/blob/master/examples/bart/README.md\"\u003ehttps://github.com/pytorch/fairseq/blob/master/examples/bart/README.md\u003c/a\u003e was used.\u003c/p\u003e\n\u003cp\u003eThis was published (\u003ca href=\"https://arxiv.org/pdf/1910.13461.pdf\"\u003ehttps://arxiv.org/pdf/1910.13461.pdf\u003c/a\u003e) in 2019 by Facebook AI, under the MIT License (\u003ca href=\"https://github.com/pytorch/fairseq)\"\u003ehttps://github.com/pytorch/fairseq)\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWe also used the templates that Kale \u0026#x26; Rastogi used in their EMNLP-20 paper, available here: \u003ca href=\"https://github.com/google-research/schema-guided-dialogue/tree/main/generation\"\u003ehttps://github.com/google-research/schema-guided-dialogue/tree/main/generation\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFor the Acceptability Classification, we used a Pre-trained RoBERTa-Base Model as described/published (and available) here:\n\u003ca href=\"https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md\"\u003ehttps://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis was published (\u003ca href=\"https://arxiv.org/pdf/1907.11692.pdf\"\u003ehttps://arxiv.org/pdf/1907.11692.pdf\u003c/a\u003e) in 2019 by Facebook AI, under the MIT License (\u003ca href=\"https://github.com/pytorch/fairseq)\"\u003ehttps://github.com/pytorch/fairseq)\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur self-training and acceptability classifier methods are drawn from two papers under submission; further details will eventually be available in these papers.\u003c/p\u003e\n\u003ch3 id=\"user-content-model-hyperparameters\"\u003eModel Hyperparameters\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eProvide the range of hyperparameters that would be required to reproduce your final model (e.g., optimizer used, number of epochs, learning rate, etc.). If hyperparameter search was used, please describe (1) the bounds for each hyperparameter, (2) the number of hyperparameter search trials, (3) the method for choosing hyperparameter values (e.g., uniform sampling, manual tuning, evolutionary optimization, etc.).\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eBART-Large Generation/Reconstruction Hyperparameters\u003c/p\u003e\n\u003cp\u003eTokenizer: BPE\nTokenizer Max Length: 256\nDropout: 0.3\nEncoder/Decoder Embedding Dim: 1024\nOptimizer: Adam\nLR: 0.000005\nWeight Decay: 0.00001\nNumber of Model Params: 514484225\u003c/p\u003e\n\u003cp\u003eAcceptability Classifier Roberta-Base Hyperparameters\u003c/p\u003e\n\u003cp\u003eTokenizer: BPE\nTokenizer Max Length: 1024\nEncoder output dropout: 0.1\nEncoder embedding dim: 768\n#encoder layers: 12\n#encoder attention heads: 12\nDecoder dropout: 0\nDecoder activation: relu\nOptimizer: Adam\nLearning rate: 0.000001\nAdam betas: [0.9, 0.999]\nWeight Decay: 0\n#Model Params: 124055810\u003c/p\u003e\n\u003cp\u003eAcceptability Classifier Data Generation Model Hyperparameters\u003c/p\u003e\n\u003cp\u003eBeam Size: 5\ntopk: 3\nMask normal: 0.5\nMask insert: 0.3\u003c/p\u003e\n\u003ch3 id=\"user-content-the-hyperparameter-specifications-for-best-performing-models\"\u003eThe Hyperparameter specifications for best performing models\u003c/h3\u003e\n\u003cp\u003eAcceptability Classification RoBERTa\u003c/p\u003e\n\u003cp\u003eIn addition to the above hyperparameters, best performing model on the val set had the following unacceptability confidence thresholds at which filtering out took place:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e        Dataset                    |     Unacceptability Threshold\n\n        webnlg                     |            0.7\n\n          e2e                         |            0.6\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSchema guided dialog      |             0.6\u003c/p\u003e\n\u003cp\u003eBounds used to calculate this were: [0.1 - 0.9] with 0.1 step size.\u003c/p\u003e\n\u003ch3 id=\"user-content-dataset-details\"\u003eDataset Details\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eInclude relevant training data statistics (e.g., number of samples used, whether some subsets of the dataset were discarded), the training/validation/test splits for the number of samples and any pre-processing steps if used.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAll the GEM End to End, WebNLG and Schema Guided Dialog train set samples were (transformed and then) used for finetuning the (pre-trained) BART-Large generation model.  The data transforms we performed for WebNLG were:   Pre-processed the dataset based on the implicit tree structure ordering from Yang et al. (2020). We additionally sorted siblings by increasing subtree depth, breaking ties by sorting alphabetically on predicate names.  The data transforms we performed for SGD were:   Create inputs with the service name, templatized inputs and five preceding turns of dialog context separated by a separator token Using the Google templates required additionally retrieving the service method call from the original SGD dataset Preceding turns were listed in order, prefixed by “user:” or “sys:”\u003c/p\u003e\n\u003ch3 id=\"user-content-dependencies-and-external-libraries\"\u003eDependencies and External Libraries\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eInclude a specification of library dependencies\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThe following Python Libraries:\u003c/p\u003e\n\u003cp\u003ePyText, FairSeq, Pandas, Numpy, Json, csv, math, sacrebleu, nltk\u003c/p\u003e\n\u003ch3 id=\"user-content-link-to-downloadable-source-code\"\u003eLink to downloadable source code\u003c/h3\u003e\n\u003cp\u003eN/A\u003c/p\u003e\n\u003ch3 id=\"user-content-computing-infrastructure-used\"\u003eComputing Infrastructure Used\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eDescribe the computing infrastructure used to train your model (e.g., number of GPUs, GPU type and vRAM) and the time taken to train your final model.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eFor dataset transforms: single CPU\u003c/p\u003e\n\u003cp\u003eFor training each generation BART-Large model: 8 GPUs, about 3.5 hours for larger datasets like SGD\u003c/p\u003e\n\u003cp\u003eFor training accuracy classifier RoBERTa-base model: 8 GPUs, data prep + training time is a function of the dataset size, taking upto 2 days on larger datasets like SGD\u003c/p\u003e\n\u003cp\u003eAll experiments were conducted on 32GB Quadro GV100 GPUs. The GPUs are part of a shared distributed cluster, which adds its own time overheads.\u003c/p\u003e\n\u003ch3 id=\"user-content-evaluation-details\"\u003eEvaluation Details\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eHow were your models evaluated? Please include evaluation metric details (including links to code), train/validation/test splits, and model performance on both test and validation sets. If more than one model was trained and evaluated, what was the number of training and evaluation runs, and the variance in scores? If human evaluation was used, please describe the experimental setup.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe used the BLEU score from the GEM-metrics script to verify that a BART-Base model with no context was slightly better than the T5 baseline on the validation data.  The small increase could be explained by the inclusion of the service name in the input.  Subsequently, we used a different version of BLEU to compare models as the GEM-metrics were not easy to use in our standard computing environment.  We compared BART-Base and BART-Large models with and without templatizing the inputs and with 0, 1 or 5 turns of preceding context, finding that BART-Large with 5 turns of preceding context and templatized inputs worked the best. We then ran 2 rounds of self-training (each round having alternating generation and reconstruction), and observed improvements in the reconstruction match accuracy during this process, even though BLEU scores were either slightly decreased or stayed the same.\u003c/p\u003e\n\u003cp\u003eWhen selecting responses using acceptability classifier, we further analyzed the cases where the selected response was different than the one outputted by the generation model, using a 2-way entailment (to establish paraphrases) via Roberta-large-mnli model between target and selected responses as well as target and original responses. We verified that there were more paraphrases in the former compared to the latter, as well as manually looked at a random sample in each case. We noted that BLEU scores were slightly changed (either increased or decreased) when adding the Acceptability Classifier, but the number of paraphrases (generated response w.r.t target response) were increased.\u003c/p\u003e\n","title":"Self-Training, Acceptability Classifiers and Context-Conditioning","type":"Shared Task 2021","background":"BART together with RoBERTa classifiers and context."}},"__N_SSG":true},"page":"/model_cards/[id]","query":{"id":"FB"},"buildId":"7CoIxUQBnvC3dSsR6Rk6K","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>