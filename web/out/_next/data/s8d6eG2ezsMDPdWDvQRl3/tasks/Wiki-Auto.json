{"pageProps":{"taskData":{"id":"Wiki-Auto","contentHtml":"<h2 id=\"table-of-contents\">Table of Contents</h2>\n<ul>\n<li><a href=\"#dataset-description\">Dataset Description</a>\n<ul>\n<li><a href=\"#dataset-and-task-summary\">Dataset and Task Summary</a></li>\n<li><a href=\"#why-is-this-dataset-part-of-gem\">Why is this dataset part of GEM?</a></li>\n<li><a href=\"#languages\">Languages</a></li>\n</ul>\n</li>\n<li><a href=\"#meta-information\">Meta Information</a>\n<ul>\n<li><a href=\"#dataset-curators\">Dataset Curators</a></li>\n<li><a href=\"#licensing-information\">Licensing Information</a></li>\n<li><a href=\"#citation-information\">Citation Information</a></li>\n<li><a href=\"#leaderboard\">Leaderboard</a></li>\n</ul>\n</li>\n<li><a href=\"#dataset-structure\">Dataset Structure</a>\n<ul>\n<li><a href=\"#data-instances\">Data Instances</a></li>\n<li><a href=\"#data-fields\">Data Fields</a></li>\n<li><a href=\"#data-statistics\">Data Statistics</a></li>\n</ul>\n</li>\n<li><a href=\"#dataset-creation\">Dataset Creation</a>\n<ul>\n<li><a href=\"#curation-rationale\">Curation Rationale</a></li>\n<li><a href=\"#communicative-goal\">Communicative Goal</a></li>\n<li><a href=\"#source-data\">Source Data</a>\n<ul>\n<li><a href=\"#initial-data-collection-and-normalization\">Initial Data Collection and Normalization</a></li>\n<li><a href=\"#who-are-the-source-language-producers\">Who are the source language producers?</a></li>\n</ul>\n</li>\n<li><a href=\"#annotations\">Annotations</a>\n<ul>\n<li><a href=\"#annotation-process\">Annotation process</a></li>\n<li><a href=\"#who-are-the-annotators\">Who are the annotators?</a></li>\n</ul>\n</li>\n<li><a href=\"#personal-and-sensitive-information\">Personal and Sensitive Information</a></li>\n</ul>\n</li>\n<li><a href=\"#changes-to-the-original-dataset-for-gem\">Changes to the Original Dataset for GEM</a></li>\n<li><a href=\"#considerations-for-using-the-data\">Considerations for Using the Data</a>\n<ul>\n<li><a href=\"#social-impact-of-the-dataset\">Social Impact of the Dataset</a></li>\n<li><a href=\"#impact-on-underserved-communities\">Impact on Underserved Communities</a></li>\n<li><a href=\"#discussion-of-biases\">Discussion of Biases</a></li>\n<li><a href=\"#other-known-limitations\">Other Known Limitations</a></li>\n</ul>\n</li>\n<li><a href=\"#getting-started-with-in-depth-research-on-the-task\">Getting started with in-depth research on the task</a></li>\n</ul>\n<h2 id=\"dataset-description\">Dataset Description</h2>\n<ul>\n<li><strong>Homepage:</strong> None (See <strong>Repository</strong>)</li>\n<li><strong>Repository:</strong> <a href=\"https://github.com/chaojiang06/wiki-auto\">Wiki-Auto repository</a></li>\n<li><strong>Paper:</strong> <a href=\"https://www.aclweb.org/anthology/2020.acl-main.709.pdf\">Neural CRF Model for Sentence Alignment in Text Simplification</a></li>\n<li><strong>Point of Contact:</strong> <a href=\"jiang.1530@osu.edu\">Chao Jiang</a></li>\n</ul>\n<h3 id=\"dataset-and-task-summary\">Dataset and Task Summary</h3>\n<p>WikiAuto provides a set of aligned sentences from English Wikipedia and Simple English Wikipedia as a resource to train sentence simplification systems.</p>\n<p>The authors first crowd-sourced a set of manual alignments between sentences in a subset of the Simple English Wikipedia and their corresponding versions in English Wikipedia (this corresponds to the <code>manual</code> config in this version of the dataset), then trained a neural CRF system to predict these alignments.</p>\n<p>The trained alignment prediction model was then applied to the other articles in Simple English Wikipedia with an English counterpart to create a larger corpus of aligned sentences (corresponding to the <code>auto</code> and <code>auto_acl</code> configs here).</p>\n<h3 id=\"why-is-this-dataset-part-of-gem\">Why is this dataset part of GEM?</h3>\n<p>Wiki-Auto is the largest open text simplification dataset currently available. It is one of the two datasets for the text simplification task in GEM. It acts as the training set.</p>\n<h3 id=\"languages\">Languages</h3>\n<p>Wiki-Auto contains English text only (BCP-47: <code>en</code>). It is presented as a translation task where Wikipedia Simple English is treated as its own idiom. For a statement of what is intended (but not always observed) to constitute Simple English on this platform, see <a href=\"https://simple.wikipedia.org/wiki/Wikipedia:About#Simple_English\">Simple English in Wikipedia</a>.</p>\n<h2 id=\"meta-information\">Meta Information</h2>\n<h3 id=\"dataset-curators\">Dataset Curators</h3>\n<p>The dataset was created by Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu from Ohio State University. The research is based upon work supported in part by the NSF awards IIS-1755898 and IIS-1822754, ODNI and  IARPA  via the  BETTER  program contract 19051600004, ARO and DARPA via the Social-Sim program contract W911NF-17-C-0095, Figure Eight AI for Everyone Award, and Criteo Faculty Research Award to Wei Xu.</p>\n<h3 id=\"licensing-information\">Licensing Information</h3>\n<p>The dataset is not licensed by itself, but the source Wikipedia data is under a <code>cc-by-sa-3.0</code> license.</p>\n<h3 id=\"citation-information\">Citation Information</h3>\n<pre><code>@inproceedings{jiang-etal-2020-neural,\n    title = \"Neural {CRF} Model for Sentence Alignment in Text Simplification\",\n    author = \"Jiang, Chao  and\n      Maddela, Mounica  and\n      Lan, Wuwei  and\n      Zhong, Yang  and\n      Xu, Wei\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-main.709\",\n    doi = \"10.18653/v1/2020.acl-main.709\",\n    pages = \"7943--7960\",\n}\n\n</code></pre>\n<h3 id=\"leaderboard\">Leaderboard</h3>\n<p>There is no official leaderboard associated with Wiki-Auto.</p>\n<h2 id=\"dataset-structure\">Dataset Structure</h2>\n<h3 id=\"data-instances\">Data Instances</h3>\n<p>The data in all of the configurations look a little different.</p>\n<p>A <code>manual</code> config instance consists of a sentence from the Simple English Wikipedia article, one from the linked English Wikipedia article, IDs for each of them, and a label indicating whether they are aligned. Sentences on either side can be repeated so that the aligned sentences are in the same instances. For example:</p>\n<pre><code>{'alignment_label': 1,\n 'normal_sentence': 'The Local Government Act 1985 is an Act of Parliament in the United Kingdom.',\n 'normal_sentence_id': '0_66252-1-0-0',\n 'simple_sentence': 'The Local Government Act 1985 was an Act of Parliament in the United Kingdom.',\n 'simple_sentence_id': '0_66252-0-0-0'}\n</code></pre>\n<p>Is followed by</p>\n<pre><code>{'alignment_label': 0,\n 'normal_sentence': 'Its main effect was to abolish the six county councils of the metropolitan counties that had been set up in 1974, 11 years earlier, by the Local Government Act 1972, along with the Greater London Council that had been established in 1965.',\n 'normal_sentence_id': '0_66252-1-0-1',\n 'simple_sentence': 'The Local Government Act 1985 was an Act of Parliament in the United Kingdom.',\n 'simple_sentence_id': '0_66252-0-0-0'}\n</code></pre>\n<p>The <code>auto</code> config shows a pair of an English and corresponding Simple English Wikipedia as an instance, with an alignment at the paragraph and sentence level:</p>\n<pre><code>{'example_id': '0',\n 'normal': {'normal_article_content': {'normal_sentence': [\"Lata Mondal ( ; born: 16 January 1993, Dhaka) is a Bangladeshi cricketer who plays for the Bangladesh national women's cricket team.\",\n    'She is a right handed batter.',\n    'Mondal was born on January 16, 1993 in Dhaka, Bangladesh.',\n    \"Mondal made her ODI career against the Ireland women's cricket team on November 26, 2011.\",\n    \"Mondal made her T20I career against the Ireland women's cricket team on August 28, 2012.\",\n    \"In October 2018, she was named in Bangladesh's squad for the 2018 ICC Women's World Twenty20 tournament in the West Indies.\",\n    \"Mondal was a member of the team that won a silver medal in cricket against the China national women's cricket team at the 2010 Asian Games in Guangzhou, China.\"],\n   'normal_sentence_id': ['normal-41918715-0-0',\n    'normal-41918715-0-1',\n    'normal-41918715-1-0',\n    'normal-41918715-2-0',\n    'normal-41918715-3-0',\n    'normal-41918715-3-1',\n    'normal-41918715-4-0']},\n  'normal_article_id': 41918715,\n  'normal_article_title': 'Lata Mondal',\n  'normal_article_url': 'https://en.wikipedia.org/wiki?curid=41918715'},\n 'paragraph_alignment': {'normal_paragraph_id': ['normal-41918715-0'],\n  'simple_paragraph_id': ['simple-702227-0']},\n 'sentence_alignment': {'normal_sentence_id': ['normal-41918715-0-0',\n   'normal-41918715-0-1'],\n  'simple_sentence_id': ['simple-702227-0-0', 'simple-702227-0-1']},\n 'simple': {'simple_article_content': {'simple_sentence': [\"Lata Mondal (born: 16 January 1993) is a Bangladeshi cricketer who plays for the Bangladesh national women's cricket team.\",\n    'She is a right handed bat.'],\n   'simple_sentence_id': ['simple-702227-0-0', 'simple-702227-0-1']},\n  'simple_article_id': 702227,\n  'simple_article_title': 'Lata Mondal',\n  'simple_article_url': 'https://simple.wikipedia.org/wiki?curid=702227'}}\n</code></pre>\n<p>Finally, the <code>auto_acl</code> config was obtained by selecting the aligned pairs of sentences from <code>auto</code> to provide a ready-to-go aligned dataset to train a sequence-to-sequence system, so an instance is a single pair of sentences:</p>\n<pre><code>{'normal_sentence': 'In early work, Rutherford discovered the concept of radioactive half-life , the radioactive element radon, and differentiated and named alpha and beta radiation .\\n',\n 'simple_sentence': 'Rutherford discovered the radioactive half-life, and the three parts of radiation which he named Alpha, Beta, and Gamma.\\n'}\n</code></pre>\n<p>Thus, for training a text simplification model for GEM, the data with the <code>auto_acl</code> config can be directly used.</p>\n<h3 id=\"data-fields\">Data Fields</h3>\n<p>The data has the following field:</p>\n<ul>\n<li><code>normal_sentence</code>: a sentence from English Wikipedia.</li>\n<li><code>normal_sentence_id</code>: a unique ID for each English Wikipedia sentence. The last two dash-separated numbers correspond to the paragraph number in the article and the sentence number in the paragraph.</li>\n<li><code>simple_sentence</code>: a sentence from Simple English Wikipedia.</li>\n<li><code>simple_sentence_id</code>: a unique ID for each Simple English Wikipedia sentence. The last two dash-separated numbers correspond to the paragraph number in the article and the sentence number in the paragraph.</li>\n<li><code>alignment_label</code>: signifies whether a pair of sentences is aligned: labels are <code>>=1:aligned</code> and <code>0:notAligned</code></li>\n<li><code>paragraph_alignment</code>: a first step of alignment mapping English and Simple English paragraphs from linked articles</li>\n<li><code>sentence_alignment</code>: the full alignment mapping English and Simple English sentences from linked articles</li>\n</ul>\n<h3 id=\"data-statistics\">Data Statistics</h3>\n<p>In <code>auto</code>, the <code>part_2</code> split corresponds to the articles used in <code>manual</code>, and <code>part_1</code> has the rest of Wikipedia.</p>\n<p>The <code>manual</code> config is provided with a <code>train</code>/<code>dev</code>/<code>test</code> split with the following amounts of data:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Tain</th>\n<th>Dev</th>\n<th>Test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Total sentence pairs</td>\n<td>373801</td>\n<td>73249</td>\n<td>118074</td>\n</tr>\n<tr>\n<td>Aligned sentence pairs</td>\n<td>1889</td>\n<td>346</td>\n<td>677</td>\n</tr>\n</tbody>\n</table>\n<p>The <code>auto_acl</code> has 488,332 complex-sentence pairs that are to be used for training the model. The average sentence length for complex and simple sentences are 26.6 and 18.7 respectively.</p>\n<h2 id=\"dataset-creation\">Dataset Creation</h2>\n<h3 id=\"curation-rationale\">Curation Rationale</h3>\n<p>Wiki-Auto provides a new version of the Wikipedia corpus that is larger, contains 75% less defective pairs and has more complex rewrites than the previous WIKILARGE dataset.</p>\n<h3 id=\"communicative-goal\">Communicative Goal</h3>\n<p>The goal is to communicate the same information as the source sentence using simpler words and grammar.</p>\n<h3 id=\"source-data\">Source Data</h3>\n<h4 id=\"initial-data-collection-and-normalization\">Initial Data Collection and Normalization</h4>\n<p>The authors mention that they \"extracted 138,095 article pairs from the 2019/09 Wikipedia dump using an improved version of the <a href=\"https://github.com/attardi/wikiextractor\">WikiExtractor</a> library\". The <a href=\"https://spacy.io/\">SpaCy</a> library is used for sentence splitting.</p>\n<h4 id=\"who-are-the-source-language-producers\">Who are the source language producers?</h4>\n<p>The dataset uses language from Wikipedia: some demographic information is provided <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Who_writes_Wikipedia%3F\">here</a>.</p>\n<h3 id=\"annotations\">Annotations</h3>\n<h4 id=\"annotation-process\">Annotation process</h4>\n<p>Sentence alignment labels were crowdsourced for 500 randomly sampled document pairs (10,123 sentence pairs total). The authors pre-selected several alignment candidates from English Wikipedia for each Simple Wikipedia sentence based on various similarity metrics, then asked the crowd-workers to annotate these pairs. Finally, they trained their alignment model on this manually annotated dataset to obtain automatically aligned sentences (138,095 document pairs, 488,332 sentence pairs).</p>\n<h4 id=\"who-are-the-annotators\">Who are the annotators?</h4>\n<p>No demographic annotation is provided for the crowd workers. The <a href=\"https://www.figure-eight.com/\">Figure Eight</a> platform was used for the annotation process.</p>\n<h3 id=\"personal-and-sensitive-information\">Personal and Sensitive Information</h3>\n<p>Since the dataset is created from Wikipedia/Simple Wikipedia, all the information contained in the dataset is already in the public domain.</p>\n<h2 id=\"changes-to-the-original-dataset-for-gem\">Changes to the Original Dataset for GEM</h2>\n<p>No change is made to the original dataset.</p>\n<h2 id=\"considerations-for-using-the-data\">Considerations for Using the Data</h2>\n<h3 id=\"social-impact-of-the-dataset\">Social Impact of the Dataset</h3>\n<p>The dataset helps move forward the research towards text simplification by creating a larger and more accurate dataset. Progress in text simplification in turn has the potential to increase the accessibility of written documents to wider audiences.</p>\n<h3 id=\"impact-on-underserved-communities\">Impact on Underserved Communities</h3>\n<p>The dataset is in English, a language with an abundance of existing resources.</p>\n<h3 id=\"discussion-of-biases\">Discussion of Biases</h3>\n<p>The dataset may contain some social biases, as the input sentences are based on Wikipedia. Studies have shown that the English Wikipedia contains both gender biases <a href=\"https://research.tudelft.nl/en/publications/is-wikipedia-succeeding-in-reducing-gender-bias-assessing-changes\">(Schmahl et al., 2020)</a> and racial biases <a href=\"https://journals.sagepub.com/doi/pdf/10.1177/2378023118823946\">(Adams et al., 2019)</a>.</p>\n<h3 id=\"other-known-limitations\">Other Known Limitations</h3>\n<p>Since the data is created using an automatic alignment model (which is not perfect) there could be still some alignment issues in the data.</p>\n<h2 id=\"getting-started-with-in-depth-research-on-the-task\">Getting started with in-depth research on the task</h2>\n<p>The dataset can be downloaded from the original repository <a href=\"https://github.com/chaojiang06/wiki-auto\">(here)</a> by the authors or can also be used via <a href=\"https://huggingface.co/datasets/wiki_auto\">HuggingFace</a> and <a href=\"https://www.tensorflow.org/datasets/overview\">TFDS</a>.</p>\n<p>The dataset repository provided by the authors also contains instructions to load a transformer-based sequence-to-sequence model trained on the dataset. There are also other recent supervised (<a href=\"https://arxiv.org/abs/1910.02677\">Martin et al., 2019</a>, <a href=\"https://www.aclweb.org/anthology/N19-1317/\">Kriz et al., 2019</a>, <a href=\"https://www.aclweb.org/anthology/P19-1331/\">Dong et al., 2019</a>, <a href=\"https://www.aclweb.org/anthology/D17-1062/\">Zhang and Lapata, 2017</a>) and unsupervised (<a href=\"https://arxiv.org/abs/2005.00352v1\">Martin et al., 2020</a>, <a href=\"https://www.aclweb.org/anthology/2020.acl-main.707/\">Kumar et al., 2020</a>, <a href=\"https://www.aclweb.org/anthology/P19-1198/\">Surya et al., 2019</a>) text simplification models that can be used as baselines.</p>\n<p>The common metric used for automatic evaluation is SARI <a href=\"https://www.aclweb.org/anthology/Q16-1029/\">(Xu et al., 2016)</a>.</p>\n","title":"Wiki-Auto","type":"Simplification","motivation":"Wiki-Auto is the largest open text simplification dataset currently available. For GEM, Wiki-Auto acts as the training set."}},"__N_SSG":true}