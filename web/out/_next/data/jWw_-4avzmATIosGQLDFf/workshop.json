{"pageProps":{"workshopData":{"contentHtml":"<h1 id=\"user-content-gem2-workshop-generation-evaluation--metrics---acl-2025\">GEM2 Workshop: Generation, Evaluation &#x26; Metrics - ACL 2025</h1>\n<p>The fourth iteration of the Generation, Evaluation &#x26; Metrics (GEM) Workshop will be held as part of ACL, July 27â€“August 1st, 2025. This year weâ€™re planning a major upgrade to the workshop, which we dub GEM2, through the introduction of a large-scale prediction benchmark, encouraging researchers from all backgrounds to submit work on meaningful, efficient and robust evaluation of LLMs.</p>\n<h2 id=\"user-content-overview\">Overview</h2>\n<p>Evaluating large language models (LLMs) is challenging. Running LLMs over medium or large scale corpus can be prohibitively expensive; they are consistently shown to be highly sensitive to prompt phrasing, and it is hard to formulate metrics which differentiate and rank different LLMs in a meaningful way. Consequently, the validity of the results obtained over popular benchmarks such as HELM or MMLU, lead to brittle conclusions. We believe that meaningful, efficient, and robust evaluation is one of the cornerstones of the scientific method, and that achieving it should be a community-wide goal.</p>\n<p>In this workshop we seek innovative research relating to the evaluation of LLMs and language generation systems in general. This includes, but is not limited to, robust, reproducible and efficient evaluation metrics, as well as new approaches for collecting evaluation data which can help in better differentiating between different systems and understanding their current bottlenecks.</p>\n<p>To facilitate and spur research in this field we will publish a large dataset of 1B model predictions together with prompts and gold standard references. This dataset will go beyond reporting just the accuracy of a model on a given sample, and will also include various axes which identify how the prompt was created and which were found to affect performance (instruction template, few-shot examples, their order, delimiters, etc.), as well as any known information about the model (pre training corpora, type of instruction-tuning, different checkpoints, and more), and the annotated gold label. Through this dataset, researchers will be able to investigate key questions such as: Are larger models more robust across different prompting configurations? Are common enumerators (e.g., A/B, 1/2) less sensitive compared to rare ones (e.g., I/IV, #/$)? Which evaluation axes should be prioritized when testing with limited resources? Can we identify patterns distinguishing examples where models show high robustness (consistent answers across configurations) versus low robustness (varying answers)?</p>\n<p>We welcome submissions related, but not limited to, the following topics:</p>\n<ul>\n<li>ðŸ’Ž Automatic evaluation of generation systems (example, example, example)</li>\n<li>ðŸ’Ž Creating evaluation corpora and challenge sets (example, example, example)</li>\n<li>ðŸ’Ž Critiques of benchmarking efforts and responsibly measuring progress in LLMs (example, example)</li>\n<li>ðŸ’Ž Effective and/or efficient NLG methods that can be applied to a wide range of languages and/or scenarios (example, example, example)</li>\n<li>ðŸ’Ž Application and evaluation of LLMs interacting with external data and tools (example, example, example)</li>\n<li>ðŸ’Ž Evaluation of sociotechnical systems employing large language models (example)</li>\n<li>ðŸ’Ž Standardizing human evaluation and making it more robust (example, example, example)</li>\n</ul>\n<p>We further invite submissions that conduct in-depth analyses of outputs of existing systems, for example through error analyses, by applying new metrics, or by testing the system on new test sets. While we encourage the use of the infrastructure the organizing team has developed as part of the GEM benchmark, its use is not required.</p>\n<p>If you are interested, you can check out last year's workshop websites from ACL 2021, EMNLP 2022, and EMNLP 2023.</p>\n<h2 id=\"user-content-industrial-track---unleashing-the-power-of-nlp-bridging-the-gap-between-academia-and-industry\">Industrial Track - Unleashing the Power of NLP: Bridging the Gap between Academia and Industry</h2>\n<p>Following the success of last iterations, GEM2 will hold an Industrial Track, which aims to provide actionable insights to industry professionals and to foster collaborations between academia and industry. This track will address the unique challenges faced by non-academic colleagues, highlighting the differences in evaluation practices between academic and industrial research, and explore the challenges in evaluating generative models with real-world data.</p>\n<p>The Industrial Track invites submissions covering the following topics, including (but not limited to):</p>\n<ul>\n<li>ðŸ’Ž Breaking Barriers: Bridging the Gap between Academic and Industrial Research (example)</li>\n<li>ðŸ’Ž From Data Diversity to Model Robustness: Challenges in Evaluating Generative Models with Real-World Data (example)</li>\n<li>ðŸ’Ž Beyond Metrics: Evaluating Generative Models for Real-World Business Impact (example, example, example)</li>\n</ul>\n<h2 id=\"user-content-how-to-submit\">How to submit?</h2>\n<p>Submissions can take either of the following forms:</p>\n<ul>\n<li>ðŸ’Ž <strong>Archival Papers</strong> describing original and unpublished work can be submitted in a between 4 and 8 page format.</li>\n<li>ðŸ’Ž <strong>Non-Archival Abstracts</strong> To discuss work already presented or under review at a peer-reviewed venue, we allow the submission of 2-page abstracts.</li>\n</ul>\n<p>All submissions are allowed unlimited space for references and appendices and should conform to ACL 2025 style guidelines. Archival paper submissions must be anonymized while abstract submissions may include author information. Final versions of accepted papers will be allowed 1 additional page of content so that reviewer comments can be taken into account.</p>\n<p>Papers should be submitted directly through OpenReview, selecting the appropriate track. We additionally welcome presentations by authors of papers in the Findings of the ACL. The selection process is managed centrally by the workshop chairs for the conference and we thus cannot respond to individual inquiries about Findings papers. However, we will try our best to accommodate authorsâ€™ requests.</p>\n<h2 id=\"user-content-important-dates\">Important Dates</h2>\n<p>Note: For any questions, please email <a href=\"mailto:gem-benchmark-chairs@googlegroups.com\">gem-benchmark-chairs@googlegroups.com</a>.</p>\n<p><strong>Paper Submission Dates</strong></p>\n<ul>\n<li>ðŸ“… <strong>April 11:</strong> Direct paper submission deadline (ARR).</li>\n<li>ðŸ“… <strong>May 5:</strong> Pre-reviewed (ARR) commitment deadline.</li>\n<li>ðŸ“… <strong>May 19:</strong> Notification of acceptance.</li>\n<li>ðŸ“… <strong>June 6:</strong> Camera-ready paper deadline.</li>\n<li>ðŸ“… <strong>July 7:</strong> Pre-recorded videos due.</li>\n<li>ðŸ“… <strong>July 31 - August 1:</strong> Workshop at ACL in Vienna.</li>\n</ul>\n"}},"__N_SSG":true}