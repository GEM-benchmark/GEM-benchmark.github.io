{"pageProps":{"taskData":{"contentHtml":"<p>This tutorial presents a full walk-through on how to get started with GEM, how to load and inspect data, how to finetune a baseline model, and how to generate predictions.\r\nThroughout this tutorial, we will focus on the CommonGen task, but we will note\r\nwhat changes to make to use another of the <a href=\"/data_cards\">GEM datasets</a>.</p>\n<p>You can also run this tutorial as a notebook <a href=\"https://colab.research.google.com/drive/1iREkGABObpdluTBNAnLhvyABEtdbLokT?usp=sharing\">here</a>.</p>\n<p><strong>SUBMITTING</strong> Our <a href=\"https://forms.gle/vbTZDMCuqzok8tTA9\">submission form</a> is permanently open! Please account for some extra time to write your model card.</p>\n<h2 id=\"user-content-table-of-contents\">Table of Contents</h2>\n<ul>\n<li><a href=\"#preliminaries\">Preliminaries</a></li>\n<li><a href=\"#loading-the-data\">Loading the data</a>\n<ul>\n<li><a href=\"#loading-a-single-example\">Loading a single example</a></li>\n</ul>\n</li>\n<li><a href=\"#finetuning-a-pretrained-model\">Finetuning a pretrained model</a></li>\n<li><a href=\"#generating-and-evaluating-predictions\">Generating and evaluating predictions</a></li>\n<li><a href=\"#generating-and-submitting-test-predictions\">Generating and submitting test predictions</a>\n<ul>\n<li><a href=\"#format-description\">Format description</a></li>\n<li><a href=\"#formatting-your-predictions\">Formatting Your Predictions</a></li>\n</ul>\n</li>\n<li><a href=\"#evaluating-your-submission-file-with-the-gem-evaluation-framework\">Evaluating your submission file with the GEM evaluation framework</a></li>\n</ul>\n<h2 id=\"user-content-preliminaries\">Preliminaries</h2>\n<p>This tutorial uses PyTorch and the HuggingFace infrastructure to finetune models.\r\nYou need to install the following dependencies:</p>\n<pre><code>pip install git+https://github.com/huggingface/datasets.git\r\npip install rouge_score\r\npip install sentencepiece\r\npip install transformers</code></pre>\n<p>We recommend you use a GPU machine. You should be able to run all the code inside of a <a href=\"https://colab.research.google.com/\">colab notebook for free GPU access</a>.</p>\n<h2 id=\"user-content-loading-the-data\">Loading the data</h2>\n<p>We will be using <a href=\"https://huggingface.co/datasets/gem\">HuggingFace datasets</a>, but the GEM datasets are available in <a href=\"https://www.tensorflow.org/datasets\">TFDS</a> as well.</p>\n<p>You can load and inspect datasets like this:</p>\n<pre><code>>> <span>from</span> datasets <span>import</span> load_dataset\r\n>> DATASET_NAME = <span>\"common_gen\"</span>\r\n>> data = load_dataset(<span>\"gem\"</span>, DATASET_NAME)\r\n>> data\r\n\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: [<span>'gem_id'</span>, <span>'gem_parent_id'</span>, <span>'concept_set_id'</span>, <span>'concepts'</span>, <span>'target'</span>, <span>'references'</span>],\r\n        num_rows: <span>67389</span>\r\n    })\r\n    validation: Dataset({\r\n        features: [<span>'gem_id'</span>, <span>'gem_parent_id'</span>, <span>'concept_set_id'</span>, <span>'concepts'</span>, <span>'target'</span>, <span>'references'</span>],\r\n        num_rows: <span>993</span>\r\n    })\r\n    test: Dataset({\r\n        features: [<span>'gem_id'</span>, <span>'gem_parent_id'</span>, <span>'concept_set_id'</span>, <span>'concepts'</span>, <span>'target'</span>, <span>'references'</span>],\r\n        num_rows: <span>1497</span>\r\n    })\r\n    challenge_train_sample: Dataset({\r\n        features: [<span>'gem_id'</span>, <span>'gem_parent_id'</span>, <span>'concept_set_id'</span>, <span>'concepts'</span>, <span>'target'</span>, <span>'references'</span>],\r\n        num_rows: <span>500</span>\r\n    })\r\n    challenge_validation_sample: Dataset({\r\n        features: [<span>'gem_id'</span>, <span>'gem_parent_id'</span>, <span>'concept_set_id'</span>, <span>'concepts'</span>, <span>'target'</span>, <span>'references'</span>],\r\n        num_rows: <span>500</span>\r\n    })\r\n    challenge_test_scramble: Dataset({\r\n        features: [<span>'gem_id'</span>, <span>'gem_parent_id'</span>, <span>'concept_set_id'</span>, <span>'concepts'</span>, <span>'target'</span>, <span>'references'</span>],\r\n        num_rows: <span>500</span>\r\n    })\r\n})</code></pre>\n<p>You can notice that challenge sets created as part of GEM act just like any other data split, which means that you can use them with exactly the same code!</p>\n<p>GEM supports many other datasets, simply pick one from this list and check out the corresponding <a href=\"/data_cards\">data cards</a>.</p>\n<h3 id=\"user-content-loading-a-single-example\">Loading a single example</h3>\n<p>Now let's look at a single example:</p>\n<pre><code>>> data[<span>'train'</span>][<span>0</span>]\r\n\r\n{\r\n    <span>'concept_set_id'</span>: <span>0</span>,\r\n    <span>'concepts'</span>: [<span>'mountain'</span>, <span>'ski'</span>, <span>'skier'</span>],\r\n    <span>'gem_id'</span>: <span>'common_gen-train-0'</span>,\r\n    <span>'references'</span>: [],\r\n    <span>'target'</span>: <span>'Skier skis down the mountain'</span>\r\n}</code></pre>\n<p>CommonGen is a task that asks for the production of a sentence (<code>target</code>) from a set of concepts (<code>concepts</code>). Since one concept set can generate multiple meaningful sentences, the example also includes a unique identifier (<code>concept_set_id</code>) so that multiple references can be linked to an input.</p>\n<p>Next, let's define utility functions that can generate batches of (tokenized) examples which we can use during training.</p>\n<p>We create a function that takes a batch from a dataset and constructs the corresponding input string. In our CommonGen example, we concatenate concepts into a single string for each instance.</p>\n<pre><code><span>def</span> <span>construct_input_for_batch</span>(<span>batch</span>):\r\n    <span>\"\"\"Construct input strings from a batch.\"\"\"</span>\r\n    source = [<span>' '</span>.join(concepts) <span>for</span> concepts <span>in</span> batch[<span>\"concepts\"</span>]]\r\n    target = batch[<span>\"target\"</span>]\r\n    <span>return</span> source, target</code></pre>\n<p>We then create a function that tokenizes the batches. Depending on your task, you might want to consider adjusting the <code>max_length</code>.</p>\n<pre><code><span>def</span> <span>batch_tokenize</span>(<span>batch, tokenizer, max_length=<span>32</span></span>):\r\n    <span>\"\"\"Construct the batch (source, target) and run them through a tokenizer.\"\"\"</span>\r\n    source, target = construct_input_for_batch(batch)\r\n    res = {\r\n        <span>\"input_ids\"</span>: tokenizer(source)[<span>\"input_ids\"</span>],\r\n        <span>\"labels\"</span>: tokenizer(\r\n            target,\r\n            padding=<span>\"max_length\"</span>,\r\n            truncation=<span>True</span>,\r\n            max_length=max_length\r\n        )[<span>\"input_ids\"</span>],\r\n    }\r\n    <span>return</span> res</code></pre>\n<p>All we need to do now to preprocess the dataset is to call <code>batch_tokenize</code> on it. For our example, we are using BART-base as a model and we need to load the corresponding tokenizer.</p>\n<pre><code><span>from</span> transformers <span>import</span> AutoTokenizer\r\n\r\nMODEL_NAME = <span>\"facebook/bart-base\"</span>\r\nMAX_LENGTH = <span>32</span>\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\r\n\r\ntrain_data_tokenized = data[<span>'train'</span>].<span>map</span>(\r\n    <span>lambda</span> batch: batch_tokenize(batch, tokenizer, max_length=MAX_LENGTH),\r\n    batched=<span>True</span>\r\n)\r\nvalid_data_tokenized = data[<span>'validation'</span>].<span>map</span>(\r\n    <span>lambda</span> batch: batch_tokenize(batch, tokenizer, max_length=MAX_LENGTH),\r\n    batched=<span>True</span>\r\n)</code></pre>\n<h2 id=\"user-content-finetuning-a-pretrained-model\">Finetuning a pretrained model</h2>\n<p>We can now utilize the preprocessed data to finetune a model. To do so, we will use the <a href=\"https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainingarguments\">Trainer API</a> which handles gradient updates, model selection, and evaluation for us.</p>\n<pre><code><span>from</span> transformers <span>import</span> AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments</code></pre>\n<p>To improve model selection, let's pick the model that has the best test performance on ROUGE-2, a metric that is typically associated with higher fluency. We can do this by constructing a function that returns a function that computes the score and we only have to pass it to our trainer.</p>\n<pre><code><span>from</span> datasets <span>import</span> load_metric\r\n\r\nrouge_scorer = load_metric(<span>\"rouge\"</span>)\r\n\r\n<span>def</span> <span>rouge_metric_builder</span>(<span>tokenizer</span>):\r\n    <span>def</span> <span>compute_rouge_metrics</span>(<span>pred</span>):\r\n        <span>\"\"\"Utility to compute ROUGE during training.\"\"\"</span>\r\n        labels_ids = pred.label_ids\r\n        pred_ids = pred.predictions\r\n        <span># All special tokens are removed.</span>\r\n        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=<span>True</span>)\r\n        labels_ids[labels_ids == -<span>100</span>] = tokenizer.pad_token_id\r\n        label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=<span>True</span>)\r\n        <span># Compute the metric.</span>\r\n        rouge_results = rouge_scorer.compute(\r\n            predictions=pred_str,\r\n            references=label_str,\r\n            rouge_types=[<span>\"rouge2\"</span>, <span>\"rougeL\"</span>],\r\n            use_agregator=<span>True</span>,\r\n            use_stemmer=<span>False</span>,\r\n        )\r\n        <span>return</span> {\r\n            <span>\"rouge2\"</span>: <span>round</span>(rouge_results[<span>'rouge2'</span>].mid.fmeasure, <span>4</span>),\r\n            <span>\"rougeL\"</span>: <span>round</span>(rouge_results[<span>'rougeL'</span>].mid.fmeasure, <span>4</span>),\r\n        }\r\n    <span>return</span> compute_rouge_metrics\r\n\r\nrouge_metric_fn = rouge_metric_builder(tokenizer)</code></pre>\n<p>We load our model and set some parameters for training and generating.</p>\n<pre><code><span>import</span> torch\r\n\r\nDEVICE = <span>\"cuda:0\"</span> <span>if</span> torch.cuda.is_available() <span>else</span> <span>\"cpu\"</span>\r\nRANDOM_SEED = <span>42</span>\r\nBEAM_SIZE = <span>4</span>\r\n\r\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\r\nmodel = model.to(DEVICE)</code></pre>\n<p>Fantastic, now all we have to do is set up our trainer class with everything we defined so far and run it!</p>\n<pre><code>train_args = Seq2SeqTrainingArguments(\r\n    output_dir=<span>\"BART-commongen\"</span>,\r\n    evaluation_strategy=<span>\"epoch\"</span>, \r\n    save_strategy=<span>\"epoch\"</span>,\r\n    logging_steps=<span>100</span>,\r\n    <span># optimization args, the trainer uses the Adam optimizer</span>\r\n    <span># and has a linear warmup for the learning rate</span>\r\n    per_device_train_batch_size=<span>32</span>,\r\n    per_device_eval_batch_size=<span>32</span>,\r\n    gradient_accumulation_steps=<span>1</span>,\r\n    learning_rate=<span>1e-04</span>,\r\n    num_train_epochs=<span>3</span>,\r\n    warmup_steps=<span>1000</span>,\r\n    <span># misc args</span>\r\n    seed=RANDOM_SEED,\r\n    disable_tqdm=<span>False</span>,\r\n    load_best_model_at_end=<span>True</span>,\r\n    metric_for_best_model=<span>\"rouge2\"</span>,\r\n    <span># generation</span>\r\n    predict_with_generate=<span>True</span>,\r\n)\r\n\r\ntrainer = Seq2SeqTrainer(\r\n    model=model,\r\n    args=train_args,\r\n    train_dataset=train_data_tokenized,\r\n    eval_dataset=valid_data_tokenized,\r\n    tokenizer=tokenizer,\r\n    compute_metrics=rouge_metric_fn,\r\n)\r\n\r\ntrainer._max_length = MAX_LENGTH\r\ntrainer._num_beams = BEAM_SIZE</code></pre>\n<p>And finally:</p>\n<pre><code>>> trainer.train()\r\n\r\nEpoch\tTraining Loss\tValidation Loss\tRouge2\t    Rougel\r\n<span>1</span>\t<span>0.953500</span>\t<span>1.113132</span>\t<span>0.122500</span>\t<span>0.322200</span>\r\n<span>2</span>\t<span>0.825300</span>\t<span>1.132310</span>\t<span>0.133800</span>\t<span>0.324600</span>\r\n<span>3</span>\t<span>0.709400</span>\t<span>1.133418</span>\t<span>0.129300</span>\t<span>0.324700</span></code></pre>\n<p>We now have a model that achieves 12.9 ROUGE-2 which can obviously still be tuned, but it is a great starting point.</p>\n<h2 id=\"user-content-generating-and-evaluating-predictions\">Generating and evaluating predictions</h2>\n<p>Given that we now have our fine-tuned model, we can use it to generate outputs for evaluation. For this, let's build another utility function that handles tokenizing, generating with beam search decoding, and de-tokenizing.</p>\n<pre><code><span>def</span> <span>beam_generate_sentences</span>(<span>\r\n    batch,\r\n    model,\r\n    tokenizer,\r\n    num_beams=<span>4</span>,\r\n    max_length=<span>32</span>,\r\n    device=<span>'cuda:0'</span>\r\n</span>):\r\n    <span>\"\"\"Generate outputs from a model with beam search decoding.\"\"\"</span>\r\n    <span># Create batch inputs.</span>\r\n    source, _ = construct_input_for_batch(batch)\r\n    <span># Use the model's tokenizer to create the batch input_ids.</span>\r\n    batch_features = tokenizer(source, padding=<span>True</span>, return_tensors=<span>'pt'</span>)\r\n    <span># Move all inputs to the device.</span>\r\n    batch_features = <span>dict</span>([(k, v.to(device)) <span>for</span> k, v <span>in</span> batch_features.items()])\r\n\r\n    <span># Generate with beam search.</span>\r\n    generated_ids = model.generate(\r\n        **batch_features,\r\n        num_beams=num_beams,\r\n        max_length=max_length,\r\n    )\r\n\r\n    <span># Use model tokenizer to decode to text.</span>\r\n    generated_sentences = [\r\n        tokenizer.decode(gen_ids.tolist(), skip_special_tokens=<span>True</span>)\r\n        <span>for</span> gen_ids <span>in</span> generated_ids\r\n    ]\r\n    <span>return</span> generated_sentences</code></pre>\n<p>We can quickly apply this function across our validation set as a sanity check.</p>\n<pre><code>valid_output = data[<span>'validation'</span>].<span>map</span>(\r\n    <span>lambda</span> batch: {<span>'generated'</span>: beam_generate_sentences(\r\n        batch,\r\n        model,\r\n        tokenizer,\r\n        num_beams=BEAM_SIZE,\r\n        max_length=MAX_LENGTH,\r\n        device=DEVICE)\r\n    },\r\n    batched=<span>True</span>,\r\n    batch_size=<span>128</span>,\r\n)\r\n\r\n<span># Evaluate for ROUGE-2/L</span>\r\nrouge_results = rouge_scorer.compute(\r\n    predictions=valid_output[<span>\"generated\"</span>],\r\n    references=valid_output[<span>\"target\"</span>],\r\n    rouge_types=[<span>\"rouge2\"</span>, <span>\"rougeL\"</span>],\r\n    use_agregator=<span>True</span>, use_stemmer=<span>False</span>,\r\n)\r\n\r\n<span>f\"R-2: <span>{rouge_results[<span>'rouge2'</span>].mid.fmeasure:<span>.3</span>f}</span> R-L: <span>{rouge_results[<span>'rougeL'</span>].mid.fmeasure:<span>.3</span>f}</span>\"</span></code></pre>\n<p>As expected, this yields the following output:</p>\n<pre><code><span>'R-2: 0.134 R-L: 0.325'</span></code></pre>\n<h2 id=\"user-content-generating-and-submitting-test-predictions\">Generating and submitting test predictions</h2>\n<p>You can submit your model along with test predictions via our <a href=\"https://forms.gle/vbTZDMCuqzok8tTA9\">submission form</a>.</p>\n<h3 id=\"user-content-format-description\">Format description</h3>\n<p>Please follow this format for your submission file:</p>\n<pre><code><span>{</span>\r\n  <span>\"submission_name\"</span><span>:</span> <span>\"An identifying name of your system\"</span><span>,</span>\r\n  <span>\"param_count\"</span><span>:</span> <span>123</span><span>,</span> # the number of parameters your system has.\r\n  <span>\"description\"</span><span>:</span> <span>\"An optional brief description of the system that will be shown on the website\"</span><span>,</span>\r\n  <span>\"tasks\"</span><span>:</span>\r\n    <span>{</span>\r\n      <span>\"dataset_identifier\"</span><span>:</span> <span>{</span>\r\n        <span>\"values\"</span><span>:</span> <span>[</span><span>\"output1\"</span><span>,</span> <span>\"output2\"</span><span>,</span> <span>\"...\"</span><span>]</span><span>,</span> # A list of system outputs\r\n        # Optionally<span>,</span> you can add the keys which are part of an example to ensure that there is no shuffling mistakes.\r\n        <span>\"keys\"</span><span>:</span> <span>[</span><span>\"schema_guided_dialog-test-9585\"</span><span>,</span> <span>\"schema_guided_dialog-test-9585\"</span><span>,</span> ...<span>]</span>\r\n        <span>}</span>\r\n    <span>}</span>\r\n<span>}</span>\r\n</code></pre>\n<p>In this case, <code>dataset_identifier</code> is the identifier of the dataset followed by an identifier of the set the outputs were created from, for example <code>_validation</code> or <code>_test</code>. That means, the common_gen validation set would have the identifier <code>common_gen_validation</code>.</p>\n<p>The <code>keys</code> field can be set to avoid accidental shuffling to impact your metrics. Simply add a list of the <code>gem_id</code> for each output example in the same order as your values.</p>\n<h3 id=\"user-content-formatting-your-predictions\">Formatting Your Predictions</h3>\n<p>For our tutorial, let's say we want to include results for the validation set and challenge set (<code>common_gen_challenge_train_sample</code>) outputs.</p>\n<pre><code>challenge_train_sample_output = data[<span>\"challenge_train_sample\"</span>].<span>map</span>(\r\n    <span>lambda</span> batch: {\r\n        <span>'generated'</span>: beam_generate_sentences(\r\n            batch,\r\n            model,\r\n            tokenizer,\r\n            num_beams=BEAM_SIZE,\r\n            max_length=MAX_LENGTH,\r\n            device=DEVICE)\r\n    },\r\n    batched=<span>True</span>,\r\n    batch_size=<span>128</span>,\r\n)</code></pre>\n<p>We add a <code>generated</code> field into the dataset which makes analysis much easier. However, in our submission file we only want the actual values and corresponding IDs. Thus, we filter:</p>\n<pre><code>valid_formatted = [o[<span>'generated'</span>] <span>for</span> o <span>in</span> valid_output]\r\nvalid_keys = [o[<span>'gem_id'</span>] <span>for</span> o <span>in</span> data[<span>'validation'</span>]]\r\n\r\nchallenge_train_sample_formatted = [o[<span>'generated'</span>] <span>for</span> o <span>in</span> challenge_train_sample_output]\r\nchallenge_train_sample_keys = [o[<span>'gem_id'</span>] <span>for</span> o <span>in</span> data[<span>'challenge_train_sample'</span>]]</code></pre>\n<p>In our final step, we only have to add the outputs to our larger submission construct.</p>\n<pre><code>SUBMISSION_NAME = <span>\"An identifying name of your system\"</span>\r\nDESCRIPTION = <span>\"An optional brief description of the system that will be shown on the website\"</span>\r\n\r\nsubmission_dict = {\r\n    <span>\"submission_name\"</span>: SUBMISSION_NAME ,\r\n    <span>\"param_count\"</span>: <span>sum</span>(p.numel() <span>for</span> p <span>in</span> model.parameters()),\r\n    <span>\"description\"</span>: DESCRIPTION,\r\n    <span>\"tasks\"</span>: {\r\n      <span>\"common_gen_validation\"</span>: {\r\n          <span>\"values\"</span>: valid_formatted, \r\n          <span>\"keys\"</span>: valid_keys\r\n          }\r\n    }\r\n}</code></pre>\n<p>This format is scalable to more tasks: you simply need to add more outputs to the <code>tasks</code> subfield.</p>\n<pre><code><span># Submit results for challenge set.</span>\r\nnew_task_name = <span>\"common_gen_challenge_train_sample\"</span>\r\nnew_task_data = {\r\n    <span>\"values\"</span>: challenge_train_sample_formatted, \r\n    <span>\"keys\"</span>: challenge_train_sample_keys\r\n} \r\nsubmission_dict[<span>\"tasks\"</span>][new_task_name] = new_task_data</code></pre>\n<p>The last step is to write our submission dictionary to a file.</p>\n<pre><code><span>import</span> json\r\n<span>with</span> <span>open</span>(<span>'gem_submission.json'</span>, <span>'w'</span>) <span>as</span> f:\r\n    f.write(json.dumps(submission_dict))</code></pre>\n<h2 id=\"user-content-evaluating-your-submission-file-with-the-gem-evaluation-framework\">Evaluating your submission file with the GEM evaluation framework</h2>\n<p>Obviously, we do not want to rely only on ROUGE scores. For this, we developed the GEM evaluation framework.</p>\n<p>You can download it by running:</p>\n<pre><code>git <span>clone</span> git@github.com:GEM-benchmark/GEM-metrics.git</code></pre>\n<p>Install the required packages:</p>\n<pre><code><span>cd</span> GEM-metrics\r\npip install -r requirements.txt</code></pre>\n<p>Assuming that you formatted and saved your outputs correctly, you can now run</p>\n<pre><code>python run_metrics.py [-r references.json] [-o outputs.scores.json] gem_submission.json</code></pre>\n<p>which will create a json file with your scores per task and challenge set. Please follow the <a href=\"https://github.com/GEM-benchmark/GEM-metrics\">README</a> for more detailed usage information.</p>\n","title":"From pretrained model to submission","type":"Modeling","background":"This tutorial shows the entire pipeline from loading the data, creating a model, to formatting the submission file from predictions."}},"__N_SSG":true}