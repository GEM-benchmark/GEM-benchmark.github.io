{"pageProps":{"taskData":{"id":"TURK","contentHtml":"<h2 id=\"user-content-table-of-contents\">Table of Contents</h2>\n<ul>\n<li><a href=\"#dataset-description\">Dataset Description</a>\n<ul>\n<li><a href=\"#dataset-and-task-summary\">Dataset and Task Summary</a></li>\n<li><a href=\"#why-is-this-dataset-part-of-gem\">Why is this dataset part of GEM?</a></li>\n<li><a href=\"#languages\">Languages</a></li>\n</ul>\n</li>\n<li><a href=\"#meta-information\">Meta Information</a>\n<ul>\n<li><a href=\"#dataset-curators\">Dataset Curators</a></li>\n<li><a href=\"#licensing-information\">Licensing Information</a></li>\n<li><a href=\"#citation-information\">Citation Information</a></li>\n<li><a href=\"#leaderboard\">Leaderboard</a></li>\n</ul>\n</li>\n<li><a href=\"#dataset-structure\">Dataset Structure</a>\n<ul>\n<li><a href=\"#data-instances\">Data Instances</a></li>\n<li><a href=\"#data-fields\">Data Fields</a></li>\n<li><a href=\"#data-statistics\">Data Statistics</a></li>\n</ul>\n</li>\n<li><a href=\"#dataset-creation\">Dataset Creation</a>\n<ul>\n<li><a href=\"#curation-rationale\">Curation Rationale</a></li>\n<li><a href=\"#communicative-goal\">Communicative Goal</a></li>\n<li><a href=\"#source-data\">Source Data</a>\n<ul>\n<li><a href=\"#initial-data-collection-and-normalization\">Initial Data Collection and Normalization</a></li>\n<li><a href=\"#who-are-the-source-language-producers\">Who are the source language producers?</a></li>\n</ul>\n</li>\n<li><a href=\"#annotations\">Annotations</a>\n<ul>\n<li><a href=\"#annotation-process\">Annotation process</a></li>\n<li><a href=\"#who-are-the-annotators\">Who are the annotators?</a></li>\n</ul>\n</li>\n<li><a href=\"#personal-and-sensitive-information\">Personal and Sensitive Information</a></li>\n</ul>\n</li>\n<li><a href=\"#changes-to-the-original-dataset-for-gem\">Changes to the Original Dataset for GEM</a>\n<ul>\n<li><a href=\"#special-test-sets\">Special test sets</a>\n<ul>\n<li><a href=\"#subpopulations\">Subpopulations</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#considerations-for-using-the-data\">Considerations for Using the Data</a>\n<ul>\n<li><a href=\"#social-impact-of-dataset\">Social Impact of Dataset</a></li>\n<li><a href=\"#impact-on-underserved-communities\">Impact on Underserved Communities</a></li>\n<li><a href=\"#discussion-of-biases\">Discussion of Biases</a></li>\n<li><a href=\"#other-known-limitations\">Other Known Limitations</a></li>\n</ul>\n</li>\n<li><a href=\"#getting-started-with-in-depth-research-on-the-task\">Getting started with in-depth research on the task</a></li>\n</ul>\n<h2 id=\"user-content-dataset-description\">Dataset Description</h2>\n<ul>\n<li><strong>Homepage:</strong> None</li>\n<li><strong>Repository:</strong> <a href=\"https://github.com/cocoxu/simplification\">TURKCorpus</a></li>\n<li><strong>Paper:</strong> <a href=\"https://aclanthology.org/Q16-1029/\">Optimizing Statistical Machine Translation for Text Simplification</a></li>\n<li><strong>Point of Contact:</strong> <a href=\"mailto:wei.xu@cc.gatech.edu\">Wei Xu</a></li>\n</ul>\n<h3 id=\"user-content-dataset-and-task-summary\">Dataset and Task Summary</h3>\n<p>TURKCorpus is a multi-reference dataset for the evaluation of sentence simplification in English. The dataset consists of 2,359 sentences from the <a href=\"https://www.aclweb.org/anthology/C10-1152/\">Parallel Wikipedia Simplification (PWKP) corpus</a>. Each sentence is associated with 8 crowdsourced simplifications that focus on only lexical paraphrasing (no sentence splitting or deletion).</p>\n<h3 id=\"user-content-why-is-this-dataset-part-of-gem\">Why is this dataset part of GEM?</h3>\n<p>TURKCorpus is a high quality simplification dataset where each source (not simple) sentence is associated with 8 human-written simplifications that focus on lexical paraphrasing. It is one of the two evaluation datasets for the text simplification task in GEM. It acts as the validation and test set for paraphrasing-based simplification that does not involve sentence splitting and deletion.</p>\n<h3 id=\"user-content-languages\">Languages</h3>\n<p>TURKCorpus contains English text only (BCP-47: <code>en</code>).</p>\n<h2 id=\"user-content-meta-information\">Meta Information</h2>\n<h3 id=\"user-content-dataset-curators\">Dataset Curators</h3>\n<p>TURKCorpus was developed by researchers at the University of Pennsylvania. The work was  supported by the NSF under grant IIS-1430651 and the NSF GRFP under grant 1232825.</p>\n<h3 id=\"user-content-licensing-information\">Licensing Information</h3>\n<p><a href=\"https://github.com/cocoxu/simplification/blob/master/LICENSE\">GNU General Public License v3.0</a></p>\n<h3 id=\"user-content-citation-information\">Citation Information</h3>\n<pre><code> @article{Xu-EtAl:2016:TACL,\n author = {Wei Xu and Courtney Napoles and Ellie Pavlick and Quanze Chen and Chris Callison-Burch},\n title = {Optimizing Statistical Machine Translation for Text Simplification},\n journal = {Transactions of the Association for Computational Linguistics},\n volume = {4},\n year = {2016},\n url = {https://cocoxu.github.io/publications/tacl2016-smt-simplification.pdf},\n pages = {401--415}\n }\n</code></pre>\n<h3 id=\"user-content-leaderboard\">Leaderboard</h3>\n<p>There is no official leaderboard.</p>\n<h2 id=\"user-content-dataset-structure\">Dataset Structure</h2>\n<h3 id=\"user-content-data-instances\">Data Instances</h3>\n<ul>\n<li><code>simplification</code> configuration: an instance consists of an original sentence and 8 possible reference simplifications that focus on lexical paraphrasing.</li>\n</ul>\n<h3 id=\"user-content-data-fields\">Data Fields</h3>\n<ul>\n<li><code>original</code>: an original sentence from the source datasets</li>\n<li><code>simplifications</code>:  a set of reference simplifications produced by crowd workers.</li>\n</ul>\n<h3 id=\"user-content-data-statistics\">Data Statistics</h3>\n<p>TURKCorpus does not contain a training set; many models use <a href=\"https://github.com/XingxingZhang/dress\">WikiLarge</a> (Zhang and Lapata, 2017) or <a href=\"https://github.com/chaojiang06/wiki-auto\">Wiki-Auto</a> (Jiang et. al 2020) for training.</p>\n<p>Each input sentence has 8 associated reference simplified sentences. 2,359 input sentences are randomly split into 2,000 validation and 359 test sentences.</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Dev</th>\n<th>Test</th>\n<th>Total</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Input Sentences</td>\n<td>2000</td>\n<td>359</td>\n<td>2359</td>\n</tr>\n<tr>\n<td>Reference Simplifications</td>\n<td>16000</td>\n<td>2872</td>\n<td>18872</td>\n</tr>\n</tbody>\n</table>\n<p>There are 21.29 tokens per reference on average.</p>\n<h2 id=\"user-content-dataset-creation\">Dataset Creation</h2>\n<h3 id=\"user-content-curation-rationale\">Curation Rationale</h3>\n<p>The TurkCorpus dataset was constructed to evaluate the task of text simplification.  It contains multiple human-written references that focus on only lexical simplification.</p>\n<h3 id=\"user-content-communicative-goal\">Communicative Goal</h3>\n<p>The goal is to communicate the main ideas of source sentence in a way that is easier to understand by non-native speakers of English.</p>\n<h3 id=\"user-content-source-data\">Source Data</h3>\n<h4 id=\"user-content-initial-data-collection-and-normalization\">Initial Data Collection and Normalization</h4>\n<p>The input sentences in the dataset are extracted from the <a href=\"https://www.aclweb.org/anthology/C10-1152/\">Parallel Wikipedia Simplification (PWKP) corpus</a>.</p>\n<h4 id=\"user-content-who-are-the-source-language-producers\">Who are the source language producers?</h4>\n<p>The references are crowdsourced from Amazon Mechanical Turk. The annotators were asked to provide simplifications without losing any information or splitting the input sentence. No other demographic or compensation information is provided in the TURKCorpus paper.</p>\n<h3 id=\"user-content-annotations\">Annotations</h3>\n<h4 id=\"user-content-annotation-process\">Annotation process</h4>\n<p>The instructions given to the annotators are available in the paper.</p>\n<h4 id=\"user-content-who-are-the-annotators\">Who are the annotators?</h4>\n<p>Reference sentences were written by workers on Amazon Mechanical Turk (AMT) with HIT approval rate over 95%. No other demographic or compensation information is provided in the paper.</p>\n<h3 id=\"user-content-personal-and-sensitive-information\">Personal and Sensitive Information</h3>\n<p>Since the dataset is created from English Wikipedia (August 22, 2009 version), all the information contained in the dataset is already in the public domain.</p>\n<h2 id=\"user-content-changes-to-the-original-dataset-for-gem\">Changes to the Original Dataset for GEM</h2>\n<p>The publicly available dataset is processed with the <a href=\"https://www.nltk.org/_modules/nltk/tokenize/treebank.html\">Penn Treebank tokenizer</a>. We reverse this process to make the format consistent with WikiAuto and ASSET.</p>\n<h3 id=\"user-content-special-test-sets\">Special test sets</h3>\n<h4 id=\"user-content-subpopulations\">Subpopulations</h4>\n<p>The goal was to assess performance when simplifying source sentences with different syntactic structure and complexity. To this end, we split the original test set according to syntactic complexity of the source sentences. To characterize sentence syntactic complexity, we use the 8-level developmental level (d-level) scale proposed by <a href=\"https://www.researchgate.net/publication/254033869_How_complex_is_that_sentence_A_proposed_revision_of_the_Rosenberg_and_Abbeduto_D-Level_Scale\">Covington et al. (2006)</a> and the implementation of <a href=\"https://www.jbe-platform.com/content/journals/10.1075/ijcl.15.4.02lu\">Lu, Xiaofei (2010)</a>.\nWe thus split the test set into 8 subsets corresponding to the 8 d-levels assigned to source sentences. We obtain the following number of instances per level and average d-level of the dataset:</p>\n<table>\n<thead>\n<tr>\n<th>Total nb. sentences</th>\n<th>L0</th>\n<th>L1</th>\n<th>L2</th>\n<th>L3</th>\n<th>L4</th>\n<th>L5</th>\n<th>L6</th>\n<th>L7</th>\n<th>Mean Level</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>359</td>\n<td>174</td>\n<td>0</td>\n<td>58</td>\n<td>22</td>\n<td>3</td>\n<td>30</td>\n<td>9</td>\n<td>63</td>\n<td>2.34</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"user-content-considerations-for-using-the-data\">Considerations for Using the Data</h2>\n<h3 id=\"user-content-social-impact-of-dataset\">Social Impact of Dataset</h3>\n<p>The dataset helps move forward the research towards text simplification by creating a higher quality validation and test dataset. Progress in text simplification in turn has the potential to increase the accessibility of written documents to wider audiences.</p>\n<h3 id=\"user-content-impact-on-underserved-communities\">Impact on Underserved Communities</h3>\n<p>The dataset is in English, a language with an abundance of existing resources.</p>\n<h3 id=\"user-content-discussion-of-biases\">Discussion of Biases</h3>\n<p>The dataset may contain some social biases, as the input sentences are based on Wikipedia. Studies have shown that the English Wikipedia contains both gender biases <a href=\"https://research.tudelft.nl/en/publications/is-wikipedia-succeeding-in-reducing-gender-bias-assessing-changes\">(Schmahl et al., 2020)</a> and racial biases <a href=\"https://journals.sagepub.com/doi/pdf/10.1177/2378023118823946\">(Adams et al., 2019)</a>.</p>\n<h3 id=\"user-content-other-known-limitations\">Other Known Limitations</h3>\n<p>Since the dataset contains only 2,359 sentences that are derived from Wikipedia, it is limited to a small subset of topics present on Wikipedia.</p>\n<h2 id=\"user-content-getting-started-with-in-depth-research-on-the-task\">Getting started with in-depth research on the task</h2>\n<p>The dataset can be downloaded from the original repository <a href=\"https://github.com/cocoxu/simplification\">(here)</a> by the authors or can also be used via <a href=\"https://huggingface.co/docs/datasets\">HuggingFace</a> and <a href=\"https://www.tensorflow.org/datasets/overview\">TFDS</a>.</p>\n<p>There are recent supervised (<a href=\"https://arxiv.org/abs/1910.02677\">Martin et al., 2019</a>, <a href=\"https://www.aclweb.org/anthology/N19-1317/\">Kriz et al., 2019</a>, <a href=\"https://www.aclweb.org/anthology/P19-1331/\">Dong et al., 2019</a>, <a href=\"https://www.aclweb.org/anthology/D17-1062/\">Zhang and Lapata, 2017</a>) and unsupervised (<a href=\"https://arxiv.org/abs/2005.00352v1\">Martin et al., 2020</a>, <a href=\"https://www.aclweb.org/anthology/2020.acl-main.707/\">Kumar et al., 2020</a>, <a href=\"https://www.aclweb.org/anthology/P19-1198/\">Surya et al., 2019</a>) text simplification models that can be used as baselines.</p>\n<p>The common metric used for automatic evaluation is SARI <a href=\"https://www.aclweb.org/anthology/Q16-1029/\">(Xu et al., 2016)</a>.</p>\n","title":"TURKCorpus","type":"Simplification","motivation":"TURKCorpus is a high-quality simplification dataset where each source sentence is associated with 8 human-written simplifications."}},"__N_SSG":true}