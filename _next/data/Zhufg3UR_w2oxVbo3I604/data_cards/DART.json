{"pageProps":{"taskData":{"id":"DART","contentHtml":"<h2 id=\"table-of-contents\">Table of Contents</h2>\n<ul>\n<li><a href=\"#dataset-description\">Dataset Description</a>\n<ul>\n<li><a href=\"#dataset-and-task-summary\">Dataset and Task Summary</a></li>\n<li><a href=\"#why-is-this-dataset-part-of-gem\">Why is this dataset part of GEM?</a></li>\n<li><a href=\"#languages\">Languages</a></li>\n</ul>\n</li>\n<li><a href=\"#meta-information\">Meta Information</a>\n<ul>\n<li><a href=\"#dataset-curators\">Dataset Curators</a></li>\n<li><a href=\"#licensing-information\">Licensing Information</a></li>\n<li><a href=\"#citation-information\">Citation Information</a></li>\n<li><a href=\"#leaderboard\">Leaderboard</a></li>\n</ul>\n</li>\n<li><a href=\"#dataset-structure\">Dataset Structure</a>\n<ul>\n<li><a href=\"#data-instances\">Data Instances</a></li>\n<li><a href=\"#data-fields\">Data Fields</a></li>\n<li><a href=\"#data-statistics\">Data Statistics</a></li>\n</ul>\n</li>\n<li><a href=\"#dataset-creation\">Dataset Creation</a>\n<ul>\n<li><a href=\"#curation-rationale\">Curation Rationale</a></li>\n<li><a href=\"#communicative-goal\">Communicative Goal</a></li>\n<li><a href=\"#source-data\">Source Data</a>\n<ul>\n<li><a href=\"#initial-data-collection-and-normalization\">Initial Data Collection and Normalization</a></li>\n<li><a href=\"#who-are-the-source-language-producers\">Who are the source language producers?</a></li>\n</ul>\n</li>\n<li><a href=\"#annotations\">Annotations</a>\n<ul>\n<li><a href=\"#annotation-process\">Annotation process</a></li>\n<li><a href=\"#who-are-the-annotators\">Who are the annotators?</a></li>\n</ul>\n</li>\n<li><a href=\"#personal-and-sensitive-information\">Personal and Sensitive Information</a></li>\n</ul>\n</li>\n<li><a href=\"#changes-to-the-original-dataset-for-gem\">Changes to the Original Dataset for GEM</a></li>\n<li><a href=\"#considerations-for-using-the-data\">Considerations for Using the Data</a>\n<ul>\n<li><a href=\"#social-impact-of-the-dataset\">Social Impact of the Dataset</a></li>\n<li><a href=\"#impact-on-underserved-communities\">Impact on Underserved Communities</a></li>\n<li><a href=\"#discussion-of-biases\">Discussion of Biases</a></li>\n<li><a href=\"#other-known-limitations\">Other Known Limitations</a></li>\n</ul>\n</li>\n<li><a href=\"#getting-started-with-in-depth-research-on-the-task\">Getting started with in-depth research on the task</a></li>\n</ul>\n<h2 id=\"dataset-description\">Dataset Description</h2>\n<ul>\n<li><strong>Homepage:</strong> None (See <strong>Repository</strong>)</li>\n<li><strong>Repository:</strong> <a href=\"https://github.com/Yale-LILY/dart\">DART repository</a></li>\n<li><strong>Paper:</strong> <a href=\"https://arxiv.org/abs/2007.02871\">DART: Open-Domain Structured Data Record to Text Generation</a></li>\n<li><strong>Point of Contact:</strong> {dragomir.radev, r.zhang}@yale.edu, {nazneen.rajani}@salesforce.com</li>\n</ul>\n<h3 id=\"dataset-and-task-summary\">Dataset and Task Summary</h3>\n<p>DART is a large and open-domain structured DAta Record to Text generation corpus with high-quality sentence annotations with each input being a set of entity-relation triples following a tree-structured ontology. It consists of 82191 examples across different domains with each input being a semantic RDF triple set derived from data records in tables and the tree ontology of table schema, annotated with sentence description that covers all facts in the triple set.</p>\n<h3 id=\"why-is-this-dataset-part-of-gem\">Why is this dataset part of GEM?</h3>\n<p>DART is one of the two datasets representing Table-to-Text NLG in GEM.</p>\n<h3 id=\"languages\">Languages</h3>\n<p>DART contains English text only (BCP-47: en).</p>\n<h2 id=\"meta-information\">Meta Information</h2>\n<h3 id=\"dataset-curators\">Dataset Curators</h3>\n<p>The dataset was curated by a joint team of researchers from the Yale University, Salesforce Research, the University of Hong Kong, MIT and the University of the Chinese Academy of Sciences.\n{dragomir.radev, r.zhang}@yale.edu, {nazneen.rajani}@salesforce.com</p>\n<h3 id=\"licensing-information\">Licensing Information</h3>\n<p>The dataset was obtained by using multiple complementary methods: (1) human annotation on open-domain Wikipedia tables from WikiTableQuestions (<a href=\"https://www.aclweb.org/anthology/P15-1142.pdf\">Pasupat and Liang, 2015</a>) and WikiSQL (<a href=\"https://arxiv.org/pdf/1709.00103.pdf\">Zhong et al., 2017</a>), (2) automatic conversion of questions in WikiSQL to declarative sentences and (3) incorporation of existing datasets including WebNLG 2017 (Gardent et al., 2017<a href=\"https://www.aclweb.org/anthology/P17-1017.pdf\">a</a>,<a href=\"https://www.aclweb.org/anthology/W17-3518.pdf\">b</a>; <a href=\"https://www.aclweb.org/anthology/W18-6543.pdf\">Shimorina and Gardent, 2018</a>) and Cleaned E2E (<a href=\"https://arxiv.org/pdf/1706.09254.pdf\">Novikova et al., 2017b</a>; Dušek et al., <a href=\"https://arxiv.org/pdf/1810.01170.pdf\">2018</a>, <a href=\"https://www.aclweb.org/anthology/W19-8652.pdf\">2019</a>)</p>\n<p>The repository code is under an <a href=\"https://github.com/Yale-LILY/dart/blob/master/LICENSE\">MIT license</a>.</p>\n<h3 id=\"citation-information\">Citation Information</h3>\n<p>@article{radev2020dart,\ntitle={DART: Open-Domain Structured Data Record to Text Generation},\nauthor={Dragomir Radev and Rui Zhang and Amrit Rau and Abhinand Sivaprasad and Chiachun Hsieh and Nazneen Fatema Rajani and Xiangru Tang and Aadit Vyas and Neha Verma and Pranav Krishna and Yangxiaokang Liu and Nadia Irwanto and Jessica Pan and Faiaz Rahman and Ahmad Zaidi and Murori Mutuma and Yasin Tarabar and Ankit Gupta and Tao Yu and Yi Chern Tan and Xi Victoria Lin and Caiming Xiong and Richard Socher},\njournal={arXiv preprint arXiv:2007.02871},\nyear={2020}\n}</p>\n<h3 id=\"leaderboard\">Leaderboard</h3>\n<p>The dataset supports an active leaderboard, the best results are tracked <a href=\"https://github.com/Yale-LILY/dart#leaderboard\">here</a>. Several state-of-the-art table-to-text models were evaluated on DART, such as BART (<a href=\"https://arxiv.org/pdf/1910.13461.pdf\">Lewis et al., 2020</a>), Seq2Seq-Att (<a href=\"https://webnlg-challenge.loria.fr/files/melbourne_report.pdf\">MELBOURNE</a>) and End-to-End Transformer (<a href=\"https://arxiv.org/pdf/1908.09022.pdf\">Castro Ferreira et al., 2019</a>).\nThe leaderboard reports BLEU, METEOR, TER, MoverScore, BERTScore and BLEURT scores.</p>\n<h2 id=\"dataset-structure\">Dataset Structure</h2>\n<h3 id=\"data-instances\">Data Instances</h3>\n<p>The DART dataset is available in the data/ directory. The dataset consists of JSON files in data/. Each JSON file contains a list of tripleset-annotation pairs of the form:\n{\n\"tripleset\": [\n[\n\"Ben Mauk\",\n\"High school\",\n\"Kenton\"\n],\n[\n\"Ben Mauk\",\n\"College\",\n\"Wake Forest Cincinnati\"\n]\n],\n\"subtree_was_extended\": false,\n\"annotations\": [\n{\n\"source\": \"WikiTableQuestions_lily\",\n\"text\": \"Ben Mauk, who attended Kenton High School, attended Wake Forest Cincinnati for college.\"\n}\n]\n}</p>\n<p>Creators provided delexicalization dictionaries in data/**/delex/ that map string entities to entity categories.</p>\n<h3 id=\"data-fields\">Data Fields</h3>\n<p>tripleset: a list of tuples, each tuple has 3 items\nsubtree_was_extended: a boolean variable (true or false)\nannotations: a list of dict, each with source and text keys.\nsource: a string mentioning the name of the source table.\ntext: a sentence string.</p>\n<h3 id=\"data-statistics\">Data Statistics</h3>\n<table>\n<thead>\n<tr>\n<th>Input Unit</th>\n<th>Examples</th>\n<th>Vocab Size</th>\n<th>Words per SR</th>\n<th>Sents per SR</th>\n<th>Tables</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Triple Set</td>\n<td>82,191</td>\n<td>33.2K</td>\n<td>21.6</td>\n<td>1.5</td>\n<td>5,623</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>Train</th>\n<th>Dev</th>\n<th>Test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>62,659</td>\n<td>6,980</td>\n<td>12,552</td>\n</tr>\n</tbody>\n</table>\n<p>Statistics of DART decomposed by different collection methods. DART exhibits a great deal of topical variety in terms of the number of unique predicates, the number of unique triples, and the vocabulary size. These statistics are computed from DART v1.1.1; the number of unique predicates reported is post-unification (see Section 3.4). SR: Surface Realization.\n(<a href=\"https://arxiv.org/pdf/2007.02871.pdf\">details in Table 1 and 2</a>).</p>\n<h2 id=\"dataset-creation\">Dataset Creation</h2>\n<h3 id=\"curation-rationale\">Curation Rationale</h3>\n<p>The dataset creators encourage through DART further research in natural language generation from semantic data.\nDART provides high-quality sentence annotations with each input being a set of entity-relation triples in a tree structure.</p>\n<h3 id=\"communicative-goal\">Communicative Goal</h3>\n<p>The speaker is required to produce coherent sentences and construct a trees structured ontology of the column headers.</p>\n<h3 id=\"source-data\">Source Data</h3>\n<p>The dataset re-uses data from the following pre-existing resources:\n(1) human annotation on open-domain Wikipedia tables from WikiTableQuestions (<a href=\"https://www.aclweb.org/anthology/P15-1142.pdf\">Pasupat and Liang,\n2015</a>) and WikiSQL (<a href=\"https://arxiv.org/pdf/1709.00103.pdf\">Zhong et al., 2017</a>)\n(2) automatic conversion of questions in\nWikiSQL to declarative sentences\n(3) incorporation of existing datasets including WebNLG 2017 (Gardent et al., 2017<a href=\"https://www.aclweb.org/anthology/P17-1017.pdf\">a</a>,<a href=\"https://www.aclweb.org/anthology/W17-3518.pdf\">b</a>; <a href=\"https://www.aclweb.org/anthology/W18-6543.pdf\">Shimorina and Gardent, 2018</a>) and Cleaned E2E (<a href=\"https://arxiv.org/pdf/1706.09254.pdf\">Novikova et al., 2017b</a>; Dušek et al., <a href=\"https://arxiv.org/pdf/1810.01170.pdf\">2018</a>, <a href=\"https://www.aclweb.org/anthology/W19-8652.pdf\">2019</a>)</p>\n<p>Creators also explored automatic alignments between the knowledge base and text including Neural Wikipedian (<a href=\"https://arxiv.org/pdf/1711.00155.pdf\">Vougiouklis et al., 2018</a>) and TRex (<a href=\"https://www.aclweb.org/anthology/L18-1544.pdf\">Elsahar et al., 2018</a>).</p>\n<p>We refer the reader to the papers describing these sources for further information.</p>\n<h4 id=\"initial-data-collection-and-normalization\">Initial Data Collection and Normalization</h4>\n<p>The training data consists of concept sets and captions for the source datasets listed above.\nFor conversion of a meaning representation (MR) to a triple set, where the NAME slot was represented as the subject.</p>\n<h4 id=\"who-are-the-source-language-producers\">Who are the source language producers?</h4>\n<p>The language producers are Wikipedia authors and/or editors for Wikipedia tables (WikiTableQuestions, WikiSQL, WebNLG), crowdworkers (E2E) and annotators (DART, E2E).</p>\n<p>No demographic information is provided.</p>\n<h3 id=\"annotations\">Annotations</h3>\n<h4 id=\"annotation-process\">Annotation process</h4>\n<p>Creators proposed a two-stage annotation process for constructing triple set sentence pairs based on a tree-structured ontology of each table. First, internal skilled annotators denote the parent column for each column header. Then, a larger number of annotators provide a sentential description of an automatically-chosen subset of table cells in a row. To form a triple set sentence pair, the highlighted cells can be converted to a connected triple set automatically according to the column ontology for the given table.</p>\n<h4 id=\"who-are-the-annotators\">Who are the annotators?</h4>\n<p>No further information about the MTurk workers has been provided.</p>\n<h3 id=\"personal-and-sensitive-information\">Personal and Sensitive Information</h3>\n<p>[N/A]</p>\n<h2 id=\"changes-to-the-original-dataset-for-gem\">Changes to the Original Dataset for GEM</h2>\n<p>No changes were made to the original dataset for GEM at the moment of writing this.\nWe may, at some future point, introduce additional test set annotation (related to difficulty/challenging-ness) or introduce challenge sets - these are at the moment only tentatively planned.</p>\n<h2 id=\"considerations-for-using-the-data\">Considerations for Using the Data</h2>\n<h3 id=\"social-impact-of-the-dataset\">Social Impact of the Dataset</h3>\n<p>The task is presented as a stepping stone towards building models that achieve more human-like text generation.</p>\n<h3 id=\"impact-on-underserved-communities\">Impact on Underserved Communities</h3>\n<p>The dataset is in English, a language with an abundance of existing resources.</p>\n<h3 id=\"discussion-of-biases\">Discussion of Biases</h3>\n<p>The dataset may contain some social biases, as the input sentences are based on Wikipedia (WikiTableQuestions, WikiSQL, WebNLG). Studies have shown that the English Wikipedia contains gender biases(<a href=\"https://www.aclweb.org/anthology/2020.emnlp-main.23.pdf\">Dinan et al., 2020</a>), racial biases([Papakyriakopoulos et al., 2020 (<a href=\"https://dl.acm.org/doi/pdf/10.1145/3351095.3372843\">https://dl.acm.org/doi/pdf/10.1145/3351095.3372843</a>)) and geographical bias(<a href=\"https://doi.org/10.5204/mcj.315\">Livingstone et al., 2010</a>). <a href=\"https://en.wikipedia.org/wiki/Racial_bias_on_Wikipedia#cite_note-23\">More info</a>.</p>\n<h3 id=\"other-known-limitations\">Other Known Limitations</h3>\n<p>The end-to-end transformer has the lowest performance since the transformer model needs intermediate pipeline planning steps to have higher performance. Similar findings can be found in <a href=\"https://arxiv.org/pdf/1908.09022.pdf\">Castro Ferreira et al., 2019</a>.</p>\n<h2 id=\"getting-started-with-in-depth-research-on-the-task\">Getting started with in-depth research on the task</h2>\n<p>Experimental results on DART shows that BART model as the highest performance among three models with a BLEU score of 37.06. This is attributed to BART’s generalization ability due to pretraining (<a href=\"https://arxiv.org/pdf/2007.02871.pdf\">Table 4</a>).</p>\n","title":"DART","type":"Structure-to-Text","motivation":"Hierarchical, structured format with its open-domain nature"}},"__N_SSG":true}