<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="Benchmark natural language generation systems with GEM."/><meta property="og:image" content="https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"/><meta name="og:title" content="GEM"/><meta name="twitter:card" content="summary_large_image"/><title>GEM Workshop 2025</title><meta name="next-head-count" content="8"/><link rel="preload" href="/_next/static/css/42fe94e3e660903d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/42fe94e3e660903d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/93c336621cfc84eb.css" as="style"/><link rel="stylesheet" href="/_next/static/css/93c336621cfc84eb.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-635a834dfd7d0dc2.js" defer=""></script><script src="/_next/static/chunks/framework-7a7e500878b44665.js" defer=""></script><script src="/_next/static/chunks/main-a56c17dda72126ba.js" defer=""></script><script src="/_next/static/chunks/pages/_app-da8862f0ec3a97c1.js" defer=""></script><script src="/_next/static/chunks/c16184b3-ddb1b99b5e568a2a.js" defer=""></script><script src="/_next/static/chunks/50-3dccc3616b494db8.js" defer=""></script><script src="/_next/static/chunks/pages/workshop/2025-call-fe26712b5f9344ae.js" defer=""></script><script src="/_next/static/64ekgEf2Sn1vpQfHbMHiq/_buildManifest.js" defer=""></script><script src="/_next/static/64ekgEf2Sn1vpQfHbMHiq/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="layout_background__oCFQX undefined"><header class="layout_header__SFlEE"><div class="navbar_navwrapper__RkXSe"><div class="navbar_gradbar__Vli6s"></div><nav class="navbar_navbar__vdWdK"><span class="utils_headingLg__RYtYb navbar_navbarlogo__u28NK"><a href="/">GEM BENCHMARK</a></span><div class="navbar_menutoggle__4Urrc" id="mobile-menu"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars navbar_bar__f8cyd" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg></div><ul><li class="navbar_navitem__15TsF navbar_pushright___9_8s"><a href="/resources">Resources</a></li><li class="navbar_navitem__15TsF"><a href="/data_cards">Data Cards</a></li><li class="navbar_navitem__15TsF"><a href="/model_cards">Model Cards</a></li><li class="navbar_navitem__15TsF"><a href="/tutorials">tutorials</a></li><li class="navbar_navitem__15TsF"><a href="/results">Results</a></li><li class="navbar_navitem__15TsF"><a href="/workshop">Workshop</a></li></ul></nav></div></header><div class="layout_container__FUycR undefined"><main><article><span class="utils_headingXl__zlq1q">GEM 💎 Workshop at ACL 2025</span><span class="utils_smallSpace__dcJPu"></span><div><h1 id="user-content-gem2-workshop-generation-evaluation--metrics---acl-2025">GEM2 Workshop: Generation, Evaluation &#x26; Metrics - ACL 2025</h1>
<p>The fourth iteration of the Generation, Evaluation &#x26; Metrics (GEM) Workshop will be held as part of <a href="https://2025.aclweb.org/">ACL</a>, July 27–August 1st, 2025. This year we’re planning a major upgrade to the workshop, which we dub GEM2, through the introduction of large-scale prediction benchmarks (<a href="https://slab-nlp.github.io/DOVE/">DOVE</a>, <a href="https://huggingface.co/datasets/allenai/DataDecide-eval-instances">DataDecide</a>), encouraging researchers from all backgrounds to submit work on meaningful, efficient and robust evaluation of LLMs. The workshop will also host the <a href="https://repronlp.github.io/">ReproNLP shared task on reproducibility of evaluations in NLP</a>, with a presentation of (i) the task and results overview by the organisers, and (ii) the results of the individual reproductions by the participants.</p>
<h2 id="user-content-overview">Overview</h2>
<p>Evaluating large language models (LLMs) is challenging. Running LLMs over medium or large scale corpus can be prohibitively expensive; they are consistently shown to be highly sensitive to prompt phrasing, and it is hard to formulate metrics which differentiate and rank different LLMs in a meaningful way. Consequently, the validity of the results obtained over popular benchmarks such as <a href="https://arxiv.org/abs/2211.09110">HELM</a> or <a href="https://arxiv.org/pdf/2009.03300v3">MMLU</a>, lead to brittle conclusions (<a href="https://arxiv.org/pdf/2310.11324">Sclar er al., 2024</a>, <a href="https://aclanthology.org/2024.tacl-1.52/">Mizrahi et al., 2024</a>, <a href="https://arxiv.org/pdf/2402.01781v2">Alzahrani et al., 2024</a>). We believe that meaningful, efficient, and robust evaluation is one of the cornerstones of the scientific method, and that achieving it should be a community-wide goal.</p>
<p>In this workshop we seek innovative research relating to the evaluation of LLMs and language generation systems in general. This includes, but is not limited to, robust, reproducible and efficient evaluation metrics, as well as new approaches for collecting evaluation data which can help in better differentiating between different systems and understanding their current bottlenecks.</p>
<p>To facilitate and spur research in this field we publish two large datasets of model predictions together with prompts and gold standard references: <a href="https://slab-nlp.github.io/DOVE/">DOVE</a> and <a href="https://huggingface.co/datasets/allenai/DataDecide-eval-instances">DataDecide</a>. These datasets go beyond reporting just the accuracy of a model on a given sample, and also include various axes which identify how the prompt was created and which were found to affect performance (instruction template, few-shot examples, their order, delimiters, etc.), as well as any known information about the model (pre training corpora, type of instruction-tuning, different checkpoints, and more), and the annotated gold label. Through this dataset, researchers will be able to investigate key questions such as: Are larger models more robust across different prompting configurations? Are common enumerators (e.g., A/B, 1/2) less sensitive compared to rare ones (e.g., I/IV, #/$)? Which evaluation axes should be prioritized when testing with limited resources? Can we identify patterns distinguishing examples where models show high robustness (consistent answers across configurations) versus low robustness (varying answers)?</p>
<p>We welcome submissions related, but not limited to, the following topics:</p>
<ul>
<li>💎 Automatic evaluation of generation systems (<a href="https://aclanthology.org/2021.gem-1.8/">Wang et al., 2021</a>, <a href="https://aclanthology.org/2021.gem-1.1/">Tanprasert and Kauchak, 2021</a>, <a href="https://aclanthology.org/2022.gem-1.26/">Popović et al., 2022</a>).</li>
<li>💎 Creating evaluation corpora and challenge sets (<a href="https://aclanthology.org/2022.tacl-1.4/">Kerutzer et al., 2022</a>, <a href="https://openreview.net/forum?id=CSi1eu_2q96">Mille et al., 2021</a>, <a href="https://aclanthology.org/2022.gem-1.6/">Chuklin et al., 2022</a>).</li>
<li>💎 Critiques of benchmarking efforts and responsibly measuring progress in LLMs (<a href="https://aclanthology.org/2020.emnlp-main.393/">Ethayarajh and Jurafsky, 2020</a>, <a href="https://openreview.net/forum?id=j6NxpQbREA1">Raji et al., 2021</a>).</li>
<li>💎 Effective and/or efficient NLG methods that can be applied to a wide range of languages and/or scenarios (<a href="https://aclanthology.org/2020.tacl-1.47/">Liu et al., 2020</a>, <a href="https://aclanthology.org/2021.gem-1.16/">DeLucia et al., 2021</a>, <a href="https://aclanthology.org/2022.gem-1.1/">Pernes et al., 2022</a>).</li>
<li>💎 Application and evaluation of LLMs interacting with external data and tools (<a href="https://arxiv.org/abs/2302.04761">Schick et al., 2023</a>, <a href="https://arxiv.org/abs/2304.09842">Lu et al., 2023</a>, <a href="https://arxiv.org/abs/2302.07842">Mialon et al., 2023</a>).</li>
<li>💎 Evaluation of sociotechnical systems employing large language models (<a href="https://dl.acm.org/doi/abs/10.1145/3531146.3533088">Weidinger et al., 2022</a>).</li>
<li>💎 Standardizing human evaluation and making it more robust (<a href="https://aclanthology.org/2021.tacl-1.87/">Freitag et al., 2021</a>, <a href="https://aclanthology.org/2022.humeval-1.7/">Saldías Fuentes et al., 2022</a>, <a href="https://aclanthology.org/2022.gem-1.12/">Mousavi et al., 2022</a>).</li>
</ul>
<p>We further invite submissions that conduct in-depth analyses of outputs of existing systems, for example through error analyses, by applying new metrics, or by testing the system on new test sets. While we encourage the use of the infrastructure the organizing team has developed as part of the <a href="https://arxiv.org/abs/2206.11249">GEM benchmark</a>, its use is not required.</p>
<p>If you are interested, you can check out last year's workshop websites from <a href="https://gem-benchmark.com/workshop/2021">ACL 2021</a>, <a href="https://gem-benchmark.com/workshop/2022">EMNLP 2022</a>, and <a href="https://gem-benchmark.com/workshop/2023">EMNLP 2023</a>.</p>
<h2 id="user-content-industrial-track---unleashing-the-power-of-nlp-bridging-the-gap-between-academia-and-industry">Industrial Track - Unleashing the Power of NLP: Bridging the Gap between Academia and Industry</h2>
<p>Following the success of last iterations, GEM2 will hold an Industrial Track, which aims to provide actionable insights to industry professionals and to foster collaborations between academia and industry. This track will address the unique challenges faced by non-academic colleagues, highlighting the differences in evaluation practices between academic and industrial research, and explore the challenges in evaluating generative models with real-world data.</p>
<p>The Industrial Track invites submissions covering the following topics, including (but not limited to):</p>
<ul>
<li>💎 Breaking Barriers: Bridging the Gap between Academic and Industrial Research (<a href="https://aclanthology.org/P17-2015">Dahlmeier 2017</a>).</li>
<li>💎 From Data Diversity to Model Robustness: Challenges in Evaluating Generative Models with Real-World Data (<a href="https://aclanthology.org/2021.sigdial-1.8/">Heidari et al., 2021</a>).</li>
<li>💎 Beyond Metrics: Evaluating Generative Models for Real-World Business Impact (<a href="https://arxiv.org/abs/1906.02243">Strubell et al., 2019</a>, <a href="https://aclanthology.org/P16-2096">Hovy et al., 2016</a>, <a href="https://arxiv.org/abs/2306.07402">Howell et al., 2023</a>).</li>
</ul>
<h2 id="user-content-repronlp">ReproNLP</h2>
<p>Make sure your paper was validated by the ReproNLP organisers, and that you select the appropriate track on OpenReview. Further details will be provided at a later stage.</p>
<h2 id="user-content-how-to-submit">How to submit?</h2>
<p>Submissions can take either of the following forms:</p>
<ul>
<li>💎 <strong>Archival Papers (with no ARR reviews available)</strong> describing original and unpublished work can be submitted in a between 4 and 8 page format.</li>
<li>💎 <strong>ARR-Reviewed Archival Papers</strong> describing original and unpublished work that already has ARR reviews can be submitted in a between 4 and 8 page format.</li>
<li>💎 <strong>Non-Archival Abstracts</strong> To discuss work already presented or under review at a peer-reviewed venue, we allow the submission of 2-page abstracts.</li>
</ul>
<p>All submissions are allowed unlimited space for references and appendices and should conform to <a href="https://2025.aclweb.org/calls/main_conference_papers/#paper-submission-details">ACL 2025 style guidelines</a>. Archival paper submissions must be anonymized while abstract submissions may include author information. Final versions of accepted papers will be allowed 1 additional page of content so that reviewer comments can be taken into account.</p>
<p>Pre-reviewed ARR papers should be submitted by filling <a href="https://docs.google.com/forms/d/e/1FAIpQLSdDUoxvdwKgwv6mOsxL7aFJ3InkyHxkPugicbnj1wbm9lSngg/viewform?usp=dialog">this short form</a>. Papers to be reviewed should be submitted directly through <a href="https://openreview.net/group?id=aclweb.org/ACL/2025/Workshop/GEM">OpenReview</a>, selecting the appropriate track. <strong>We additionally welcome presentations by authors of papers in the Findings of the ACL</strong>. The selection process is managed centrally by the workshop chairs for the conference and we thus cannot respond to individual inquiries about Findings papers. However, we will try our best to accommodate authors’ requests.</p>
<h2 id="user-content-important-dates">Important Dates</h2>
<p>Note: For any questions, please email <a href="mailto:gem-benchmark-chairs@googlegroups.com">gem-benchmark-chairs@googlegroups.com</a>.</p>
<p><strong>Paper Submission Dates</strong></p>
<ul>
<li>📅 <strong>April 11:</strong> Direct paper submission deadline (ARR).</li>
<li>📅 <del>May 5</del> <strong>May 17:</strong> Pre-reviewed (ARR) commitment deadline.</li>
<li>📅 <del>May 19</del> <strong>May 25:</strong> Notification of acceptance.</li>
<li>📅 <del>June 6</del> <strong>June 12:</strong> Camera-ready paper deadline.</li>
<li>📅 <strong>July 7:</strong> Pre-recorded videos due.</li>
<li>📅 <strong>July 31 - August 1:</strong> Workshop at ACL in Vienna.</li>
</ul>
<h2 id="user-content-organizers">Organizers</h2>
<ul>
<li>Ofir Arviv, IBM Research</li>
<li>Miruna Clinciu, Heriot Watt University</li>
<li>Kaustubh Dhole, Emory University</li>
<li>Rotem Dror, University of Haifa</li>
<li>Sebastian Gehrmann, Bloomberg</li>
<li>Eliya Habba, Hebrew University of Jerusalem</li>
<li>Itay Itzhak, Hebrew University of Jerusalem</li>
<li>Yotam Perlitz, IBM Research</li>
<li>Simon Mille, Dublin City University</li>
<li>Enrico Santus, Bloomberg</li>
<li>Michal Shmueli Scheuer, IBM Research</li>
<li>João Sedoc, New York University</li>
<li>Gabriel Stanovsky, Hebrew University of Jerusalem</li>
<li>Oyvind Tafjord, Allen Institute for Artificial Intelligence</li>
</ul>
</div></article></main><div class="layout_push__lpoMK"></div></div><footer class="layout_footer__WlhMu utils_eggshell__3hbbY"><span class="layout_backToHome__D9QFr"><a href="/">← Home</a></span><span>If you have any questions, please join our <a href="https://groups.google.com/g/gem-benchmark" target="_blank" class="utils_accentUnderline__VG89l">google group</a> for support.</span></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"workshopData":{"contentHtml":"\u003ch1 id=\"user-content-gem2-workshop-generation-evaluation--metrics---acl-2025\"\u003eGEM2 Workshop: Generation, Evaluation \u0026#x26; Metrics - ACL 2025\u003c/h1\u003e\n\u003cp\u003eThe fourth iteration of the Generation, Evaluation \u0026#x26; Metrics (GEM) Workshop will be held as part of \u003ca href=\"https://2025.aclweb.org/\"\u003eACL\u003c/a\u003e, July 27–August 1st, 2025. This year we’re planning a major upgrade to the workshop, which we dub GEM2, through the introduction of large-scale prediction benchmarks (\u003ca href=\"https://slab-nlp.github.io/DOVE/\"\u003eDOVE\u003c/a\u003e, \u003ca href=\"https://huggingface.co/datasets/allenai/DataDecide-eval-instances\"\u003eDataDecide\u003c/a\u003e), encouraging researchers from all backgrounds to submit work on meaningful, efficient and robust evaluation of LLMs. The workshop will also host the \u003ca href=\"https://repronlp.github.io/\"\u003eReproNLP shared task on reproducibility of evaluations in NLP\u003c/a\u003e, with a presentation of (i) the task and results overview by the organisers, and (ii) the results of the individual reproductions by the participants.\u003c/p\u003e\n\u003ch2 id=\"user-content-overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eEvaluating large language models (LLMs) is challenging. Running LLMs over medium or large scale corpus can be prohibitively expensive; they are consistently shown to be highly sensitive to prompt phrasing, and it is hard to formulate metrics which differentiate and rank different LLMs in a meaningful way. Consequently, the validity of the results obtained over popular benchmarks such as \u003ca href=\"https://arxiv.org/abs/2211.09110\"\u003eHELM\u003c/a\u003e or \u003ca href=\"https://arxiv.org/pdf/2009.03300v3\"\u003eMMLU\u003c/a\u003e, lead to brittle conclusions (\u003ca href=\"https://arxiv.org/pdf/2310.11324\"\u003eSclar er al., 2024\u003c/a\u003e, \u003ca href=\"https://aclanthology.org/2024.tacl-1.52/\"\u003eMizrahi et al., 2024\u003c/a\u003e, \u003ca href=\"https://arxiv.org/pdf/2402.01781v2\"\u003eAlzahrani et al., 2024\u003c/a\u003e). We believe that meaningful, efficient, and robust evaluation is one of the cornerstones of the scientific method, and that achieving it should be a community-wide goal.\u003c/p\u003e\n\u003cp\u003eIn this workshop we seek innovative research relating to the evaluation of LLMs and language generation systems in general. This includes, but is not limited to, robust, reproducible and efficient evaluation metrics, as well as new approaches for collecting evaluation data which can help in better differentiating between different systems and understanding their current bottlenecks.\u003c/p\u003e\n\u003cp\u003eTo facilitate and spur research in this field we publish two large datasets of model predictions together with prompts and gold standard references: \u003ca href=\"https://slab-nlp.github.io/DOVE/\"\u003eDOVE\u003c/a\u003e and \u003ca href=\"https://huggingface.co/datasets/allenai/DataDecide-eval-instances\"\u003eDataDecide\u003c/a\u003e. These datasets go beyond reporting just the accuracy of a model on a given sample, and also include various axes which identify how the prompt was created and which were found to affect performance (instruction template, few-shot examples, their order, delimiters, etc.), as well as any known information about the model (pre training corpora, type of instruction-tuning, different checkpoints, and more), and the annotated gold label. Through this dataset, researchers will be able to investigate key questions such as: Are larger models more robust across different prompting configurations? Are common enumerators (e.g., A/B, 1/2) less sensitive compared to rare ones (e.g., I/IV, #/$)? Which evaluation axes should be prioritized when testing with limited resources? Can we identify patterns distinguishing examples where models show high robustness (consistent answers across configurations) versus low robustness (varying answers)?\u003c/p\u003e\n\u003cp\u003eWe welcome submissions related, but not limited to, the following topics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e💎 Automatic evaluation of generation systems (\u003ca href=\"https://aclanthology.org/2021.gem-1.8/\"\u003eWang et al., 2021\u003c/a\u003e, \u003ca href=\"https://aclanthology.org/2021.gem-1.1/\"\u003eTanprasert and Kauchak, 2021\u003c/a\u003e, \u003ca href=\"https://aclanthology.org/2022.gem-1.26/\"\u003ePopović et al., 2022\u003c/a\u003e).\u003c/li\u003e\n\u003cli\u003e💎 Creating evaluation corpora and challenge sets (\u003ca href=\"https://aclanthology.org/2022.tacl-1.4/\"\u003eKerutzer et al., 2022\u003c/a\u003e, \u003ca href=\"https://openreview.net/forum?id=CSi1eu_2q96\"\u003eMille et al., 2021\u003c/a\u003e, \u003ca href=\"https://aclanthology.org/2022.gem-1.6/\"\u003eChuklin et al., 2022\u003c/a\u003e).\u003c/li\u003e\n\u003cli\u003e💎 Critiques of benchmarking efforts and responsibly measuring progress in LLMs (\u003ca href=\"https://aclanthology.org/2020.emnlp-main.393/\"\u003eEthayarajh and Jurafsky, 2020\u003c/a\u003e, \u003ca href=\"https://openreview.net/forum?id=j6NxpQbREA1\"\u003eRaji et al., 2021\u003c/a\u003e).\u003c/li\u003e\n\u003cli\u003e💎 Effective and/or efficient NLG methods that can be applied to a wide range of languages and/or scenarios (\u003ca href=\"https://aclanthology.org/2020.tacl-1.47/\"\u003eLiu et al., 2020\u003c/a\u003e, \u003ca href=\"https://aclanthology.org/2021.gem-1.16/\"\u003eDeLucia et al., 2021\u003c/a\u003e, \u003ca href=\"https://aclanthology.org/2022.gem-1.1/\"\u003ePernes et al., 2022\u003c/a\u003e).\u003c/li\u003e\n\u003cli\u003e💎 Application and evaluation of LLMs interacting with external data and tools (\u003ca href=\"https://arxiv.org/abs/2302.04761\"\u003eSchick et al., 2023\u003c/a\u003e, \u003ca href=\"https://arxiv.org/abs/2304.09842\"\u003eLu et al., 2023\u003c/a\u003e, \u003ca href=\"https://arxiv.org/abs/2302.07842\"\u003eMialon et al., 2023\u003c/a\u003e).\u003c/li\u003e\n\u003cli\u003e💎 Evaluation of sociotechnical systems employing large language models (\u003ca href=\"https://dl.acm.org/doi/abs/10.1145/3531146.3533088\"\u003eWeidinger et al., 2022\u003c/a\u003e).\u003c/li\u003e\n\u003cli\u003e💎 Standardizing human evaluation and making it more robust (\u003ca href=\"https://aclanthology.org/2021.tacl-1.87/\"\u003eFreitag et al., 2021\u003c/a\u003e, \u003ca href=\"https://aclanthology.org/2022.humeval-1.7/\"\u003eSaldías Fuentes et al., 2022\u003c/a\u003e, \u003ca href=\"https://aclanthology.org/2022.gem-1.12/\"\u003eMousavi et al., 2022\u003c/a\u003e).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe further invite submissions that conduct in-depth analyses of outputs of existing systems, for example through error analyses, by applying new metrics, or by testing the system on new test sets. While we encourage the use of the infrastructure the organizing team has developed as part of the \u003ca href=\"https://arxiv.org/abs/2206.11249\"\u003eGEM benchmark\u003c/a\u003e, its use is not required.\u003c/p\u003e\n\u003cp\u003eIf you are interested, you can check out last year's workshop websites from \u003ca href=\"https://gem-benchmark.com/workshop/2021\"\u003eACL 2021\u003c/a\u003e, \u003ca href=\"https://gem-benchmark.com/workshop/2022\"\u003eEMNLP 2022\u003c/a\u003e, and \u003ca href=\"https://gem-benchmark.com/workshop/2023\"\u003eEMNLP 2023\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"user-content-industrial-track---unleashing-the-power-of-nlp-bridging-the-gap-between-academia-and-industry\"\u003eIndustrial Track - Unleashing the Power of NLP: Bridging the Gap between Academia and Industry\u003c/h2\u003e\n\u003cp\u003eFollowing the success of last iterations, GEM2 will hold an Industrial Track, which aims to provide actionable insights to industry professionals and to foster collaborations between academia and industry. This track will address the unique challenges faced by non-academic colleagues, highlighting the differences in evaluation practices between academic and industrial research, and explore the challenges in evaluating generative models with real-world data.\u003c/p\u003e\n\u003cp\u003eThe Industrial Track invites submissions covering the following topics, including (but not limited to):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e💎 Breaking Barriers: Bridging the Gap between Academic and Industrial Research (\u003ca href=\"https://aclanthology.org/P17-2015\"\u003eDahlmeier 2017\u003c/a\u003e).\u003c/li\u003e\n\u003cli\u003e💎 From Data Diversity to Model Robustness: Challenges in Evaluating Generative Models with Real-World Data (\u003ca href=\"https://aclanthology.org/2021.sigdial-1.8/\"\u003eHeidari et al., 2021\u003c/a\u003e).\u003c/li\u003e\n\u003cli\u003e💎 Beyond Metrics: Evaluating Generative Models for Real-World Business Impact (\u003ca href=\"https://arxiv.org/abs/1906.02243\"\u003eStrubell et al., 2019\u003c/a\u003e, \u003ca href=\"https://aclanthology.org/P16-2096\"\u003eHovy et al., 2016\u003c/a\u003e, \u003ca href=\"https://arxiv.org/abs/2306.07402\"\u003eHowell et al., 2023\u003c/a\u003e).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"user-content-repronlp\"\u003eReproNLP\u003c/h2\u003e\n\u003cp\u003eMake sure your paper was validated by the ReproNLP organisers, and that you select the appropriate track on OpenReview. Further details will be provided at a later stage.\u003c/p\u003e\n\u003ch2 id=\"user-content-how-to-submit\"\u003eHow to submit?\u003c/h2\u003e\n\u003cp\u003eSubmissions can take either of the following forms:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e💎 \u003cstrong\u003eArchival Papers (with no ARR reviews available)\u003c/strong\u003e describing original and unpublished work can be submitted in a between 4 and 8 page format.\u003c/li\u003e\n\u003cli\u003e💎 \u003cstrong\u003eARR-Reviewed Archival Papers\u003c/strong\u003e describing original and unpublished work that already has ARR reviews can be submitted in a between 4 and 8 page format.\u003c/li\u003e\n\u003cli\u003e💎 \u003cstrong\u003eNon-Archival Abstracts\u003c/strong\u003e To discuss work already presented or under review at a peer-reviewed venue, we allow the submission of 2-page abstracts.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAll submissions are allowed unlimited space for references and appendices and should conform to \u003ca href=\"https://2025.aclweb.org/calls/main_conference_papers/#paper-submission-details\"\u003eACL 2025 style guidelines\u003c/a\u003e. Archival paper submissions must be anonymized while abstract submissions may include author information. Final versions of accepted papers will be allowed 1 additional page of content so that reviewer comments can be taken into account.\u003c/p\u003e\n\u003cp\u003ePre-reviewed ARR papers should be submitted by filling \u003ca href=\"https://docs.google.com/forms/d/e/1FAIpQLSdDUoxvdwKgwv6mOsxL7aFJ3InkyHxkPugicbnj1wbm9lSngg/viewform?usp=dialog\"\u003ethis short form\u003c/a\u003e. Papers to be reviewed should be submitted directly through \u003ca href=\"https://openreview.net/group?id=aclweb.org/ACL/2025/Workshop/GEM\"\u003eOpenReview\u003c/a\u003e, selecting the appropriate track. \u003cstrong\u003eWe additionally welcome presentations by authors of papers in the Findings of the ACL\u003c/strong\u003e. The selection process is managed centrally by the workshop chairs for the conference and we thus cannot respond to individual inquiries about Findings papers. However, we will try our best to accommodate authors’ requests.\u003c/p\u003e\n\u003ch2 id=\"user-content-important-dates\"\u003eImportant Dates\u003c/h2\u003e\n\u003cp\u003eNote: For any questions, please email \u003ca href=\"mailto:gem-benchmark-chairs@googlegroups.com\"\u003egem-benchmark-chairs@googlegroups.com\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePaper Submission Dates\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e📅 \u003cstrong\u003eApril 11:\u003c/strong\u003e Direct paper submission deadline (ARR).\u003c/li\u003e\n\u003cli\u003e📅 \u003cdel\u003eMay 5\u003c/del\u003e \u003cstrong\u003eMay 17:\u003c/strong\u003e Pre-reviewed (ARR) commitment deadline.\u003c/li\u003e\n\u003cli\u003e📅 \u003cdel\u003eMay 19\u003c/del\u003e \u003cstrong\u003eMay 25:\u003c/strong\u003e Notification of acceptance.\u003c/li\u003e\n\u003cli\u003e📅 \u003cdel\u003eJune 6\u003c/del\u003e \u003cstrong\u003eJune 12:\u003c/strong\u003e Camera-ready paper deadline.\u003c/li\u003e\n\u003cli\u003e📅 \u003cstrong\u003eJuly 7:\u003c/strong\u003e Pre-recorded videos due.\u003c/li\u003e\n\u003cli\u003e📅 \u003cstrong\u003eJuly 31 - August 1:\u003c/strong\u003e Workshop at ACL in Vienna.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"user-content-organizers\"\u003eOrganizers\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eOfir Arviv, IBM Research\u003c/li\u003e\n\u003cli\u003eMiruna Clinciu, Heriot Watt University\u003c/li\u003e\n\u003cli\u003eKaustubh Dhole, Emory University\u003c/li\u003e\n\u003cli\u003eRotem Dror, University of Haifa\u003c/li\u003e\n\u003cli\u003eSebastian Gehrmann, Bloomberg\u003c/li\u003e\n\u003cli\u003eEliya Habba, Hebrew University of Jerusalem\u003c/li\u003e\n\u003cli\u003eItay Itzhak, Hebrew University of Jerusalem\u003c/li\u003e\n\u003cli\u003eYotam Perlitz, IBM Research\u003c/li\u003e\n\u003cli\u003eSimon Mille, Dublin City University\u003c/li\u003e\n\u003cli\u003eEnrico Santus, Bloomberg\u003c/li\u003e\n\u003cli\u003eMichal Shmueli Scheuer, IBM Research\u003c/li\u003e\n\u003cli\u003eJoão Sedoc, New York University\u003c/li\u003e\n\u003cli\u003eGabriel Stanovsky, Hebrew University of Jerusalem\u003c/li\u003e\n\u003cli\u003eOyvind Tafjord, Allen Institute for Artificial Intelligence\u003c/li\u003e\n\u003c/ul\u003e\n"}},"__N_SSG":true},"page":"/workshop/2025-call","query":{},"buildId":"64ekgEf2Sn1vpQfHbMHiq","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>