<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="Benchmark natural language generation systems with GEM."/><meta property="og:image" content="https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"/><meta name="og:title" content="GEM"/><meta name="twitter:card" content="summary_large_image"/><title>GEM Workshop 2025</title><meta name="next-head-count" content="8"/><link rel="preload" href="/_next/static/css/86a77084a15a5546.css" as="style"/><link rel="stylesheet" href="/_next/static/css/86a77084a15a5546.css" data-n-g=""/><link rel="preload" href="/_next/static/css/50ad98e60bd49ad7.css" as="style"/><link rel="stylesheet" href="/_next/static/css/50ad98e60bd49ad7.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-a031d64b6565e4ad.js" defer=""></script><script src="/_next/static/chunks/framework-7a7e500878b44665.js" defer=""></script><script src="/_next/static/chunks/main-a56c17dda72126ba.js" defer=""></script><script src="/_next/static/chunks/pages/_app-da8862f0ec3a97c1.js" defer=""></script><script src="/_next/static/chunks/c16184b3-ddb1b99b5e568a2a.js" defer=""></script><script src="/_next/static/chunks/50-3dccc3616b494db8.js" defer=""></script><script src="/_next/static/chunks/pages/workshop-6dddaa9b5449a80b.js" defer=""></script><script src="/_next/static/jWw_-4avzmATIosGQLDFf/_buildManifest.js" defer=""></script><script src="/_next/static/jWw_-4avzmATIosGQLDFf/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="layout_background__oCFQX undefined"><header class="layout_header__SFlEE"><div class="navbar_navwrapper__RkXSe"><div class="navbar_gradbar__Vli6s"></div><nav class="navbar_navbar__vdWdK"><span class="utils_headingLg__RYtYb navbar_navbarlogo__u28NK"><a href="/">GEM BENCHMARK</a></span><div class="navbar_menutoggle__4Urrc" id="mobile-menu"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars navbar_bar__f8cyd" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg></div><ul><li class="navbar_navitem__15TsF navbar_pushright___9_8s"><a href="/resources">Resources</a></li><li class="navbar_navitem__15TsF"><a href="/data_cards">Data Cards</a></li><li class="navbar_navitem__15TsF"><a href="/model_cards">Model Cards</a></li><li class="navbar_navitem__15TsF"><a href="/tutorials">tutorials</a></li><li class="navbar_navitem__15TsF"><a href="/results">Results</a></li><li class="navbar_navitem__15TsF"><a href="/workshop">Workshop</a></li></ul></nav></div></header><div class="layout_container__FUycR undefined"><main><article><span class="utils_headingXl__zlq1q">GEM ğŸ’ Workshop at EMNLP 2025</span><span class="utils_smallSpace__dcJPu"></span><div><h1 id="user-content-gem2-workshop-generation-evaluation--metrics---acl-2025">GEM2 Workshop: Generation, Evaluation &#x26; Metrics - ACL 2025</h1>
<p>The fourth iteration of the Generation, Evaluation &#x26; Metrics (GEM) Workshop will be held as part of ACL, July 27â€“August 1st, 2025. This year weâ€™re planning a major upgrade to the workshop, which we dub GEM2, through the introduction of a large-scale prediction benchmark, encouraging researchers from all backgrounds to submit work on meaningful, efficient and robust evaluation of LLMs.</p>
<h2 id="user-content-overview">Overview</h2>
<p>Evaluating large language models (LLMs) is challenging. Running LLMs over medium or large scale corpus can be prohibitively expensive; they are consistently shown to be highly sensitive to prompt phrasing, and it is hard to formulate metrics which differentiate and rank different LLMs in a meaningful way. Consequently, the validity of the results obtained over popular benchmarks such as HELM or MMLU, lead to brittle conclusions. We believe that meaningful, efficient, and robust evaluation is one of the cornerstones of the scientific method, and that achieving it should be a community-wide goal.</p>
<p>In this workshop we seek innovative research relating to the evaluation of LLMs and language generation systems in general. This includes, but is not limited to, robust, reproducible and efficient evaluation metrics, as well as new approaches for collecting evaluation data which can help in better differentiating between different systems and understanding their current bottlenecks.</p>
<p>To facilitate and spur research in this field we will publish a large dataset of 1B model predictions together with prompts and gold standard references. This dataset will go beyond reporting just the accuracy of a model on a given sample, and will also include various axes which identify how the prompt was created and which were found to affect performance (instruction template, few-shot examples, their order, delimiters, etc.), as well as any known information about the model (pre training corpora, type of instruction-tuning, different checkpoints, and more), and the annotated gold label. Through this dataset, researchers will be able to investigate key questions such as: Are larger models more robust across different prompting configurations? Are common enumerators (e.g., A/B, 1/2) less sensitive compared to rare ones (e.g., I/IV, #/$)? Which evaluation axes should be prioritized when testing with limited resources? Can we identify patterns distinguishing examples where models show high robustness (consistent answers across configurations) versus low robustness (varying answers)?</p>
<p>We welcome submissions related, but not limited to, the following topics:</p>
<ul>
<li>ğŸ’ Automatic evaluation of generation systems (example, example, example)</li>
<li>ğŸ’ Creating evaluation corpora and challenge sets (example, example, example)</li>
<li>ğŸ’ Critiques of benchmarking efforts and responsibly measuring progress in LLMs (example, example)</li>
<li>ğŸ’ Effective and/or efficient NLG methods that can be applied to a wide range of languages and/or scenarios (example, example, example)</li>
<li>ğŸ’ Application and evaluation of LLMs interacting with external data and tools (example, example, example)</li>
<li>ğŸ’ Evaluation of sociotechnical systems employing large language models (example)</li>
<li>ğŸ’ Standardizing human evaluation and making it more robust (example, example, example)</li>
</ul>
<p>We further invite submissions that conduct in-depth analyses of outputs of existing systems, for example through error analyses, by applying new metrics, or by testing the system on new test sets. While we encourage the use of the infrastructure the organizing team has developed as part of the GEM benchmark, its use is not required.</p>
<p>If you are interested, you can check out last year's workshop websites from ACL 2021, EMNLP 2022, and EMNLP 2023.</p>
<h2 id="user-content-industrial-track---unleashing-the-power-of-nlp-bridging-the-gap-between-academia-and-industry">Industrial Track - Unleashing the Power of NLP: Bridging the Gap between Academia and Industry</h2>
<p>Following the success of last iterations, GEM2 will hold an Industrial Track, which aims to provide actionable insights to industry professionals and to foster collaborations between academia and industry. This track will address the unique challenges faced by non-academic colleagues, highlighting the differences in evaluation practices between academic and industrial research, and explore the challenges in evaluating generative models with real-world data.</p>
<p>The Industrial Track invites submissions covering the following topics, including (but not limited to):</p>
<ul>
<li>ğŸ’ Breaking Barriers: Bridging the Gap between Academic and Industrial Research (example)</li>
<li>ğŸ’ From Data Diversity to Model Robustness: Challenges in Evaluating Generative Models with Real-World Data (example)</li>
<li>ğŸ’ Beyond Metrics: Evaluating Generative Models for Real-World Business Impact (example, example, example)</li>
</ul>
<h2 id="user-content-how-to-submit">How to submit?</h2>
<p>Submissions can take either of the following forms:</p>
<ul>
<li>ğŸ’ <strong>Archival Papers</strong> describing original and unpublished work can be submitted in a between 4 and 8 page format.</li>
<li>ğŸ’ <strong>Non-Archival Abstracts</strong> To discuss work already presented or under review at a peer-reviewed venue, we allow the submission of 2-page abstracts.</li>
</ul>
<p>All submissions are allowed unlimited space for references and appendices and should conform to ACL 2025 style guidelines. Archival paper submissions must be anonymized while abstract submissions may include author information. Final versions of accepted papers will be allowed 1 additional page of content so that reviewer comments can be taken into account.</p>
<p>Papers should be submitted directly through OpenReview, selecting the appropriate track. We additionally welcome presentations by authors of papers in the Findings of the ACL. The selection process is managed centrally by the workshop chairs for the conference and we thus cannot respond to individual inquiries about Findings papers. However, we will try our best to accommodate authorsâ€™ requests.</p>
<h2 id="user-content-important-dates">Important Dates</h2>
<p>Note: For any questions, please email <a href="mailto:gem-benchmark-chairs@googlegroups.com">gem-benchmark-chairs@googlegroups.com</a>.</p>
<p><strong>Paper Submission Dates</strong></p>
<ul>
<li>ğŸ“… <strong>April 11:</strong> Direct paper submission deadline (ARR).</li>
<li>ğŸ“… <strong>May 5:</strong> Pre-reviewed (ARR) commitment deadline.</li>
<li>ğŸ“… <strong>May 19:</strong> Notification of acceptance.</li>
<li>ğŸ“… <strong>June 6:</strong> Camera-ready paper deadline.</li>
<li>ğŸ“… <strong>July 7:</strong> Pre-recorded videos due.</li>
<li>ğŸ“… <strong>July 31 - August 1:</strong> Workshop at ACL in Vienna.</li>
</ul>
</div></article></main><div class="layout_push__lpoMK"></div></div><footer class="layout_footer__WlhMu utils_eggshell__3hbbY"><span class="layout_backToHome__D9QFr"><a href="/">â† Home</a></span><span>If you have any questions, please join our <a href="https://groups.google.com/g/gem-benchmark" target="_blank" class="utils_accentUnderline__VG89l">google group</a> for support.</span></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"workshopData":{"contentHtml":"\u003ch1 id=\"user-content-gem2-workshop-generation-evaluation--metrics---acl-2025\"\u003eGEM2 Workshop: Generation, Evaluation \u0026#x26; Metrics - ACL 2025\u003c/h1\u003e\n\u003cp\u003eThe fourth iteration of the Generation, Evaluation \u0026#x26; Metrics (GEM) Workshop will be held as part of ACL, July 27â€“August 1st, 2025. This year weâ€™re planning a major upgrade to the workshop, which we dub GEM2, through the introduction of a large-scale prediction benchmark, encouraging researchers from all backgrounds to submit work on meaningful, efficient and robust evaluation of LLMs.\u003c/p\u003e\n\u003ch2 id=\"user-content-overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eEvaluating large language models (LLMs) is challenging. Running LLMs over medium or large scale corpus can be prohibitively expensive; they are consistently shown to be highly sensitive to prompt phrasing, and it is hard to formulate metrics which differentiate and rank different LLMs in a meaningful way. Consequently, the validity of the results obtained over popular benchmarks such as HELM or MMLU, lead to brittle conclusions. We believe that meaningful, efficient, and robust evaluation is one of the cornerstones of the scientific method, and that achieving it should be a community-wide goal.\u003c/p\u003e\n\u003cp\u003eIn this workshop we seek innovative research relating to the evaluation of LLMs and language generation systems in general. This includes, but is not limited to, robust, reproducible and efficient evaluation metrics, as well as new approaches for collecting evaluation data which can help in better differentiating between different systems and understanding their current bottlenecks.\u003c/p\u003e\n\u003cp\u003eTo facilitate and spur research in this field we will publish a large dataset of 1B model predictions together with prompts and gold standard references. This dataset will go beyond reporting just the accuracy of a model on a given sample, and will also include various axes which identify how the prompt was created and which were found to affect performance (instruction template, few-shot examples, their order, delimiters, etc.), as well as any known information about the model (pre training corpora, type of instruction-tuning, different checkpoints, and more), and the annotated gold label. Through this dataset, researchers will be able to investigate key questions such as: Are larger models more robust across different prompting configurations? Are common enumerators (e.g., A/B, 1/2) less sensitive compared to rare ones (e.g., I/IV, #/$)? Which evaluation axes should be prioritized when testing with limited resources? Can we identify patterns distinguishing examples where models show high robustness (consistent answers across configurations) versus low robustness (varying answers)?\u003c/p\u003e\n\u003cp\u003eWe welcome submissions related, but not limited to, the following topics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eğŸ’ Automatic evaluation of generation systems (example, example, example)\u003c/li\u003e\n\u003cli\u003eğŸ’ Creating evaluation corpora and challenge sets (example, example, example)\u003c/li\u003e\n\u003cli\u003eğŸ’ Critiques of benchmarking efforts and responsibly measuring progress in LLMs (example, example)\u003c/li\u003e\n\u003cli\u003eğŸ’ Effective and/or efficient NLG methods that can be applied to a wide range of languages and/or scenarios (example, example, example)\u003c/li\u003e\n\u003cli\u003eğŸ’ Application and evaluation of LLMs interacting with external data and tools (example, example, example)\u003c/li\u003e\n\u003cli\u003eğŸ’ Evaluation of sociotechnical systems employing large language models (example)\u003c/li\u003e\n\u003cli\u003eğŸ’ Standardizing human evaluation and making it more robust (example, example, example)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe further invite submissions that conduct in-depth analyses of outputs of existing systems, for example through error analyses, by applying new metrics, or by testing the system on new test sets. While we encourage the use of the infrastructure the organizing team has developed as part of the GEM benchmark, its use is not required.\u003c/p\u003e\n\u003cp\u003eIf you are interested, you can check out last year's workshop websites from ACL 2021, EMNLP 2022, and EMNLP 2023.\u003c/p\u003e\n\u003ch2 id=\"user-content-industrial-track---unleashing-the-power-of-nlp-bridging-the-gap-between-academia-and-industry\"\u003eIndustrial Track - Unleashing the Power of NLP: Bridging the Gap between Academia and Industry\u003c/h2\u003e\n\u003cp\u003eFollowing the success of last iterations, GEM2 will hold an Industrial Track, which aims to provide actionable insights to industry professionals and to foster collaborations between academia and industry. This track will address the unique challenges faced by non-academic colleagues, highlighting the differences in evaluation practices between academic and industrial research, and explore the challenges in evaluating generative models with real-world data.\u003c/p\u003e\n\u003cp\u003eThe Industrial Track invites submissions covering the following topics, including (but not limited to):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eğŸ’ Breaking Barriers: Bridging the Gap between Academic and Industrial Research (example)\u003c/li\u003e\n\u003cli\u003eğŸ’ From Data Diversity to Model Robustness: Challenges in Evaluating Generative Models with Real-World Data (example)\u003c/li\u003e\n\u003cli\u003eğŸ’ Beyond Metrics: Evaluating Generative Models for Real-World Business Impact (example, example, example)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"user-content-how-to-submit\"\u003eHow to submit?\u003c/h2\u003e\n\u003cp\u003eSubmissions can take either of the following forms:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eğŸ’ \u003cstrong\u003eArchival Papers\u003c/strong\u003e describing original and unpublished work can be submitted in a between 4 and 8 page format.\u003c/li\u003e\n\u003cli\u003eğŸ’ \u003cstrong\u003eNon-Archival Abstracts\u003c/strong\u003e To discuss work already presented or under review at a peer-reviewed venue, we allow the submission of 2-page abstracts.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAll submissions are allowed unlimited space for references and appendices and should conform to ACL 2025 style guidelines. Archival paper submissions must be anonymized while abstract submissions may include author information. Final versions of accepted papers will be allowed 1 additional page of content so that reviewer comments can be taken into account.\u003c/p\u003e\n\u003cp\u003ePapers should be submitted directly through OpenReview, selecting the appropriate track. We additionally welcome presentations by authors of papers in the Findings of the ACL. The selection process is managed centrally by the workshop chairs for the conference and we thus cannot respond to individual inquiries about Findings papers. However, we will try our best to accommodate authorsâ€™ requests.\u003c/p\u003e\n\u003ch2 id=\"user-content-important-dates\"\u003eImportant Dates\u003c/h2\u003e\n\u003cp\u003eNote: For any questions, please email \u003ca href=\"mailto:gem-benchmark-chairs@googlegroups.com\"\u003egem-benchmark-chairs@googlegroups.com\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePaper Submission Dates\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eğŸ“… \u003cstrong\u003eApril 11:\u003c/strong\u003e Direct paper submission deadline (ARR).\u003c/li\u003e\n\u003cli\u003eğŸ“… \u003cstrong\u003eMay 5:\u003c/strong\u003e Pre-reviewed (ARR) commitment deadline.\u003c/li\u003e\n\u003cli\u003eğŸ“… \u003cstrong\u003eMay 19:\u003c/strong\u003e Notification of acceptance.\u003c/li\u003e\n\u003cli\u003eğŸ“… \u003cstrong\u003eJune 6:\u003c/strong\u003e Camera-ready paper deadline.\u003c/li\u003e\n\u003cli\u003eğŸ“… \u003cstrong\u003eJuly 7:\u003c/strong\u003e Pre-recorded videos due.\u003c/li\u003e\n\u003cli\u003eğŸ“… \u003cstrong\u003eJuly 31 - August 1:\u003c/strong\u003e Workshop at ACL in Vienna.\u003c/li\u003e\n\u003c/ul\u003e\n"}},"__N_SSG":true},"page":"/workshop","query":{},"buildId":"jWw_-4avzmATIosGQLDFf","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>