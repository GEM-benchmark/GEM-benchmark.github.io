<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="Benchmark natural language generation systems with GEM."/><meta property="og:image" content="https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"/><meta name="og:title" content="GEM"/><meta name="twitter:card" content="summary_large_image"/><title>GEM Workshop 2025</title><meta name="next-head-count" content="8"/><link rel="preload" href="/_next/static/css/42fe94e3e660903d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/42fe94e3e660903d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/93c336621cfc84eb.css" as="style"/><link rel="stylesheet" href="/_next/static/css/93c336621cfc84eb.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-635a834dfd7d0dc2.js" defer=""></script><script src="/_next/static/chunks/framework-7a7e500878b44665.js" defer=""></script><script src="/_next/static/chunks/main-a56c17dda72126ba.js" defer=""></script><script src="/_next/static/chunks/pages/_app-da8862f0ec3a97c1.js" defer=""></script><script src="/_next/static/chunks/c16184b3-ddb1b99b5e568a2a.js" defer=""></script><script src="/_next/static/chunks/50-3dccc3616b494db8.js" defer=""></script><script src="/_next/static/chunks/pages/workshop-2657bdc38527a188.js" defer=""></script><script src="/_next/static/64ekgEf2Sn1vpQfHbMHiq/_buildManifest.js" defer=""></script><script src="/_next/static/64ekgEf2Sn1vpQfHbMHiq/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="layout_background__oCFQX undefined"><header class="layout_header__SFlEE"><div class="navbar_navwrapper__RkXSe"><div class="navbar_gradbar__Vli6s"></div><nav class="navbar_navbar__vdWdK"><span class="utils_headingLg__RYtYb navbar_navbarlogo__u28NK"><a href="/">GEM BENCHMARK</a></span><div class="navbar_menutoggle__4Urrc" id="mobile-menu"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars navbar_bar__f8cyd" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg></div><ul><li class="navbar_navitem__15TsF navbar_pushright___9_8s"><a href="/resources">Resources</a></li><li class="navbar_navitem__15TsF"><a href="/data_cards">Data Cards</a></li><li class="navbar_navitem__15TsF"><a href="/model_cards">Model Cards</a></li><li class="navbar_navitem__15TsF"><a href="/tutorials">tutorials</a></li><li class="navbar_navitem__15TsF"><a href="/results">Results</a></li><li class="navbar_navitem__15TsF"><a href="/workshop">Workshop</a></li></ul></nav></div></header><div class="layout_container__FUycR undefined"><main><article><span class="utils_headingXl__zlq1q">GEM üíé Workshop at ACL 2025</span><span class="utils_smallSpace__dcJPu"></span><div><p>The fourth iteration of the Generation, Evaluation &#x26; Metrics (GEM) Workshop will be held as part of <a href="https://2025.aclweb.org/">ACL</a>, July 31, 2025.</p>
<p>The workshop will be held in hybrid mode with sessions in-person and via the conference portal.</p>
<h2 id="user-content-schedule">Schedule</h2>
<p>All times in Vienna local time.</p>
<table>
<thead>
<tr>
<th>Start</th>
<th>End</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>9:00</td>
<td>10:25</td>
<td>Opening Remarks, Keynotes by Barbara Plank and Leshem Choshen</td>
</tr>
<tr>
<td>10:25</td>
<td>10:55</td>
<td>Coffee Break</td>
</tr>
<tr>
<td>10:55</td>
<td>11:30</td>
<td>Talk Session 1</td>
</tr>
<tr>
<td>11:30</td>
<td>12:30</td>
<td>Poster Session Part 1</td>
</tr>
<tr>
<td>12:30</td>
<td>14:00</td>
<td>Lunch Break</td>
</tr>
<tr>
<td>14:00</td>
<td>15:00</td>
<td>Poster Session Part 2</td>
</tr>
<tr>
<td>15:00</td>
<td>15:30</td>
<td>Talk Session 2</td>
</tr>
<tr>
<td>15:30</td>
<td>16:00</td>
<td>Coffee Break</td>
</tr>
<tr>
<td>16:00</td>
<td>16:15</td>
<td>Talk Session 3</td>
</tr>
<tr>
<td>16:15</td>
<td>16:55</td>
<td>Keynote by Ehud Reiter</td>
</tr>
<tr>
<td>16:55</td>
<td>17:40</td>
<td>Panel Discussion</td>
</tr>
<tr>
<td>17:40</td>
<td>17:50</td>
<td>Closing Remarks</td>
</tr>
</tbody>
</table>
<h2 id="user-content-keynotes">Keynotes</h2>
<h3 id="user-content-keynote-1---barbara-plank">Keynote 1 - Barbara Plank</h3>
<h4 id="user-content-ambiguity-consistency-and-reasoning-in-llms">Ambiguity, Consistency and Reasoning in LLMs</h4>
<p><strong>ABSTRACT</strong></p>
<p>Large Language Models (LLMs) are powerful yet fallible tools, often struggling with ambiguity, inconsistency, and flawed reasoning. This talk explores some of our recent research exposing these limitations in text and language-vision models. We examine how they misinterpret ambiguous entities, fail to maintain self-consistency, and exhibit biases when these issues remain unresolved. Using insights from controlled studies and new benchmarks, we dissect how models ‚Äúknow‚Äù but often cannot ‚Äúapply‚Äù or ‚Äúverify‚Äù that knowledge. We also highlight a promising intervention ‚Äî vector ablation ‚Äî to surgically address false refusals without sacrificing model accuracy. Together, these findings reveal the critical need for more work on nuanced evaluation and fine-grained control mechanisms in future LLM development.</p>
<p><strong>BIO</strong></p>
<p>Barbara Plank is Full Professor for AI and Computational Linguistics at LMU Munich, where she directs the MaiNLP lab and co-directs the Center for Information and Language Processing. She is also an ELLIS Fellow and Visiting Full Professor at IT University of Copenhagen. Her research lab focuses on human-facing NLP: to make NLP models and evaluation more robust and inclusive, so that NLP can deal better with underlying shifts in data due to language variation, is fairer, and embraces human label variation.</p>
<h3 id="user-content-keynote-2---leshem-choshen">Keynote 2 - Leshem Choshen</h3>
<h4 id="user-content-evaluation-at-the-heart-of-the-ai-wave">Evaluation at the Heart of the AI Wave</h4>
<p><strong>ABSTRACT</strong></p>
<p>The AI wind also blows in the sails of evaluation, creating a mass of evaluation works. In this talk, Leshem will present some of the most pressing and open problems in evaluation and exemplify those with efforts they participated in. These "blue sea" problems include pretraining evaluation, unified evaluation, multicultural evaluation, and contamination.</p>
<p><strong>BIO</strong></p>
<p>Leshem Choshen is a postdoctoral researcher at MIT and MIT-IBM, studying communal LLMs, from community-built LLMs to LLMs for humans and communities. They co-created model merging, TIES merging, and the babyLM pretraining challenge. They are constantly working with the community to gather chats (please contribute), LLM games in textArena and other efforts that call for community involvement. During this work, they emphasize evaluation aspects, including reliable and efficient evaluation, tinyBenchmarks, benchmark agreement testing, and pretraining evaluation.</p>
<h3 id="user-content-keynote-3---ehud-reiter">Keynote 3 - Ehud Reiter</h3>
<h4 id="user-content-we-should-evaluate-real-world-impact">We Should Evaluate Real-World Impact</h4>
<p><strong>ABSTRACT</strong></p>
<p>The ACL community has shown very little interest in evaluating the real-world impact of deployed NLP systems. This limits the usefulness and rate of adoption of NLP in areas such as medicine. I will discuss various ways of evaluating real-world impact, and then share the results of a structured survey of the ACL Anthology, which suggests that perhaps 0.1% of its papers evaluate real-world impact; furthermore, most Anthology papers which include impact evaluations present them very sketchily and instead focus on metric evaluations. I will conclude with a discussion of when impact evaluation is appropriate, and steps the community could take to encourage it.</p>
<p><strong>BIO</strong></p>
<p>Ehud Reiter is a Professor of Computing Science at the University of Aberdeen and was formerly Chief Scientist of Arria NLG (a spinout he cofounded). He has been working on Natural Language Generation for 35 years, and in recent years has focused on healthcare applications and the evaluation of language generation. He is one of the most cited researchers in NLG, and his awards include an INLG Test of Time award for his work on data-to-text. He writes a widely read blog on NLG and evaluation (ehudreiter.com), and wrote a book on NLG which was published in November 2024.</p>
<h3 id="user-content-panelists">Panelists</h3>
<h4 id="user-content-deuwe-kiela">Deuwe Kiela</h4>
<h4 id="user-content-thiago-castro-ferreira">Thiago Castro Ferreira</h4>
<h4 id="user-content-pushkar-mishra">Pushkar Mishra</h4>
<h2 id="user-content-sessions-and-papers">Sessions and Papers</h2>
<h3 id="user-content-talk-session-1">Talk Session 1</h3>
<table>
<thead>
<tr>
<th>Title</th>
<th>Authors</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReproNLP Shared Task Overview</td>
<td>Anya Belz for the <a href="https://repronlp.github.io/">https://repronlp.github.io/</a> Team</td>
</tr>
<tr>
<td>Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs</td>
<td>Minsuh Joo</td>
</tr>
</tbody>
</table>
<h3 id="user-content-talk-session-2">Talk Session 2</h3>
<table>
<thead>
<tr>
<th>Title</th>
<th>Authors</th>
</tr>
</thead>
<tbody>
<tr>
<td>CoKe: Customizable Fine-Grained Story Evaluation via Chain-of-Keyword Rationalization</td>
<td>Joshi Brihi, Sriram Venkatapathy, Mohit Bansal, Nanyun Peng, Haw-Shiuan Chang</td>
</tr>
<tr>
<td>PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory</td>
<td>Junho Myung, Yeon Su, Sunwoo Kim, Shin Yoo, Alice Oh</td>
</tr>
</tbody>
</table>
<h3 id="user-content-talk-session-3">Talk Session 3</h3>
<table>
<thead>
<tr>
<th>Title</th>
<th>Authors</th>
</tr>
</thead>
<tbody>
<tr>
<td>Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans</td>
<td>Javier Conde, Miguel Gonzalez, Maria Grandury, Pedro Reviriego, Gonzalo Martinez, Marc Brysbaert</td>
</tr>
</tbody>
</table>
<h3 id="user-content-poster-session---in-person">Poster Session - In-Person</h3>
<p>All posters can be presented during both parts of the split poster session (with lunch break in between).</p>
<ul>
<li>Does Biomedical Training Lead to Better Medical Performance? Amin Dada, Marie Bauer, Jean-Philippe Corbeil, Amanda Butler, Osman Alperen, Constantin Marc, Kaleb E, Julian Friedrich, Jens Kleesiek</li>
<li>HEDS 3.0: The Human Evaluation Data Sheet Version 3.0 Anya Belz, Craig Thomson</li>
<li>ARGENT: Automatic Reference-free Evaluation for Open-Ended Text Generation without Source Inputs Xinyue Zhang, Agathe Zecevic, Sebastian Zeki, Angus Roberts</li>
<li>Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool for Perceived Socio-Economic Bias in LLMs Jasmin Wachter, Michael Radloff, Maja Smolej, Katharina Kinder-Kurlanda</li>
<li>Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons Isik Baran, Tu Anh, Jan Niehues, ?? ??</li>
<li>Free-text Rationale Generation under Readability Level Control Yi-Sheng Hsu, Nils Feldhus, Sherzod Hakimov</li>
<li>Selective Shot Learning for Code Explanation Paheli Bhattacharya, Rishabh Gupta</li>
<li>Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation? Evangelia Gogoulou, Shorouq Zahra, Liane Guillou, Luise D√ºrlich, Joakim Nivre</li>
<li>Evaluating LLMs with Multiple Problems at once Zhengxiang Wang, Jordan Kodner, Owen Rambow</li>
<li>Learning and Evaluating Factual Clarification Question Generation Without Examples Matthew Toles, Yukun Huang, Zhou Yu</li>
<li>SECQUE: A Benchmark for Evaluating Real-World Financial Analysis Capabilities Noga BenYoash, Menachem Brief, Oded Ovadia, Gil Shenderovitz, Moshik Mishaeli, Rachel Lemberg, Eitam Sheetrit</li>
<li>One ruler to measure them all: Benchmarking multilingual long-context language models Yekyung Kim, Jenna Russell, Marzena Karpinska, Mohit Iyyer</li>
<li>Measure only what is measurable: towards conversation requirements for evaluating task-oriented dialogue systems Emiel van, Anouck Braggaar, Emmelyn Croes, Florian Kunneman, Christine Liebrecht, Gabri√´lla Martijn</li>
<li>Are Bias Evaluation Methods Biased ? Lina Berrayana, Sean Rooney, Luis Garc√©s-Erice, Ioana Giurgiu</li>
<li>IRSum: One Model to Rule Summarization and Retrieval Sotaro Takeshita, Simone Paolo, Kai Eckert</li>
<li>Metric assessment protocol in the context of answer fluctuation on MCQ tasks Ekaterina Goliakova, Xavier Renard, Marie-Jeanne Lesot, Thibault Laugel, Christophe Marsala, Marcin Detyniecki</li>
<li>CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset Jind≈ôich Helcl, Andrei-Alexandru Manea, Gianluca Vico, Jind≈ôich Libovick√Ω</li>
<li>Using LLM-as-judge Evaluation for Sanity-checking Results and Reproducibility of Human Evaluations of NLP Systems Rudali Huidrom, Anya Belz</li>
<li>HuGME: A benchmark system for evaluating Hungarian generative LLMs No√©mi Ligeti-Nagy, G√°bor Madar√°sz, Fl√≥ra F√∂ldesi, P√©ter Hatvani, Mariann Lengyel, M√°ty√°s Osv√°th, Bence S√°rossy, Krist√≥f Varga, Gy≈ëz≈ë Zijian, Enik≈ë H√©ja, Tam√°s V√°radi, G√°bor Pr√≥sz√©ky</li>
<li>CacheSaver: A Modular Framework for Efficient, Cost-Effective, and Reproducible LLM Inference Nearchos Potamitis, Lars Henning, Laurent Bindschaedler, Niket Tandon, Akhil Arora</li>
<li>OpeNLGauge: An Explainable Metric for NLG Evaluation with Open-Weights LLMs Ivan Kart√°ƒç, Mateusz Lango, Ondrej Dusek</li>
<li>Investigating the Robustness of Retrieval-Augmented Generation at the Query Level Sezen Per√ßin, Xin Su, Qutub Sha, Phillip Howard, Aleksei Kuvshinov, Leo Schwinn, Kay-Ulrich Scholl</li>
<li>Sourcing Fresh Resources for Table-to-Text Generation Evaluation Krist√Ωna Onderkov√°, Ondrej Platek, Zdenƒõk Kasner, Ondrej Dusek</li>
<li>Big Escape Benchmark: Evaluating Human-Like Reasoning in Language Models via Real-World Escape Room Challenges Zinan Tang, QiYao Sun</li>
<li>Event-based evaluation of abstractive news summarization Huiling You, Samia Touileb, Lilja √òvrelid, Erik Velldal</li>
<li>Prompt-Based Evolution for Diverse and Generalizable Toxic Language Datasets Iago Alves, Julia Soares, Fernanda Bufon, Diogo Fernandes, Arlindo Rodrigues</li>
<li>Faithfulness Metrics Do Not Generalize Well: A Case Study in Summarization Patr√≠cia Schmidtov√°, Ondrej Dusek, Saad Mahamood</li>
<li>Fine-Tune on the Format: First Improving Multiple-Choice Evaluation for Intermediate LLM Checkpoints Alec Bunn, Ben Bogin, Sarah Wiegreffe</li>
<li>Improving Large Language Model Confidence Estimates using Extractive Rationales for Classification Jane Arleth, Iris Hendrickx, Martha Larson</li>
<li>ReproHum #0729-04: Human Evaluation Reproduction Report for "MemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes" Simeon Junker</li>
<li>ReproHum #0031-01: Reproducing the Human Evaluation of Readability from "It is AI‚Äôs Turn to Ask Humans a Question" Daniel Braun</li>
<li>ReproHum #0033-05: Human Evaluation of Factuality from A Multidisciplinary Perspective Andra-Maria Florescu, Marius Miclu»õa-C√¢mpeanu, Stefana Arina, Liviu P</li>
<li>ReproHum: #0744-02: Investigating the Reproducibility of Semantic Preservation Human Evaluations Mohammad Arvan, Natalie Parde</li>
<li>ReproHum #0669-08: Reproducing Sentiment Transfer Evaluation Krist√Ωna Onderkov√°, Mateusz Lango, Patr√≠cia Schmidtov√°, Ondrej Dusek</li>
<li>ReproHum #0729-04: Partial reproduction of the human evaluation of the MemSum and NeuSum summarisation systems Simon Mille, Michela Lorandi</li>
<li>Curse of bilinguality: Evaluating monolingual and bilingual language models on Chinese linguistic benchmarks Yuwen Zhou, Yevgen Matusevych</li>
<li>Towards Better Open-Ended Text Generation: A Multicriteria Evaluation Framework Matthias A√üenmacher, Esteban Garces</li>
<li>Bridging the LLM Accessibility Divide? Performance, Fairness, and Cost of Closed versus Open LLMs for Automated Essay Scoring Kezia Oketch, John  Lalor</li>
<li>Prompt, Translate, Fine-Tune, Re-Initialize, or Instruction-Tune? Adapting LLMs for In-Context Learning in Low-Resource Languages Christopher Toukmaji</li>
<li>Winning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in QA Agents Ashley Lewis</li>
<li>Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts? Sohee Yang</li>
<li>Ad-hoc Concept Forming in the Game Codenames as a Means for Evaluating Large Language Models Sherzod Hakimov, David Schlangen</li>
<li>Evaluating Grounded Reasoning by Code-Assisted LLMs for Mathematics Zena Al</li>
<li>From Calculation to Adjudication: Examining LLM Judges on Mathematical Reasoning Tasks Andreas Stephan</li>
<li>PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins John Lalor</li>
<li>Coreference as an indicator of context scope in multimodal narrative Nikolai Ilinykh, Shalom Lappin, Asad B., Sharid Lo√°iciga</li>
<li>PATCH! {P}sychometrics-{A}ssis{T}ed Ben{CH}marking of Large Language Models against Human Populations: A Case Study of Proficiency in 8th Grade Mathematics Qixiang Fang</li>
<li>MCQFormatBench: Robustness Tests for Multiple-Choice Questions Hiroo Takizawa</li>
<li>(Dis)improved?! How Simplified Language Affects Large Language Model Performance across Languages Miriam Ansch√ºtz, Anastasiya Damaratskaya, Chaeeun Joy, Arthur Schmalz, Edoardo Mosca, Georg Groh</li>
<li>Finance Language Model Evaluation (FLaME) Glenn Matlin, Mika Okamoto, Huzaifa Pardawala, Yang Yang, Sudheer Chava</li>
<li>sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting Sanchit Ahuja</li>
<li>Single- vs. Dual-Prompt Dialogue Generation With LLMs For Job Interviews In Human Resources Joachim De</li>
<li>U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs Vitaliy Polshkov</li>
<li>[Findings] Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences? Yingjin Song, Albert Gatt</li>
<li>[Findings] Structured Discourse Representation for Factual Consistency Verification Kun Zhang</li>
<li>[Findings] Evaluating LLMs‚Äô Assessment of Mixed-Context Hallucination Through the Lens of Summarization Siya Qi</li>
<li>[Findings] Assessing the Reasoning Capabilities of LLMs in the context of Evidence-based Claim Verification Mahmud Akhter, Maria Liakata</li>
</ul>
<h3 id="user-content-poster-session---virtual">Poster Session - Virtual</h3>
<ul>
<li>Multi-Dimensional Evaluation of Open-Source Language Models: Based on Machine Learning and Bayesian Optimization Qingchen Yu</li>
<li>Spatial Representation of Large Language Models in 2D Scene WenyaWu Weihong Deng</li>
<li>The Fellowship of the LLMs: Multi-Model Workflows for Synthetic Preference Optimization Dataset Generation Samee Arif, Sualeha Farid, Abdul Hameed, Awais Athar, Agha Ali</li>
<li>Leveraging LLM-based sentiment analysis for portfolio optimization with proximal policy optimization Kemal Kirtac, Guido Germano</li>
<li>Evaluating LLMs Beyond Standard Text: A Benchmark on Non-Traditional Text Variations Jihyun Kim, Yejee Kim, HyunJeong Kang, Sumyeong Kim, Minji Son, Hyeseung Han, Kyungwoo Song</li>
<li>Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation Ziqiao Ma, Jing Ding, Xuejun Zhang, Dezhi Luo, Jiahe Ding, Sihan Xu, Yuchen Huang, Run Peng, Joyce Chai</li>
<li>Can Perplexity Predict Finetuning Performance? An Investigation of Tokenization Effects on Sequential Language Models for Nepali Nishant Luitel, Nirajan Bekoju, Anand Kumar, Subarna Shakya</li>
<li>Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs Jing Yang, Kong Aik, Woon-Seng Gan</li>
<li>(Towards) Scalable Reliable Automated Evaluation with Large Language Models Bertil Braun, Martin Forell</li>
<li>Clustering Zero-Shot Uncertainty Estimations to Assess LLM Response Accuracy for Yes/No Q&#x26;A Christopher T., Amy Vennos, W. Graham, Daniel Dakota,</li>
<li>Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges Christopher T., Amy Vennos, W. Graham, Daniel Dakota</li>
<li>Are AI Datasets Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts German Gritsai, Anastasia Voznyuk, Andrey Grabovoy, Yury Chekhovich</li>
<li>Analyzing the Sensitivity of Vision Language Models in Visual Question Answering Monika shah, Sudarshan Balaji, Somdeb Sarkhel, Sanorita Dey, Deepak Venugopal</li>
<li>ELAB: Extensive LLM Alignment Benchmark in Persian Language Zahra Pourbahman, Fatemeh rajabi, Mohammadhossein Sadeghi, Omid Ghahroodi, Somayeh Bakhshaei, Arash Amini, Reza Kazemi, Mahdieh Soleymani</li>
<li>Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish Elif Ecem, Ay≈üe Aysu, Ahmet Kaan, Seyma Erdem, Burak Aytan, Busra Tufan, Abdullah Topraksoy, Esra Darici, Cagri Toraman</li>
<li>Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data? Xuan Qi, Jiahao Qiu, Xinzhe Juan, Yue Wu, Mengdi Wang</li>
<li>ReproHum #0744-02: A Reproduction of the Human Evaluation of Meaning Preservation in ``Factorising Meaning and Form for Intent-Preserving Paraphrasing'' Julius Steen, Katja Markert</li>
<li>ReproHum #0067-01: A Reproduction of the Evaluation of Cross-Lingual Summarization Supryadi Chuang Liu, Deyi Xiong</li>
<li>Fine-Grained Constraint Generation-Verification for Improved Instruction-Following Zhixiang Liang, Zhenyu Hou, Xiao Wang</li>
<li>Natural Language Counterfactual Explanations in Financial Text Classification: A Comparison of Generators and Evaluation Metrics Karol Dobiczek</li>
<li>An Analysis of Datasets, Metrics and Models in Keyphrase Generation Florian Boudin, Akiko Aizawa</li>
</ul>
<h2 id="user-content-important-dates">Important Dates</h2>
<p><code>July 31</code> Workshop Date</p>
<h3 id="user-content-organization">Organization</h3>
<h2 id="user-content-organizers">Organizers</h2>
<ul>
<li>Sebastian Gehrmann, Bloomberg</li>
<li>Gabriel Stanovsky, Hebrew University of Jerusalem</li>
<li>Simon Mille, Dublin City University</li>
<li>Enrico Santus, Bloomberg</li>
<li>Miruna Clinciu, Heriot Watt University</li>
<li>Kaustubh Dhole, Emory University</li>
<li>Yotam Perlitz, IBM Research</li>
<li>Rotem Dror, University of Haifa</li>
<li>Itay Itzhak, Hebrew University of Jerusalem</li>
<li>Ofir Arviv, IBM Research</li>
<li>Eliya Habba, Hebrew University of Jerusalem</li>
<li>Michal Shmueli Scheuer, IBM Research</li>
<li>Jo√£o Sedoc, New York University</li>
<li>Oyvind Tafjord, Allen Institute for Artificial Intelligence</li>
</ul>
</div></article></main><div class="layout_push__lpoMK"></div></div><footer class="layout_footer__WlhMu utils_eggshell__3hbbY"><span class="layout_backToHome__D9QFr"><a href="/">‚Üê Home</a></span><span>If you have any questions, please join our <a href="https://groups.google.com/g/gem-benchmark" target="_blank" class="utils_accentUnderline__VG89l">google group</a> for support.</span></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"workshopData":{"contentHtml":"\u003cp\u003eThe fourth iteration of the Generation, Evaluation \u0026#x26; Metrics (GEM) Workshop will be held as part of \u003ca href=\"https://2025.aclweb.org/\"\u003eACL\u003c/a\u003e, July 31, 2025.\u003c/p\u003e\n\u003cp\u003eThe workshop will be held in hybrid mode with sessions in-person and via the conference portal.\u003c/p\u003e\n\u003ch2 id=\"user-content-schedule\"\u003eSchedule\u003c/h2\u003e\n\u003cp\u003eAll times in Vienna local time.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eStart\u003c/th\u003e\n\u003cth\u003eEnd\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e9:00\u003c/td\u003e\n\u003ctd\u003e10:25\u003c/td\u003e\n\u003ctd\u003eOpening Remarks, Keynotes by Barbara Plank and Leshem Choshen\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e10:25\u003c/td\u003e\n\u003ctd\u003e10:55\u003c/td\u003e\n\u003ctd\u003eCoffee Break\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e10:55\u003c/td\u003e\n\u003ctd\u003e11:30\u003c/td\u003e\n\u003ctd\u003eTalk Session 1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e11:30\u003c/td\u003e\n\u003ctd\u003e12:30\u003c/td\u003e\n\u003ctd\u003ePoster Session Part 1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e12:30\u003c/td\u003e\n\u003ctd\u003e14:00\u003c/td\u003e\n\u003ctd\u003eLunch Break\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e14:00\u003c/td\u003e\n\u003ctd\u003e15:00\u003c/td\u003e\n\u003ctd\u003ePoster Session Part 2\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e15:00\u003c/td\u003e\n\u003ctd\u003e15:30\u003c/td\u003e\n\u003ctd\u003eTalk Session 2\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e15:30\u003c/td\u003e\n\u003ctd\u003e16:00\u003c/td\u003e\n\u003ctd\u003eCoffee Break\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e16:00\u003c/td\u003e\n\u003ctd\u003e16:15\u003c/td\u003e\n\u003ctd\u003eTalk Session 3\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e16:15\u003c/td\u003e\n\u003ctd\u003e16:55\u003c/td\u003e\n\u003ctd\u003eKeynote by Ehud Reiter\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e16:55\u003c/td\u003e\n\u003ctd\u003e17:40\u003c/td\u003e\n\u003ctd\u003ePanel Discussion\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e17:40\u003c/td\u003e\n\u003ctd\u003e17:50\u003c/td\u003e\n\u003ctd\u003eClosing Remarks\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"user-content-keynotes\"\u003eKeynotes\u003c/h2\u003e\n\u003ch3 id=\"user-content-keynote-1---barbara-plank\"\u003eKeynote 1 - Barbara Plank\u003c/h3\u003e\n\u003ch4 id=\"user-content-ambiguity-consistency-and-reasoning-in-llms\"\u003eAmbiguity, Consistency and Reasoning in LLMs\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eABSTRACT\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLarge Language Models (LLMs) are powerful yet fallible tools, often struggling with ambiguity, inconsistency, and flawed reasoning. This talk explores some of our recent research exposing these limitations in text and language-vision models. We examine how they misinterpret ambiguous entities, fail to maintain self-consistency, and exhibit biases when these issues remain unresolved. Using insights from controlled studies and new benchmarks, we dissect how models ‚Äúknow‚Äù but often cannot ‚Äúapply‚Äù or ‚Äúverify‚Äù that knowledge. We also highlight a promising intervention ‚Äî vector ablation ‚Äî to surgically address false refusals without sacrificing model accuracy. Together, these findings reveal the critical need for more work on nuanced evaluation and fine-grained control mechanisms in future LLM development.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBIO\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eBarbara Plank is Full Professor for AI and Computational Linguistics at LMU Munich, where she directs the MaiNLP lab and co-directs the Center for Information and Language Processing. She is also an ELLIS Fellow and Visiting Full Professor at IT University of Copenhagen. Her research lab focuses on human-facing NLP: to make NLP models and evaluation more robust and inclusive, so that NLP can deal better with underlying shifts in data due to language variation, is fairer, and embraces human label variation.\u003c/p\u003e\n\u003ch3 id=\"user-content-keynote-2---leshem-choshen\"\u003eKeynote 2 - Leshem Choshen\u003c/h3\u003e\n\u003ch4 id=\"user-content-evaluation-at-the-heart-of-the-ai-wave\"\u003eEvaluation at the Heart of the AI Wave\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eABSTRACT\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe AI wind also blows in the sails of evaluation, creating a mass of evaluation works. In this talk, Leshem will present some of the most pressing and open problems in evaluation and exemplify those with efforts they participated in. These \"blue sea\" problems include pretraining evaluation, unified evaluation, multicultural evaluation, and contamination.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBIO\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLeshem Choshen is a postdoctoral researcher at MIT and MIT-IBM, studying communal LLMs, from community-built LLMs to LLMs for humans and communities. They co-created model merging, TIES merging, and the babyLM pretraining challenge. They are constantly working with the community to gather chats (please contribute), LLM games in textArena and other efforts that call for community involvement. During this work, they emphasize evaluation aspects, including reliable and efficient evaluation, tinyBenchmarks, benchmark agreement testing, and pretraining evaluation.\u003c/p\u003e\n\u003ch3 id=\"user-content-keynote-3---ehud-reiter\"\u003eKeynote 3 - Ehud Reiter\u003c/h3\u003e\n\u003ch4 id=\"user-content-we-should-evaluate-real-world-impact\"\u003eWe Should Evaluate Real-World Impact\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eABSTRACT\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe ACL community has shown very little interest in evaluating the real-world impact of deployed NLP systems. This limits the usefulness and rate of adoption of NLP in areas such as medicine. I will discuss various ways of evaluating real-world impact, and then share the results of a structured survey of the ACL Anthology, which suggests that perhaps 0.1% of its papers evaluate real-world impact; furthermore, most Anthology papers which include impact evaluations present them very sketchily and instead focus on metric evaluations. I will conclude with a discussion of when impact evaluation is appropriate, and steps the community could take to encourage it.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBIO\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eEhud Reiter is a Professor of Computing Science at the University of Aberdeen and was formerly Chief Scientist of Arria NLG (a spinout he cofounded). He has been working on Natural Language Generation for 35 years, and in recent years has focused on healthcare applications and the evaluation of language generation. He is one of the most cited researchers in NLG, and his awards include an INLG Test of Time award for his work on data-to-text. He writes a widely read blog on NLG and evaluation (ehudreiter.com), and wrote a book on NLG which was published in November 2024.\u003c/p\u003e\n\u003ch3 id=\"user-content-panelists\"\u003ePanelists\u003c/h3\u003e\n\u003ch4 id=\"user-content-deuwe-kiela\"\u003eDeuwe Kiela\u003c/h4\u003e\n\u003ch4 id=\"user-content-thiago-castro-ferreira\"\u003eThiago Castro Ferreira\u003c/h4\u003e\n\u003ch4 id=\"user-content-pushkar-mishra\"\u003ePushkar Mishra\u003c/h4\u003e\n\u003ch2 id=\"user-content-sessions-and-papers\"\u003eSessions and Papers\u003c/h2\u003e\n\u003ch3 id=\"user-content-talk-session-1\"\u003eTalk Session 1\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eTitle\u003c/th\u003e\n\u003cth\u003eAuthors\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eReproNLP Shared Task Overview\u003c/td\u003e\n\u003ctd\u003eAnya Belz for the \u003ca href=\"https://repronlp.github.io/\"\u003ehttps://repronlp.github.io/\u003c/a\u003e Team\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs\u003c/td\u003e\n\u003ctd\u003eMinsuh Joo\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"user-content-talk-session-2\"\u003eTalk Session 2\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eTitle\u003c/th\u003e\n\u003cth\u003eAuthors\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eCoKe: Customizable Fine-Grained Story Evaluation via Chain-of-Keyword Rationalization\u003c/td\u003e\n\u003ctd\u003eJoshi Brihi, Sriram Venkatapathy, Mohit Bansal, Nanyun Peng, Haw-Shiuan Chang\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ePapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory\u003c/td\u003e\n\u003ctd\u003eJunho Myung, Yeon Su, Sunwoo Kim, Shin Yoo, Alice Oh\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"user-content-talk-session-3\"\u003eTalk Session 3\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eTitle\u003c/th\u003e\n\u003cth\u003eAuthors\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003ePsycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans\u003c/td\u003e\n\u003ctd\u003eJavier Conde, Miguel Gonzalez, Maria Grandury, Pedro Reviriego, Gonzalo Martinez, Marc Brysbaert\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"user-content-poster-session---in-person\"\u003ePoster Session - In-Person\u003c/h3\u003e\n\u003cp\u003eAll posters can be presented during both parts of the split poster session (with lunch break in between).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDoes Biomedical Training Lead to Better Medical Performance? Amin Dada, Marie Bauer, Jean-Philippe Corbeil, Amanda Butler, Osman Alperen, Constantin Marc, Kaleb E, Julian Friedrich, Jens Kleesiek\u003c/li\u003e\n\u003cli\u003eHEDS 3.0: The Human Evaluation Data Sheet Version 3.0 Anya Belz, Craig Thomson\u003c/li\u003e\n\u003cli\u003eARGENT: Automatic Reference-free Evaluation for Open-Ended Text Generation without Source Inputs Xinyue Zhang, Agathe Zecevic, Sebastian Zeki, Angus Roberts\u003c/li\u003e\n\u003cli\u003eAre LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool for Perceived Socio-Economic Bias in LLMs Jasmin Wachter, Michael Radloff, Maja Smolej, Katharina Kinder-Kurlanda\u003c/li\u003e\n\u003cli\u003eKnockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons Isik Baran, Tu Anh, Jan Niehues, ?? ??\u003c/li\u003e\n\u003cli\u003eFree-text Rationale Generation under Readability Level Control Yi-Sheng Hsu, Nils Feldhus, Sherzod Hakimov\u003c/li\u003e\n\u003cli\u003eSelective Shot Learning for Code Explanation Paheli Bhattacharya, Rishabh Gupta\u003c/li\u003e\n\u003cli\u003eCan LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation? Evangelia Gogoulou, Shorouq Zahra, Liane Guillou, Luise D√ºrlich, Joakim Nivre\u003c/li\u003e\n\u003cli\u003eEvaluating LLMs with Multiple Problems at once Zhengxiang Wang, Jordan Kodner, Owen Rambow\u003c/li\u003e\n\u003cli\u003eLearning and Evaluating Factual Clarification Question Generation Without Examples Matthew Toles, Yukun Huang, Zhou Yu\u003c/li\u003e\n\u003cli\u003eSECQUE: A Benchmark for Evaluating Real-World Financial Analysis Capabilities Noga BenYoash, Menachem Brief, Oded Ovadia, Gil Shenderovitz, Moshik Mishaeli, Rachel Lemberg, Eitam Sheetrit\u003c/li\u003e\n\u003cli\u003eOne ruler to measure them all: Benchmarking multilingual long-context language models Yekyung Kim, Jenna Russell, Marzena Karpinska, Mohit Iyyer\u003c/li\u003e\n\u003cli\u003eMeasure only what is measurable: towards conversation requirements for evaluating task-oriented dialogue systems Emiel van, Anouck Braggaar, Emmelyn Croes, Florian Kunneman, Christine Liebrecht, Gabri√´lla Martijn\u003c/li\u003e\n\u003cli\u003eAre Bias Evaluation Methods Biased ? Lina Berrayana, Sean Rooney, Luis Garc√©s-Erice, Ioana Giurgiu\u003c/li\u003e\n\u003cli\u003eIRSum: One Model to Rule Summarization and Retrieval Sotaro Takeshita, Simone Paolo, Kai Eckert\u003c/li\u003e\n\u003cli\u003eMetric assessment protocol in the context of answer fluctuation on MCQ tasks Ekaterina Goliakova, Xavier Renard, Marie-Jeanne Lesot, Thibault Laugel, Christophe Marsala, Marcin Detyniecki\u003c/li\u003e\n\u003cli\u003eCUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset Jind≈ôich Helcl, Andrei-Alexandru Manea, Gianluca Vico, Jind≈ôich Libovick√Ω\u003c/li\u003e\n\u003cli\u003eUsing LLM-as-judge Evaluation for Sanity-checking Results and Reproducibility of Human Evaluations of NLP Systems Rudali Huidrom, Anya Belz\u003c/li\u003e\n\u003cli\u003eHuGME: A benchmark system for evaluating Hungarian generative LLMs No√©mi Ligeti-Nagy, G√°bor Madar√°sz, Fl√≥ra F√∂ldesi, P√©ter Hatvani, Mariann Lengyel, M√°ty√°s Osv√°th, Bence S√°rossy, Krist√≥f Varga, Gy≈ëz≈ë Zijian, Enik≈ë H√©ja, Tam√°s V√°radi, G√°bor Pr√≥sz√©ky\u003c/li\u003e\n\u003cli\u003eCacheSaver: A Modular Framework for Efficient, Cost-Effective, and Reproducible LLM Inference Nearchos Potamitis, Lars Henning, Laurent Bindschaedler, Niket Tandon, Akhil Arora\u003c/li\u003e\n\u003cli\u003eOpeNLGauge: An Explainable Metric for NLG Evaluation with Open-Weights LLMs Ivan Kart√°ƒç, Mateusz Lango, Ondrej Dusek\u003c/li\u003e\n\u003cli\u003eInvestigating the Robustness of Retrieval-Augmented Generation at the Query Level Sezen Per√ßin, Xin Su, Qutub Sha, Phillip Howard, Aleksei Kuvshinov, Leo Schwinn, Kay-Ulrich Scholl\u003c/li\u003e\n\u003cli\u003eSourcing Fresh Resources for Table-to-Text Generation Evaluation Krist√Ωna Onderkov√°, Ondrej Platek, Zdenƒõk Kasner, Ondrej Dusek\u003c/li\u003e\n\u003cli\u003eBig Escape Benchmark: Evaluating Human-Like Reasoning in Language Models via Real-World Escape Room Challenges Zinan Tang, QiYao Sun\u003c/li\u003e\n\u003cli\u003eEvent-based evaluation of abstractive news summarization Huiling You, Samia Touileb, Lilja √òvrelid, Erik Velldal\u003c/li\u003e\n\u003cli\u003ePrompt-Based Evolution for Diverse and Generalizable Toxic Language Datasets Iago Alves, Julia Soares, Fernanda Bufon, Diogo Fernandes, Arlindo Rodrigues\u003c/li\u003e\n\u003cli\u003eFaithfulness Metrics Do Not Generalize Well: A Case Study in Summarization Patr√≠cia Schmidtov√°, Ondrej Dusek, Saad Mahamood\u003c/li\u003e\n\u003cli\u003eFine-Tune on the Format: First Improving Multiple-Choice Evaluation for Intermediate LLM Checkpoints Alec Bunn, Ben Bogin, Sarah Wiegreffe\u003c/li\u003e\n\u003cli\u003eImproving Large Language Model Confidence Estimates using Extractive Rationales for Classification Jane Arleth, Iris Hendrickx, Martha Larson\u003c/li\u003e\n\u003cli\u003eReproHum #0729-04: Human Evaluation Reproduction Report for \"MemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes\" Simeon Junker\u003c/li\u003e\n\u003cli\u003eReproHum #0031-01: Reproducing the Human Evaluation of Readability from \"It is AI‚Äôs Turn to Ask Humans a Question\" Daniel Braun\u003c/li\u003e\n\u003cli\u003eReproHum #0033-05: Human Evaluation of Factuality from A Multidisciplinary Perspective Andra-Maria Florescu, Marius Miclu»õa-C√¢mpeanu, Stefana Arina, Liviu P\u003c/li\u003e\n\u003cli\u003eReproHum: #0744-02: Investigating the Reproducibility of Semantic Preservation Human Evaluations Mohammad Arvan, Natalie Parde\u003c/li\u003e\n\u003cli\u003eReproHum #0669-08: Reproducing Sentiment Transfer Evaluation Krist√Ωna Onderkov√°, Mateusz Lango, Patr√≠cia Schmidtov√°, Ondrej Dusek\u003c/li\u003e\n\u003cli\u003eReproHum #0729-04: Partial reproduction of the human evaluation of the MemSum and NeuSum summarisation systems Simon Mille, Michela Lorandi\u003c/li\u003e\n\u003cli\u003eCurse of bilinguality: Evaluating monolingual and bilingual language models on Chinese linguistic benchmarks Yuwen Zhou, Yevgen Matusevych\u003c/li\u003e\n\u003cli\u003eTowards Better Open-Ended Text Generation: A Multicriteria Evaluation Framework Matthias A√üenmacher, Esteban Garces\u003c/li\u003e\n\u003cli\u003eBridging the LLM Accessibility Divide? Performance, Fairness, and Cost of Closed versus Open LLMs for Automated Essay Scoring Kezia Oketch, John  Lalor\u003c/li\u003e\n\u003cli\u003ePrompt, Translate, Fine-Tune, Re-Initialize, or Instruction-Tune? Adapting LLMs for In-Context Learning in Low-Resource Languages Christopher Toukmaji\u003c/li\u003e\n\u003cli\u003eWinning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in QA Agents Ashley Lewis\u003c/li\u003e\n\u003cli\u003eDo Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts? Sohee Yang\u003c/li\u003e\n\u003cli\u003eAd-hoc Concept Forming in the Game Codenames as a Means for Evaluating Large Language Models Sherzod Hakimov, David Schlangen\u003c/li\u003e\n\u003cli\u003eEvaluating Grounded Reasoning by Code-Assisted LLMs for Mathematics Zena Al\u003c/li\u003e\n\u003cli\u003eFrom Calculation to Adjudication: Examining LLM Judges on Mathematical Reasoning Tasks Andreas Stephan\u003c/li\u003e\n\u003cli\u003ePersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins John Lalor\u003c/li\u003e\n\u003cli\u003eCoreference as an indicator of context scope in multimodal narrative Nikolai Ilinykh, Shalom Lappin, Asad B., Sharid Lo√°iciga\u003c/li\u003e\n\u003cli\u003ePATCH! {P}sychometrics-{A}ssis{T}ed Ben{CH}marking of Large Language Models against Human Populations: A Case Study of Proficiency in 8th Grade Mathematics Qixiang Fang\u003c/li\u003e\n\u003cli\u003eMCQFormatBench: Robustness Tests for Multiple-Choice Questions Hiroo Takizawa\u003c/li\u003e\n\u003cli\u003e(Dis)improved?! How Simplified Language Affects Large Language Model Performance across Languages Miriam Ansch√ºtz, Anastasiya Damaratskaya, Chaeeun Joy, Arthur Schmalz, Edoardo Mosca, Georg Groh\u003c/li\u003e\n\u003cli\u003eFinance Language Model Evaluation (FLaME) Glenn Matlin, Mika Okamoto, Huzaifa Pardawala, Yang Yang, Sudheer Chava\u003c/li\u003e\n\u003cli\u003esPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting Sanchit Ahuja\u003c/li\u003e\n\u003cli\u003eSingle- vs. Dual-Prompt Dialogue Generation With LLMs For Job Interviews In Human Resources Joachim De\u003c/li\u003e\n\u003cli\u003eU-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs Vitaliy Polshkov\u003c/li\u003e\n\u003cli\u003e[Findings] Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences? Yingjin Song, Albert Gatt\u003c/li\u003e\n\u003cli\u003e[Findings] Structured Discourse Representation for Factual Consistency Verification Kun Zhang\u003c/li\u003e\n\u003cli\u003e[Findings] Evaluating LLMs‚Äô Assessment of Mixed-Context Hallucination Through the Lens of Summarization Siya Qi\u003c/li\u003e\n\u003cli\u003e[Findings] Assessing the Reasoning Capabilities of LLMs in the context of Evidence-based Claim Verification Mahmud Akhter, Maria Liakata\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"user-content-poster-session---virtual\"\u003ePoster Session - Virtual\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eMulti-Dimensional Evaluation of Open-Source Language Models: Based on Machine Learning and Bayesian Optimization Qingchen Yu\u003c/li\u003e\n\u003cli\u003eSpatial Representation of Large Language Models in 2D Scene WenyaWu Weihong Deng\u003c/li\u003e\n\u003cli\u003eThe Fellowship of the LLMs: Multi-Model Workflows for Synthetic Preference Optimization Dataset Generation Samee Arif, Sualeha Farid, Abdul Hameed, Awais Athar, Agha Ali\u003c/li\u003e\n\u003cli\u003eLeveraging LLM-based sentiment analysis for portfolio optimization with proximal policy optimization Kemal Kirtac, Guido Germano\u003c/li\u003e\n\u003cli\u003eEvaluating LLMs Beyond Standard Text: A Benchmark on Non-Traditional Text Variations Jihyun Kim, Yejee Kim, HyunJeong Kang, Sumyeong Kim, Minji Son, Hyeseung Han, Kyungwoo Song\u003c/li\u003e\n\u003cli\u003eVision-Language Models Are Not Pragmatically Competent in Referring Expression Generation Ziqiao Ma, Jing Ding, Xuejun Zhang, Dezhi Luo, Jiahe Ding, Sihan Xu, Yuchen Huang, Run Peng, Joyce Chai\u003c/li\u003e\n\u003cli\u003eCan Perplexity Predict Finetuning Performance? An Investigation of Tokenization Effects on Sequential Language Models for Nepali Nishant Luitel, Nirajan Bekoju, Anand Kumar, Subarna Shakya\u003c/li\u003e\n\u003cli\u003eModeling the One-to-Many Property in Open-Domain Dialogue with LLMs Jing Yang, Kong Aik, Woon-Seng Gan\u003c/li\u003e\n\u003cli\u003e(Towards) Scalable Reliable Automated Evaluation with Large Language Models Bertil Braun, Martin Forell\u003c/li\u003e\n\u003cli\u003eClustering Zero-Shot Uncertainty Estimations to Assess LLM Response Accuracy for Yes/No Q\u0026#x26;A Christopher T., Amy Vennos, W. Graham, Daniel Dakota,\u003c/li\u003e\n\u003cli\u003eJudging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges Christopher T., Amy Vennos, W. Graham, Daniel Dakota\u003c/li\u003e\n\u003cli\u003eAre AI Datasets Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts German Gritsai, Anastasia Voznyuk, Andrey Grabovoy, Yury Chekhovich\u003c/li\u003e\n\u003cli\u003eAnalyzing the Sensitivity of Vision Language Models in Visual Question Answering Monika shah, Sudarshan Balaji, Somdeb Sarkhel, Sanorita Dey, Deepak Venugopal\u003c/li\u003e\n\u003cli\u003eELAB: Extensive LLM Alignment Benchmark in Persian Language Zahra Pourbahman, Fatemeh rajabi, Mohammadhossein Sadeghi, Omid Ghahroodi, Somayeh Bakhshaei, Arash Amini, Reza Kazemi, Mahdieh Soleymani\u003c/li\u003e\n\u003cli\u003eEvaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish Elif Ecem, Ay≈üe Aysu, Ahmet Kaan, Seyma Erdem, Burak Aytan, Busra Tufan, Abdullah Topraksoy, Esra Darici, Cagri Toraman\u003c/li\u003e\n\u003cli\u003eShallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data? Xuan Qi, Jiahao Qiu, Xinzhe Juan, Yue Wu, Mengdi Wang\u003c/li\u003e\n\u003cli\u003eReproHum #0744-02: A Reproduction of the Human Evaluation of Meaning Preservation in ``Factorising Meaning and Form for Intent-Preserving Paraphrasing'' Julius Steen, Katja Markert\u003c/li\u003e\n\u003cli\u003eReproHum #0067-01: A Reproduction of the Evaluation of Cross-Lingual Summarization Supryadi Chuang Liu, Deyi Xiong\u003c/li\u003e\n\u003cli\u003eFine-Grained Constraint Generation-Verification for Improved Instruction-Following Zhixiang Liang, Zhenyu Hou, Xiao Wang\u003c/li\u003e\n\u003cli\u003eNatural Language Counterfactual Explanations in Financial Text Classification: A Comparison of Generators and Evaluation Metrics Karol Dobiczek\u003c/li\u003e\n\u003cli\u003eAn Analysis of Datasets, Metrics and Models in Keyphrase Generation Florian Boudin, Akiko Aizawa\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"user-content-important-dates\"\u003eImportant Dates\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003eJuly 31\u003c/code\u003e Workshop Date\u003c/p\u003e\n\u003ch3 id=\"user-content-organization\"\u003eOrganization\u003c/h3\u003e\n\u003ch2 id=\"user-content-organizers\"\u003eOrganizers\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSebastian Gehrmann, Bloomberg\u003c/li\u003e\n\u003cli\u003eGabriel Stanovsky, Hebrew University of Jerusalem\u003c/li\u003e\n\u003cli\u003eSimon Mille, Dublin City University\u003c/li\u003e\n\u003cli\u003eEnrico Santus, Bloomberg\u003c/li\u003e\n\u003cli\u003eMiruna Clinciu, Heriot Watt University\u003c/li\u003e\n\u003cli\u003eKaustubh Dhole, Emory University\u003c/li\u003e\n\u003cli\u003eYotam Perlitz, IBM Research\u003c/li\u003e\n\u003cli\u003eRotem Dror, University of Haifa\u003c/li\u003e\n\u003cli\u003eItay Itzhak, Hebrew University of Jerusalem\u003c/li\u003e\n\u003cli\u003eOfir Arviv, IBM Research\u003c/li\u003e\n\u003cli\u003eEliya Habba, Hebrew University of Jerusalem\u003c/li\u003e\n\u003cli\u003eMichal Shmueli Scheuer, IBM Research\u003c/li\u003e\n\u003cli\u003eJo√£o Sedoc, New York University\u003c/li\u003e\n\u003cli\u003eOyvind Tafjord, Allen Institute for Artificial Intelligence\u003c/li\u003e\n\u003c/ul\u003e\n"}},"__N_SSG":true},"page":"/workshop","query":{},"buildId":"64ekgEf2Sn1vpQfHbMHiq","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>