<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="Benchmark natural language generation systems with GEM."/><meta property="og:image" content="https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"/><meta name="og:title" content="GEM"/><meta name="twitter:card" content="summary_large_image"/><title>GEM Workshop 2023</title><meta name="next-head-count" content="8"/><link rel="preload" href="/_next/static/css/86a77084a15a5546.css" as="style"/><link rel="stylesheet" href="/_next/static/css/86a77084a15a5546.css" data-n-g=""/><link rel="preload" href="/_next/static/css/50ad98e60bd49ad7.css" as="style"/><link rel="stylesheet" href="/_next/static/css/50ad98e60bd49ad7.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-a73844ba913878ac.js" defer=""></script><script src="/_next/static/chunks/framework-7a7e500878b44665.js" defer=""></script><script src="/_next/static/chunks/main-a56c17dda72126ba.js" defer=""></script><script src="/_next/static/chunks/pages/_app-da8862f0ec3a97c1.js" defer=""></script><script src="/_next/static/chunks/c16184b3-ddb1b99b5e568a2a.js" defer=""></script><script src="/_next/static/chunks/50-3dccc3616b494db8.js" defer=""></script><script src="/_next/static/chunks/pages/workshop-ab0e5c9fcf25aeda.js" defer=""></script><script src="/_next/static/V8ziX7lLuetLbU22tSnhs/_buildManifest.js" defer=""></script><script src="/_next/static/V8ziX7lLuetLbU22tSnhs/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="layout_background__oCFQX undefined"><header class="layout_header__SFlEE"><div class="navbar_navwrapper__RkXSe"><div class="navbar_gradbar__Vli6s"></div><nav class="navbar_navbar__vdWdK"><span class="utils_headingLg__RYtYb navbar_navbarlogo__u28NK"><a href="/">GEM BENCHMARK</a></span><div class="navbar_menutoggle__4Urrc" id="mobile-menu"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars navbar_bar__f8cyd" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg></div><ul><li class="navbar_navitem__15TsF navbar_pushright___9_8s"><a href="/resources">Resources</a></li><li class="navbar_navitem__15TsF"><a href="/data_cards">Data Cards</a></li><li class="navbar_navitem__15TsF"><a href="/model_cards">Model Cards</a></li><li class="navbar_navitem__15TsF"><a href="/tutorials">tutorials</a></li><li class="navbar_navitem__15TsF"><a href="/results">Results</a></li><li class="navbar_navitem__15TsF"><a href="/papers">Papers</a></li><li class="navbar_navitem__15TsF"><a href="/workshop">Workshop</a></li></ul></nav></div></header><div class="layout_container__FUycR undefined"><main><article><span class="utils_headingXl__zlq1q">GEM üíé Workshop at EMNLP 2023</span><span class="utils_smallSpace__dcJPu"></span><div><p>The Third Version of the <a href="https://gem-benchmark.com/">Generation, Evaluation &#x26; Metrics (GEM) Workshop</a> will be held as part of <a href="https://2023.emnlp.org/">EMNLP</a>, üìÖ December 6, 2023.</p>
<h2 id="user-content-overview">Overview</h2>
<p>Many new NLP applications are cast through the lens of natural language generation. With the advent of these new approaches, many opportunities arise: generation in previously less studied languages, new evaluation paradigms, methods for corpus creation, more efficient architectures, strategies for safe deployments, among many others. At the same time, we can learn from the rich history of NLG research to further improve generation methods.
These developments require robust and sound NLG evaluation processes. To that end, the GEM workshop aims to encourage the development of model auditing and human evaluation strategies, and to popularize model evaluations in languages beyond English.</p>
<p>If you are interested, you can check out last year's workshop websites from <a href="/workshop/2021">ACL 2021</a> and <a href="/workshop/2022">EMNLP 2022</a>. Our call for this workshop can be found <a href="/workshop/2023-call">here</a>.</p>
<h2 id="user-content-schedule">Schedule</h2>
<p>All times in local Singapore Time, please use a converter like <a href="https://www.timeanddate.com/worldclock/converter.html?iso=20231108T180000&#x26;p1=236">this one</a> to if you are in a different time zone.
To accomodate attendees from as many time zones as possible, we will have a virtual-only part in the evening.</p>
<table>
<thead>
<tr>
<th>Start</th>
<th>End</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>9:00</td>
<td>10:30</td>
<td>Opening Remarks + 6 x 10+2 minutes talk</td>
</tr>
<tr>
<td>10:30</td>
<td>11:00</td>
<td>Coffee Break</td>
</tr>
<tr>
<td>11:00</td>
<td>12:30</td>
<td>Poster Session I</td>
</tr>
<tr>
<td>12:30</td>
<td>14:00</td>
<td>Lunch Break</td>
</tr>
<tr>
<td>14:00</td>
<td>15:30</td>
<td>7 x 10+2 minutes talk</td>
</tr>
<tr>
<td>15:30</td>
<td>16:00</td>
<td>Coffee Break</td>
</tr>
<tr>
<td>16:00</td>
<td>17:30</td>
<td>Poster Session II</td>
</tr>
</tbody>
</table>
<h2 id="user-content-sessions-and-papers">Sessions and Papers</h2>
<h3 id="user-content-talk-session-i-900-1030am">Talk Session I (9:00-10:30am)</h3>
<table>
<thead>
<tr>
<th>ID</th>
<th>Type</th>
<th>Title</th>
<th>Authors</th>
</tr>
</thead>
<tbody>
<tr>
<td>271</td>
<td>Findings</td>
<td>Vector-Quantized Prompt Learning for Paraphrase Generation</td>
<td>Haotian Luo, Yixin Liu, Peidong Liu, Xianggen Liu</td>
</tr>
<tr>
<td>1154</td>
<td>Findings</td>
<td>Can Large Language Models Fix Data Annotation Errors? An Empirical Study Using Debatepedia for Query-Focused Text Summarization</td>
<td>Md Tahmid Rahman Laskar, Mizanur Rahman, Israt Jahan, Enamul Hoque, Jimmy Huang</td>
</tr>
<tr>
<td>1562</td>
<td>Findings</td>
<td>Geographical Erasure in Language Generation</td>
<td>Pola Schw√∂bel, Jacek Golebiowski, Michele Donini, Cedric Archambeau, Danish Pruthi</td>
</tr>
<tr>
<td>1834</td>
<td>Findings</td>
<td>A Comprehensive Evaluation of Tool-Assisted Generation Strategies</td>
<td>Alon Jacovi, Avi Caciularu, Jonathan Herzig, Roee Aharoni, Bernd Bohnet, Mor Geva</td>
</tr>
<tr>
<td>5166</td>
<td>Findings</td>
<td>‚ÄúKelly is a Warm Person, Joseph is a Role Model‚Äù: Gender Biases in LLM-Generated Reference Letters</td>
<td>Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang, Nanyun Peng</td>
</tr>
<tr>
<td>11</td>
<td>Main Track</td>
<td>FACTSCORE: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</td>
<td>Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer and Hannaneh Hajishirzi</td>
</tr>
</tbody>
</table>
<h3 id="user-content-poster-session-i-1100-1230pm">Poster Session I (11:00-12:30pm)</h3>
<table>
<thead>
<tr>
<th>ID</th>
<th>Type</th>
<th>Title</th>
<th>Authors</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>Main Track</td>
<td>Contextualizing the Limits of Model &#x26; Evaluation Dataset Curation on Semantic Similarity Classification Tasks</td>
<td>Daniel Theron</td>
</tr>
<tr>
<td>4</td>
<td>Main Track</td>
<td>Dialogue Quality and Emotion Annotations for Customer Support Conversations</td>
<td>John Mendonca, Patr√≠cia Pereira, Miguel Menezes, Vera Cabarr√£o, Ana C Farinha, Helena Moniz, Alon Lavie and Isabel Trancoso</td>
</tr>
<tr>
<td>7</td>
<td>Main Track</td>
<td>Formalizing content creation and evaluation methods for AI-generated social media content</td>
<td>Christian Jensen and Axel H√∏jmark</td>
</tr>
<tr>
<td>9</td>
<td>Main Track</td>
<td>Automatic Evaluation of Generative Models with Instruction Tuning</td>
<td>Shuhaib Mehri and Vered Shwartz</td>
</tr>
<tr>
<td>12</td>
<td>Main Track</td>
<td>Effective Proxy for Human Labeling: Ensemble Disagreement Scores in Large Language Models for Industrial NLP</td>
<td>Wei Du, Laksh Advani, Yashmeet Gambhir, Daniel Perry, Prashant Shiralkar, Zhengzheng Xing and Aaron Colak</td>
</tr>
<tr>
<td>14</td>
<td>Main Track</td>
<td>Automatic Reflection Generation for Peer-to-Peer Counseling</td>
<td>Emma O'Neil, Jo√£o Sedoc, Diyi Yang, Haiyi Zhu and Lyle Ungar</td>
</tr>
<tr>
<td>16</td>
<td>Main Track</td>
<td>One-Shot and Few-Shot Exemplification Modeling</td>
<td>John Harvill, Hee Suk Yoon, Eunseop Yoon, Mark Hasegawa-Johnson and Chang Yoo</td>
</tr>
<tr>
<td>21</td>
<td>Main Track</td>
<td>QAMPARI: A Benchmark for Open-domain Questions with Many Answers</td>
<td>Samuel Amouyal, Tomer Wolfson, Ohad Rubin, Ori Yoran, Jonathan Herzig and Jonathan Berant</td>
</tr>
<tr>
<td>23</td>
<td>Main Track</td>
<td>Unveiling Safety Vulnerabilities of Large Language Models</td>
<td>George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby Tavor, Orna Raz and Eitan Farchi</td>
</tr>
<tr>
<td>24</td>
<td>Main Track</td>
<td>Adapting Pre-trained Generative Models for Extractive Question Answering</td>
<td>Prabir Mallick, Tapas Nayak and Indrajit Bhattacharya</td>
</tr>
<tr>
<td>25</td>
<td>Main Track</td>
<td>Predicting Question-Answering Performance of Large Language Models through Semantic Consistency</td>
<td>Ella Rabinovich, Samuel Ackerman, Orna Raz, Eitan Farchi and Ateret Anaby Tavor</td>
</tr>
<tr>
<td>28</td>
<td>Main Track</td>
<td>Towards Effective Long-Form QA with Evidence Augmentation</td>
<td>Mengxia Yu, Sara Rosenthal, Mihaela Bornea and Avi Sil</td>
</tr>
<tr>
<td>30</td>
<td>Main Track</td>
<td>Harnessing the Plug-and-Play Controller by Prompting</td>
<td>Hao Wang and Lei Sha</td>
</tr>
<tr>
<td>32</td>
<td>Main Track</td>
<td>Context and Literacy Aware Learnable Metric for Text Simplification</td>
<td>Jeongwon Kwak, Hyeryun Park, Kyungmo Kim and Jinwook Choi</td>
</tr>
<tr>
<td>33</td>
<td>Main Track</td>
<td>Synthetic Dialogue Dataset Generation using LLM Agents</td>
<td>Yelaman Abdullin, Diego Molla, Bahadorreza Ofoghi, John Yearwood and Qingyang Li</td>
</tr>
<tr>
<td>34</td>
<td>Main Track</td>
<td>An Empirical Bayes Framework for Open-Domain Dialogue Generation</td>
<td>Jing Yang Lee, Kong Aik Lee and Woon Seng Gan</td>
</tr>
<tr>
<td>38</td>
<td>Main Track</td>
<td>ChatGPT as a Java Decompiler</td>
<td>Bradley McDanel and Zhanhao Liu</td>
</tr>
<tr>
<td>43</td>
<td>Main Track</td>
<td>Targeted Image Data Augmentation Increases Basic Skills Captioning Robustness</td>
<td>Valentin Barriere, Felipe del Rio, Andres Carvallo, Carlos Aspillaga, Eugenio Herrera-Berg and Cristian Buc</td>
</tr>
<tr>
<td>45</td>
<td>Main Track</td>
<td>Separating form and meaning: Using self-consistency to quantify task understanding across multiple senses</td>
<td>Xenia Ohmer, Elia Bruni and Dieuwke Hupkes</td>
</tr>
<tr>
<td>46</td>
<td>Main Track</td>
<td>Text Encoders Lack Knowledge: Leveraging Generative LLMs for Domain-Specific Semantic Textual Similarity</td>
<td>Joseph Gatto, Omar Sharif, Parker Seegmiller, Philip Bohlman and Sarah Masud Preum</td>
</tr>
<tr>
<td>51</td>
<td>Main Track</td>
<td>To Burst or Not to Burst: Generating and Quantifying Improbable Text</td>
<td>Kuleen Sasse, Efsun Sarioglu Kayi, Samuel Barham and Edward Staley</td>
</tr>
<tr>
<td>52</td>
<td>Main Track</td>
<td>Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs</td>
<td>Xue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen and Shashi Bhushan TN</td>
</tr>
<tr>
<td>54</td>
<td>Main Track</td>
<td>RankAug: Augmented data ranking for text classification</td>
<td>Tiasa Singha Roy and Priyam Basu</td>
</tr>
<tr>
<td>67</td>
<td>Main Track</td>
<td>Post Turing: Mapping the landscape of LLM Evaluation</td>
<td>Alexey Tikhonov and Ivan Yamshchikov</td>
</tr>
<tr>
<td>62</td>
<td>Main Track</td>
<td>PersonalityChat: Conversation Distillation for Personalized Dialog Modeling with Facts and Traits</td>
<td>Ehsan Lotfi, Maxime De Bruyn, Jeska Buhmann and Walter Daelemans</td>
</tr>
<tr>
<td>63</td>
<td>Main Track</td>
<td>How well ChatGPT understand Malaysian English? An Evaluation on Named Entity Recognition and Relation Extraction</td>
<td>MohanRaj Chanthran, Lay-Ki Soon, Ong Huey Fang and Bhawani Selvaretnam</td>
</tr>
<tr>
<td>57</td>
<td>Extended Abstract</td>
<td>Robust Tooling and New Resources for Large Language Model Evaluation via Catwalk</td>
<td>Kyle Richardson, Ian Magnusson, Oyvind Tafjord,Akshita Bhagia, Iz Beltagy, Arman Cohan, Pradeep Dasigi,Jesse Dodge, Dirk Groeneveld, Yuling Gu, Ananya Harsh Jha, Tushar Khot and Nishant Subramani</td>
</tr>
<tr>
<td>58</td>
<td>Extended Abstract</td>
<td>GUMSum: Multi-Genre Data and Evaluation for English Abstractive Summarization</td>
<td>Yang Janet Liu and Amir Zeldes</td>
</tr>
<tr>
<td>60</td>
<td>Extended Abstract</td>
<td>NewsMet: A ‚ÄòDo It All' dataset of contemporary Metaphors in News headlines</td>
<td>Rohan Joseph, Timothy Liu, Aik Beng Ng, Simon See and Sunny Rai</td>
</tr>
<tr>
<td>20</td>
<td>Extended Abstract</td>
<td>On the State of German (Abstractive) Text Summarization</td>
<td>Dennis Aumiller, Jing Fan and Michael Gertz</td>
</tr>
<tr>
<td>31</td>
<td>Extended Abstract</td>
<td>Measuring misogyny in natural language generation: preliminary results from a case study on two Reddit communities</td>
<td>Aaron Snoswell, Lucinda Nelson, Hao Xue, Flora Salim, Nicolas Suzor and Jean Burgess</td>
</tr>
<tr>
<td>35</td>
<td>Extended Abstract</td>
<td>On the Learnability of Watermarks for Language Models</td>
<td>Chenchen Gu, Xiang Lisa Li, Percy Liang and Tatsunori Hashimoto</td>
</tr>
<tr>
<td>47</td>
<td>Extended Abstract</td>
<td>Does Writing with Language Models Reduce Content Diversity?</td>
<td>Vishakh Padmakumar and He He</td>
</tr>
</tbody>
</table>
<h3 id="user-content-talk-session-ii-1400-1530pm">Talk Session II (14:00-15:30pm)</h3>
<table>
<thead>
<tr>
<th>ID</th>
<th>Type</th>
<th>Title</th>
<th>Authors</th>
</tr>
</thead>
<tbody>
<tr>
<td>36</td>
<td>Main Track</td>
<td>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models</td>
<td>Joseph Marvin Imperial and Harish Tayyar Madabushi</td>
</tr>
<tr>
<td>41</td>
<td>Main Track</td>
<td>Multi-domain Summarization from Leaderboards to Practice: Re-examining Automatic and Human Evaluation</td>
<td>David Demeter, Oshin Agarwal, Simon Ben Igeri, Marko Sterbentz, Neil Molino, John Conroy and Ani Nenkova</td>
</tr>
<tr>
<td>56</td>
<td>Main Track</td>
<td>Elo Uncovered: Robustness and Best Practices in Language Model Evaluation</td>
<td>Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker and Marzieh Fadaee</td>
</tr>
<tr>
<td>39</td>
<td>Extended Abstract</td>
<td>Generative language models exhibit social identity biases</td>
<td>Tiancheng Hu, Yara Kyrychenko, Jon Roozenbeek and Nigel Collier</td>
</tr>
<tr>
<td>70</td>
<td>Industry Track</td>
<td>A Simple yet Efficient Ensemble Approach for AI-generated Text Detection</td>
<td>Harika Abburi, Kalyani Roy, Michael Suesserman, Nirmala Pudota, Balaji Veeramani, Edward Bowen and Sanmitra Bhattacharya</td>
</tr>
<tr>
<td>17</td>
<td>Industry Track</td>
<td>Leveraging Large Language Models for Enhanced Product Descriptions in eCommerce</td>
<td>Jianghong Zhou, Bo Liu, Jhalak Nilesh Acharya, Yao Hong, Kuang-chih Lee and Musen Wen</td>
</tr>
<tr>
<td>55</td>
<td>Industry Track</td>
<td>Separating the Wheat from the Chaff with BREAD: An open-source benchmark and metrics to detect redundancy in text</td>
<td>Isaac Caswell, Lisa Wang and Isabel Papadimitriou</td>
</tr>
</tbody>
</table>
<h3 id="user-content-poster-session-ii-1600-1730pm">Poster Session II (16:00-17:30pm)</h3>
<table>
<thead>
<tr>
<th>ID</th>
<th>Type</th>
<th>Title</th>
<th>Authors</th>
</tr>
</thead>
<tbody>
<tr>
<td>223</td>
<td>Findings</td>
<td>MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space</td>
<td>Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, Xueqi Cheng, Tat-Seng Chua</td>
</tr>
<tr>
<td>300</td>
<td>Findings</td>
<td>DeltaScore: Story Evaluation with Perturbations</td>
<td>Zhuohan Xie, Miao Li, Trevor Cohn, Jey Han Lau</td>
</tr>
<tr>
<td>469</td>
<td>Findings</td>
<td>Show, Write, and Retrieve: Entity-aware Article Generation and Retrieval</td>
<td>Zhongping Zhang, Yiwen Gu, Bryan A. Plummer</td>
</tr>
<tr>
<td>575</td>
<td>Findings</td>
<td>Adversarial Text Generation by Search and Learning</td>
<td>Guoyi Li, Bingkang Shi, Zongzhen Liu, Dehan Kong, Yulei Wu, Xiaodan Zhang, Longtao Huang, Honglei Lyu</td>
</tr>
<tr>
<td>651</td>
<td>Findings</td>
<td>On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study</td>
<td>Polina Zablotskaia, Du Phan, Joshua Maynez, Shashi Narayan, Jie Ren, Jeremiah Zhe Liu</td>
</tr>
<tr>
<td>731</td>
<td>Findings</td>
<td>GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence</td>
<td>Zhihua Wen, Zhiliang Tian, Wei Wu, Yuxin Yang, Yanqi Shi, Zhen Huang, Dongsheng Li</td>
</tr>
<tr>
<td>963</td>
<td>Findings</td>
<td>A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing</td>
<td>Carlos G√≥mez-Rodr√≠guez, Paul Williams</td>
</tr>
<tr>
<td>1470</td>
<td>Findings</td>
<td>Uniform Complexity for Text Generation</td>
<td>Joseph Marvin Imperial, Harish Tayyar Madabushi</td>
</tr>
<tr>
<td>1548</td>
<td>Findings</td>
<td>Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance</td>
<td>Thiemo Wambsganss, Xiaotian Su, Vinitra Swamy, Seyed Parsa Neshaei, Roman Rietsche, Tanja K√§ser</td>
</tr>
<tr>
<td>1807</td>
<td>Findings</td>
<td>Miracle: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control</td>
<td>Zhenyi Lu, Wei Wei, Xiaoye Qu, Xian-Ling Mao, Dangyang Chen, Jixiong Chen</td>
</tr>
<tr>
<td>1897</td>
<td>Findings</td>
<td>Stylized Dialogue Generation with Feature-Guided Knowledge Augmentation</td>
<td>Jinpeng Li, Zekai Zhang, Xiuying Chen, Dongyan Zhao, Rui Yan</td>
</tr>
<tr>
<td>1992</td>
<td>Findings</td>
<td>Harnessing the power of LLMs: Evaluating human-AI text co-creation through the lens of news headline generation</td>
<td>Zijian Ding, Alison Smith-Renner, Wenjuan Zhang, Joel R. Tetreault, Alejandro Jaimes</td>
</tr>
<tr>
<td>1993</td>
<td>Findings</td>
<td>InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation</td>
<td>Renzhi Wang, Jing Li, Piji Li</td>
</tr>
<tr>
<td>2053</td>
<td>Findings</td>
<td>The Iron(ic) Melting Pot: Reviewing Human Evaluation in Humour, Irony and Sarcasm Generation</td>
<td>Tyler Loakman, Aaron Maladry, Chenghua Lin</td>
</tr>
<tr>
<td>2490</td>
<td>Findings</td>
<td>Ask To The Point: Open-Domain Entity-Centric Question Generation</td>
<td>Yuxiang Liu, Jie Huang, Kevin Chang</td>
</tr>
<tr>
<td>2493</td>
<td>Findings</td>
<td>Frugal Prompting for Dialog Models</td>
<td>Bishal Santra, Sakya Basak, Abhinandan De, Manish Gupta, Pawan Goyal</td>
</tr>
<tr>
<td>2716</td>
<td>Findings</td>
<td>Towards Informative Open-ended Text Generation with Dynamic Knowledge Triples</td>
<td>Zixuan Ren, Yang Zhao, Chengqing Zong</td>
</tr>
<tr>
<td>2876</td>
<td>Findings</td>
<td>Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements</td>
<td>Yushan Qian, Weinan Zhang, Ting Liu</td>
</tr>
<tr>
<td>3010</td>
<td>Findings</td>
<td>T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics</td>
<td>Yiwei Qin, Weizhe Yuan, Graham Neubig, Pengfei Liu</td>
</tr>
<tr>
<td>3019</td>
<td>Findings</td>
<td>NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark</td>
<td>Oscar Sainz, Jon Ander Campos, Iker Garc√≠a-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, Eneko Agirre</td>
</tr>
<tr>
<td>3386</td>
<td>Findings</td>
<td>Narrative Order Aware Story Generation via Bidirectional Pretraining Model with Optimal Transport Reward</td>
<td>Zhicong Lu, Li Jin, Guangluan Xu, Linmei Hu, Nayu Liu, Xiaoyu Li, Xian Sun, Zequn Zhang, kaiwen wei</td>
</tr>
<tr>
<td>3613</td>
<td>Findings</td>
<td>Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models</td>
<td>Luiza Amador Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker</td>
</tr>
<tr>
<td>3726</td>
<td>Findings</td>
<td>Don‚Äôt Add, don‚Äôt Miss: Effective Content Preserving Generation from Pre-Selected Text Spans</td>
<td>Aviv Slobodkin, Avi Caciularu, Eran Hirsch, Ido Dagan</td>
</tr>
<tr>
<td>3802</td>
<td>Findings</td>
<td>Ensemble-Instruct: Instruction Tuning Data Generation with a Heterogeneous Mixture of LMs</td>
<td>Young-Suk Lee, Md Arafat Sultan, Yousef El-Kurdi, Tahira Naseem, Asim Munawar, Radu Florian, Salim Roukos, Ram√≥n Fernandez Astudillo</td>
</tr>
<tr>
<td>4841</td>
<td>Findings</td>
<td>A Closer Look into Using Large Language Models for Automatic Evaluation</td>
<td>Cheng-Han Chiang, Hung-yi Lee</td>
</tr>
<tr>
<td>4954</td>
<td>Findings</td>
<td>Pseudointelligence: A Unifying Lens on Language Model Evaluation</td>
<td>Shikhar Murty, Orr Paradise, Pratyusha Sharma</td>
</tr>
<tr>
<td>5156</td>
<td>Findings</td>
<td>Improving Pacing in Long-Form Story Planning</td>
<td>Yichen Wang, Kevin Yang, Xiaoming Liu, Dan Klein</td>
</tr>
<tr>
<td>5563</td>
<td>Findings</td>
<td>Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models</td>
<td>Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, Lingpeng Kong</td>
</tr>
<tr>
<td>5603</td>
<td>Findings</td>
<td>Exploring Context-Aware Evaluation Metrics for Machine Translation</td>
<td>Xinyu Hu, Xunjian Yin, Xiaojun Wan</td>
</tr>
</tbody>
</table>
<h2 id="user-content-organization">Organization</h2>
<p><em>Contact</em>:
<a href="mailto:gem-benchmark-chairs@googlegroups.com">gem-benchmark-chairs@googlegroups.com</a></p>
<p><em>General Chairs</em></p>
<p>Khyathi Raghavi Chandu (AI2)</p>
<p>Elizabeth Clark (Google Deepmind)</p>
<p>Kaustubh Dhole (Emory University)</p>
<p>Sebastian Gehrmann (Bloomberg)</p>
<p>Jo√£o Sedoc (NYU)</p>
<p>Alex Wang (Cohere)</p>
<p><em>Industry Track Chairs</em></p>
<p>Enrico Santus (Bloomberg)</p>
<p>Hooman Sedghamiz (Bayer)</p>
</div></article></main><div class="layout_push__lpoMK"></div></div><footer class="layout_footer__WlhMu utils_eggshell__3hbbY"><span class="layout_backToHome__D9QFr"><a href="/">‚Üê Home</a></span><span>If you have any questions, please join our <a href="https://groups.google.com/g/gem-benchmark" target="_blank" class="utils_accentUnderline__VG89l">google group</a> for support.</span></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"workshopData":{"contentHtml":"\u003cp\u003eThe Third Version of the \u003ca href=\"https://gem-benchmark.com/\"\u003eGeneration, Evaluation \u0026#x26; Metrics (GEM) Workshop\u003c/a\u003e will be held as part of \u003ca href=\"https://2023.emnlp.org/\"\u003eEMNLP\u003c/a\u003e, üìÖ December 6, 2023.\u003c/p\u003e\n\u003ch2 id=\"user-content-overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eMany new NLP applications are cast through the lens of natural language generation. With the advent of these new approaches, many opportunities arise: generation in previously less studied languages, new evaluation paradigms, methods for corpus creation, more efficient architectures, strategies for safe deployments, among many others. At the same time, we can learn from the rich history of NLG research to further improve generation methods.\nThese developments require robust and sound NLG evaluation processes. To that end, the GEM workshop aims to encourage the development of model auditing and human evaluation strategies, and to popularize model evaluations in languages beyond English.\u003c/p\u003e\n\u003cp\u003eIf you are interested, you can check out last year's workshop websites from \u003ca href=\"/workshop/2021\"\u003eACL 2021\u003c/a\u003e and \u003ca href=\"/workshop/2022\"\u003eEMNLP 2022\u003c/a\u003e. Our call for this workshop can be found \u003ca href=\"/workshop/2023-call\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"user-content-schedule\"\u003eSchedule\u003c/h2\u003e\n\u003cp\u003eAll times in local Singapore Time, please use a converter like \u003ca href=\"https://www.timeanddate.com/worldclock/converter.html?iso=20231108T180000\u0026#x26;p1=236\"\u003ethis one\u003c/a\u003e to if you are in a different time zone.\nTo accomodate attendees from as many time zones as possible, we will have a virtual-only part in the evening.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eStart\u003c/th\u003e\n\u003cth\u003eEnd\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e9:00\u003c/td\u003e\n\u003ctd\u003e10:30\u003c/td\u003e\n\u003ctd\u003eOpening Remarks + 6 x 10+2 minutes talk\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e10:30\u003c/td\u003e\n\u003ctd\u003e11:00\u003c/td\u003e\n\u003ctd\u003eCoffee Break\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e11:00\u003c/td\u003e\n\u003ctd\u003e12:30\u003c/td\u003e\n\u003ctd\u003ePoster Session I\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e12:30\u003c/td\u003e\n\u003ctd\u003e14:00\u003c/td\u003e\n\u003ctd\u003eLunch Break\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e14:00\u003c/td\u003e\n\u003ctd\u003e15:30\u003c/td\u003e\n\u003ctd\u003e7 x 10+2 minutes talk\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e15:30\u003c/td\u003e\n\u003ctd\u003e16:00\u003c/td\u003e\n\u003ctd\u003eCoffee Break\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e16:00\u003c/td\u003e\n\u003ctd\u003e17:30\u003c/td\u003e\n\u003ctd\u003ePoster Session II\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"user-content-sessions-and-papers\"\u003eSessions and Papers\u003c/h2\u003e\n\u003ch3 id=\"user-content-talk-session-i-900-1030am\"\u003eTalk Session I (9:00-10:30am)\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eID\u003c/th\u003e\n\u003cth\u003eType\u003c/th\u003e\n\u003cth\u003eTitle\u003c/th\u003e\n\u003cth\u003eAuthors\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e271\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eVector-Quantized Prompt Learning for Paraphrase Generation\u003c/td\u003e\n\u003ctd\u003eHaotian Luo, Yixin Liu, Peidong Liu, Xianggen Liu\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e1154\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eCan Large Language Models Fix Data Annotation Errors? An Empirical Study Using Debatepedia for Query-Focused Text Summarization\u003c/td\u003e\n\u003ctd\u003eMd Tahmid Rahman Laskar, Mizanur Rahman, Israt Jahan, Enamul Hoque, Jimmy Huang\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e1562\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eGeographical Erasure in Language Generation\u003c/td\u003e\n\u003ctd\u003ePola Schw√∂bel, Jacek Golebiowski, Michele Donini, Cedric Archambeau, Danish Pruthi\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e1834\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eA Comprehensive Evaluation of Tool-Assisted Generation Strategies\u003c/td\u003e\n\u003ctd\u003eAlon Jacovi, Avi Caciularu, Jonathan Herzig, Roee Aharoni, Bernd Bohnet, Mor Geva\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e5166\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003e‚ÄúKelly is a Warm Person, Joseph is a Role Model‚Äù: Gender Biases in LLM-Generated Reference Letters\u003c/td\u003e\n\u003ctd\u003eYixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang, Nanyun Peng\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e11\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eFACTSCORE: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation\u003c/td\u003e\n\u003ctd\u003eSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer and Hannaneh Hajishirzi\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"user-content-poster-session-i-1100-1230pm\"\u003ePoster Session I (11:00-12:30pm)\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eID\u003c/th\u003e\n\u003cth\u003eType\u003c/th\u003e\n\u003cth\u003eTitle\u003c/th\u003e\n\u003cth\u003eAuthors\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e3\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eContextualizing the Limits of Model \u0026#x26; Evaluation Dataset Curation on Semantic Similarity Classification Tasks\u003c/td\u003e\n\u003ctd\u003eDaniel Theron\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eDialogue Quality and Emotion Annotations for Customer Support Conversations\u003c/td\u003e\n\u003ctd\u003eJohn Mendonca, Patr√≠cia Pereira, Miguel Menezes, Vera Cabarr√£o, Ana C Farinha, Helena Moniz, Alon Lavie and Isabel Trancoso\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e7\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eFormalizing content creation and evaluation methods for AI-generated social media content\u003c/td\u003e\n\u003ctd\u003eChristian Jensen and Axel H√∏jmark\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e9\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eAutomatic Evaluation of Generative Models with Instruction Tuning\u003c/td\u003e\n\u003ctd\u003eShuhaib Mehri and Vered Shwartz\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e12\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eEffective Proxy for Human Labeling: Ensemble Disagreement Scores in Large Language Models for Industrial NLP\u003c/td\u003e\n\u003ctd\u003eWei Du, Laksh Advani, Yashmeet Gambhir, Daniel Perry, Prashant Shiralkar, Zhengzheng Xing and Aaron Colak\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e14\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eAutomatic Reflection Generation for Peer-to-Peer Counseling\u003c/td\u003e\n\u003ctd\u003eEmma O'Neil, Jo√£o Sedoc, Diyi Yang, Haiyi Zhu and Lyle Ungar\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e16\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eOne-Shot and Few-Shot Exemplification Modeling\u003c/td\u003e\n\u003ctd\u003eJohn Harvill, Hee Suk Yoon, Eunseop Yoon, Mark Hasegawa-Johnson and Chang Yoo\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e21\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eQAMPARI: A Benchmark for Open-domain Questions with Many Answers\u003c/td\u003e\n\u003ctd\u003eSamuel Amouyal, Tomer Wolfson, Ohad Rubin, Ori Yoran, Jonathan Herzig and Jonathan Berant\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e23\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eUnveiling Safety Vulnerabilities of Large Language Models\u003c/td\u003e\n\u003ctd\u003eGeorge Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby Tavor, Orna Raz and Eitan Farchi\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e24\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eAdapting Pre-trained Generative Models for Extractive Question Answering\u003c/td\u003e\n\u003ctd\u003ePrabir Mallick, Tapas Nayak and Indrajit Bhattacharya\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e25\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003ePredicting Question-Answering Performance of Large Language Models through Semantic Consistency\u003c/td\u003e\n\u003ctd\u003eElla Rabinovich, Samuel Ackerman, Orna Raz, Eitan Farchi and Ateret Anaby Tavor\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e28\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eTowards Effective Long-Form QA with Evidence Augmentation\u003c/td\u003e\n\u003ctd\u003eMengxia Yu, Sara Rosenthal, Mihaela Bornea and Avi Sil\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e30\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eHarnessing the Plug-and-Play Controller by Prompting\u003c/td\u003e\n\u003ctd\u003eHao Wang and Lei Sha\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e32\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eContext and Literacy Aware Learnable Metric for Text Simplification\u003c/td\u003e\n\u003ctd\u003eJeongwon Kwak, Hyeryun Park, Kyungmo Kim and Jinwook Choi\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e33\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eSynthetic Dialogue Dataset Generation using LLM Agents\u003c/td\u003e\n\u003ctd\u003eYelaman Abdullin, Diego Molla, Bahadorreza Ofoghi, John Yearwood and Qingyang Li\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e34\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eAn Empirical Bayes Framework for Open-Domain Dialogue Generation\u003c/td\u003e\n\u003ctd\u003eJing Yang Lee, Kong Aik Lee and Woon Seng Gan\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e38\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eChatGPT as a Java Decompiler\u003c/td\u003e\n\u003ctd\u003eBradley McDanel and Zhanhao Liu\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e43\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eTargeted Image Data Augmentation Increases Basic Skills Captioning Robustness\u003c/td\u003e\n\u003ctd\u003eValentin Barriere, Felipe del Rio, Andres Carvallo, Carlos Aspillaga, Eugenio Herrera-Berg and Cristian Buc\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e45\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eSeparating form and meaning: Using self-consistency to quantify task understanding across multiple senses\u003c/td\u003e\n\u003ctd\u003eXenia Ohmer, Elia Bruni and Dieuwke Hupkes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e46\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eText Encoders Lack Knowledge: Leveraging Generative LLMs for Domain-Specific Semantic Textual Similarity\u003c/td\u003e\n\u003ctd\u003eJoseph Gatto, Omar Sharif, Parker Seegmiller, Philip Bohlman and Sarah Masud Preum\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e51\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eTo Burst or Not to Burst: Generating and Quantifying Improbable Text\u003c/td\u003e\n\u003ctd\u003eKuleen Sasse, Efsun Sarioglu Kayi, Samuel Barham and Edward Staley\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e52\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eAre Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs\u003c/td\u003e\n\u003ctd\u003eXue-Yong Fu, Md Tahmid Rahman Laskar, Cheng Chen and Shashi Bhushan TN\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e54\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eRankAug: Augmented data ranking for text classification\u003c/td\u003e\n\u003ctd\u003eTiasa Singha Roy and Priyam Basu\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e67\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003ePost Turing: Mapping the landscape of LLM Evaluation\u003c/td\u003e\n\u003ctd\u003eAlexey Tikhonov and Ivan Yamshchikov\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e62\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003ePersonalityChat: Conversation Distillation for Personalized Dialog Modeling with Facts and Traits\u003c/td\u003e\n\u003ctd\u003eEhsan Lotfi, Maxime De Bruyn, Jeska Buhmann and Walter Daelemans\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e63\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eHow well ChatGPT understand Malaysian English? An Evaluation on Named Entity Recognition and Relation Extraction\u003c/td\u003e\n\u003ctd\u003eMohanRaj Chanthran, Lay-Ki Soon, Ong Huey Fang and Bhawani Selvaretnam\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e57\u003c/td\u003e\n\u003ctd\u003eExtended Abstract\u003c/td\u003e\n\u003ctd\u003eRobust Tooling and New Resources for Large Language Model Evaluation via Catwalk\u003c/td\u003e\n\u003ctd\u003eKyle Richardson, Ian Magnusson, Oyvind Tafjord,Akshita Bhagia, Iz Beltagy, Arman Cohan, Pradeep Dasigi,Jesse Dodge, Dirk Groeneveld, Yuling Gu, Ananya Harsh Jha, Tushar Khot and Nishant Subramani\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e58\u003c/td\u003e\n\u003ctd\u003eExtended Abstract\u003c/td\u003e\n\u003ctd\u003eGUMSum: Multi-Genre Data and Evaluation for English Abstractive Summarization\u003c/td\u003e\n\u003ctd\u003eYang Janet Liu and Amir Zeldes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e60\u003c/td\u003e\n\u003ctd\u003eExtended Abstract\u003c/td\u003e\n\u003ctd\u003eNewsMet: A ‚ÄòDo It All' dataset of contemporary Metaphors in News headlines\u003c/td\u003e\n\u003ctd\u003eRohan Joseph, Timothy Liu, Aik Beng Ng, Simon See and Sunny Rai\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e20\u003c/td\u003e\n\u003ctd\u003eExtended Abstract\u003c/td\u003e\n\u003ctd\u003eOn the State of German (Abstractive) Text Summarization\u003c/td\u003e\n\u003ctd\u003eDennis Aumiller, Jing Fan and Michael Gertz\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e31\u003c/td\u003e\n\u003ctd\u003eExtended Abstract\u003c/td\u003e\n\u003ctd\u003eMeasuring misogyny in natural language generation: preliminary results from a case study on two Reddit communities\u003c/td\u003e\n\u003ctd\u003eAaron Snoswell, Lucinda Nelson, Hao Xue, Flora Salim, Nicolas Suzor and Jean Burgess\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e35\u003c/td\u003e\n\u003ctd\u003eExtended Abstract\u003c/td\u003e\n\u003ctd\u003eOn the Learnability of Watermarks for Language Models\u003c/td\u003e\n\u003ctd\u003eChenchen Gu, Xiang Lisa Li, Percy Liang and Tatsunori Hashimoto\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e47\u003c/td\u003e\n\u003ctd\u003eExtended Abstract\u003c/td\u003e\n\u003ctd\u003eDoes Writing with Language Models Reduce Content Diversity?\u003c/td\u003e\n\u003ctd\u003eVishakh Padmakumar and He He\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"user-content-talk-session-ii-1400-1530pm\"\u003eTalk Session II (14:00-15:30pm)\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eID\u003c/th\u003e\n\u003cth\u003eType\u003c/th\u003e\n\u003cth\u003eTitle\u003c/th\u003e\n\u003cth\u003eAuthors\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e36\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eFlesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models\u003c/td\u003e\n\u003ctd\u003eJoseph Marvin Imperial and Harish Tayyar Madabushi\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e41\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eMulti-domain Summarization from Leaderboards to Practice: Re-examining Automatic and Human Evaluation\u003c/td\u003e\n\u003ctd\u003eDavid Demeter, Oshin Agarwal, Simon Ben Igeri, Marko Sterbentz, Neil Molino, John Conroy and Ani Nenkova\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e56\u003c/td\u003e\n\u003ctd\u003eMain Track\u003c/td\u003e\n\u003ctd\u003eElo Uncovered: Robustness and Best Practices in Language Model Evaluation\u003c/td\u003e\n\u003ctd\u003eMeriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker and Marzieh Fadaee\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e39\u003c/td\u003e\n\u003ctd\u003eExtended Abstract\u003c/td\u003e\n\u003ctd\u003eGenerative language models exhibit social identity biases\u003c/td\u003e\n\u003ctd\u003eTiancheng Hu, Yara Kyrychenko, Jon Roozenbeek and Nigel Collier\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e70\u003c/td\u003e\n\u003ctd\u003eIndustry Track\u003c/td\u003e\n\u003ctd\u003eA Simple yet Efficient Ensemble Approach for AI-generated Text Detection\u003c/td\u003e\n\u003ctd\u003eHarika Abburi, Kalyani Roy, Michael Suesserman, Nirmala Pudota, Balaji Veeramani, Edward Bowen and Sanmitra Bhattacharya\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e17\u003c/td\u003e\n\u003ctd\u003eIndustry Track\u003c/td\u003e\n\u003ctd\u003eLeveraging Large Language Models for Enhanced Product Descriptions in eCommerce\u003c/td\u003e\n\u003ctd\u003eJianghong Zhou, Bo Liu, Jhalak Nilesh Acharya, Yao Hong, Kuang-chih Lee and Musen Wen\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e55\u003c/td\u003e\n\u003ctd\u003eIndustry Track\u003c/td\u003e\n\u003ctd\u003eSeparating the Wheat from the Chaff with BREAD: An open-source benchmark and metrics to detect redundancy in text\u003c/td\u003e\n\u003ctd\u003eIsaac Caswell, Lisa Wang and Isabel Papadimitriou\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"user-content-poster-session-ii-1600-1730pm\"\u003ePoster Session II (16:00-17:30pm)\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eID\u003c/th\u003e\n\u003cth\u003eType\u003c/th\u003e\n\u003cth\u003eTitle\u003c/th\u003e\n\u003cth\u003eAuthors\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e223\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eMacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space\u003c/td\u003e\n\u003ctd\u003eHanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, Xueqi Cheng, Tat-Seng Chua\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e300\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eDeltaScore: Story Evaluation with Perturbations\u003c/td\u003e\n\u003ctd\u003eZhuohan Xie, Miao Li, Trevor Cohn, Jey Han Lau\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e469\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eShow, Write, and Retrieve: Entity-aware Article Generation and Retrieval\u003c/td\u003e\n\u003ctd\u003eZhongping Zhang, Yiwen Gu, Bryan A. Plummer\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e575\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eAdversarial Text Generation by Search and Learning\u003c/td\u003e\n\u003ctd\u003eGuoyi Li, Bingkang Shi, Zongzhen Liu, Dehan Kong, Yulei Wu, Xiaodan Zhang, Longtao Huang, Honglei Lyu\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e651\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eOn Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study\u003c/td\u003e\n\u003ctd\u003ePolina Zablotskaia, Du Phan, Joshua Maynez, Shashi Narayan, Jie Ren, Jeremiah Zhe Liu\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e731\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eGROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence\u003c/td\u003e\n\u003ctd\u003eZhihua Wen, Zhiliang Tian, Wei Wu, Yuxin Yang, Yanqi Shi, Zhen Huang, Dongsheng Li\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e963\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eA Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing\u003c/td\u003e\n\u003ctd\u003eCarlos G√≥mez-Rodr√≠guez, Paul Williams\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e1470\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eUniform Complexity for Text Generation\u003c/td\u003e\n\u003ctd\u003eJoseph Marvin Imperial, Harish Tayyar Madabushi\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e1548\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eUnraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance\u003c/td\u003e\n\u003ctd\u003eThiemo Wambsganss, Xiaotian Su, Vinitra Swamy, Seyed Parsa Neshaei, Roman Rietsche, Tanja K√§ser\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e1807\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eMiracle: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control\u003c/td\u003e\n\u003ctd\u003eZhenyi Lu, Wei Wei, Xiaoye Qu, Xian-Ling Mao, Dangyang Chen, Jixiong Chen\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e1897\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eStylized Dialogue Generation with Feature-Guided Knowledge Augmentation\u003c/td\u003e\n\u003ctd\u003eJinpeng Li, Zekai Zhang, Xiuying Chen, Dongyan Zhao, Rui Yan\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e1992\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eHarnessing the power of LLMs: Evaluating human-AI text co-creation through the lens of news headline generation\u003c/td\u003e\n\u003ctd\u003eZijian Ding, Alison Smith-Renner, Wenjuan Zhang, Joel R. Tetreault, Alejandro Jaimes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e1993\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eInfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation\u003c/td\u003e\n\u003ctd\u003eRenzhi Wang, Jing Li, Piji Li\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2053\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eThe Iron(ic) Melting Pot: Reviewing Human Evaluation in Humour, Irony and Sarcasm Generation\u003c/td\u003e\n\u003ctd\u003eTyler Loakman, Aaron Maladry, Chenghua Lin\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2490\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eAsk To The Point: Open-Domain Entity-Centric Question Generation\u003c/td\u003e\n\u003ctd\u003eYuxiang Liu, Jie Huang, Kevin Chang\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2493\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eFrugal Prompting for Dialog Models\u003c/td\u003e\n\u003ctd\u003eBishal Santra, Sakya Basak, Abhinandan De, Manish Gupta, Pawan Goyal\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2716\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eTowards Informative Open-ended Text Generation with Dynamic Knowledge Triples\u003c/td\u003e\n\u003ctd\u003eZixuan Ren, Yang Zhao, Chengqing Zong\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2876\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eHarnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements\u003c/td\u003e\n\u003ctd\u003eYushan Qian, Weinan Zhang, Ting Liu\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e3010\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eT5Score: Discriminative Fine-tuning of Generative Evaluation Metrics\u003c/td\u003e\n\u003ctd\u003eYiwei Qin, Weizhe Yuan, Graham Neubig, Pengfei Liu\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e3019\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eNLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark\u003c/td\u003e\n\u003ctd\u003eOscar Sainz, Jon Ander Campos, Iker Garc√≠a-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, Eneko Agirre\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e3386\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eNarrative Order Aware Story Generation via Bidirectional Pretraining Model with Optimal Transport Reward\u003c/td\u003e\n\u003ctd\u003eZhicong Lu, Li Jin, Guangluan Xu, Linmei Hu, Nayu Liu, Xiaoyu Li, Xian Sun, Zequn Zhang, kaiwen wei\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e3613\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eGoodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models\u003c/td\u003e\n\u003ctd\u003eLuiza Amador Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e3726\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eDon‚Äôt Add, don‚Äôt Miss: Effective Content Preserving Generation from Pre-Selected Text Spans\u003c/td\u003e\n\u003ctd\u003eAviv Slobodkin, Avi Caciularu, Eran Hirsch, Ido Dagan\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e3802\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eEnsemble-Instruct: Instruction Tuning Data Generation with a Heterogeneous Mixture of LMs\u003c/td\u003e\n\u003ctd\u003eYoung-Suk Lee, Md Arafat Sultan, Yousef El-Kurdi, Tahira Naseem, Asim Munawar, Radu Florian, Salim Roukos, Ram√≥n Fernandez Astudillo\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e4841\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eA Closer Look into Using Large Language Models for Automatic Evaluation\u003c/td\u003e\n\u003ctd\u003eCheng-Han Chiang, Hung-yi Lee\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e4954\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003ePseudointelligence: A Unifying Lens on Language Model Evaluation\u003c/td\u003e\n\u003ctd\u003eShikhar Murty, Orr Paradise, Pratyusha Sharma\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e5156\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eImproving Pacing in Long-Form Story Planning\u003c/td\u003e\n\u003ctd\u003eYichen Wang, Kevin Yang, Xiaoming Liu, Dan Klein\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e5563\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eBridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models\u003c/td\u003e\n\u003ctd\u003eShansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, Lingpeng Kong\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e5603\u003c/td\u003e\n\u003ctd\u003eFindings\u003c/td\u003e\n\u003ctd\u003eExploring Context-Aware Evaluation Metrics for Machine Translation\u003c/td\u003e\n\u003ctd\u003eXinyu Hu, Xunjian Yin, Xiaojun Wan\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"user-content-organization\"\u003eOrganization\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eContact\u003c/em\u003e:\n\u003ca href=\"mailto:gem-benchmark-chairs@googlegroups.com\"\u003egem-benchmark-chairs@googlegroups.com\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eGeneral Chairs\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eKhyathi Raghavi Chandu (AI2)\u003c/p\u003e\n\u003cp\u003eElizabeth Clark (Google Deepmind)\u003c/p\u003e\n\u003cp\u003eKaustubh Dhole (Emory University)\u003c/p\u003e\n\u003cp\u003eSebastian Gehrmann (Bloomberg)\u003c/p\u003e\n\u003cp\u003eJo√£o Sedoc (NYU)\u003c/p\u003e\n\u003cp\u003eAlex Wang (Cohere)\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eIndustry Track Chairs\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eEnrico Santus (Bloomberg)\u003c/p\u003e\n\u003cp\u003eHooman Sedghamiz (Bayer)\u003c/p\u003e\n"}},"__N_SSG":true},"page":"/workshop","query":{},"buildId":"V8ziX7lLuetLbU22tSnhs","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>