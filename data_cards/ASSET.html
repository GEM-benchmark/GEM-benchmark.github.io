<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="Benchmark natural language generation systems with GEM."/><meta property="og:image" content="https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"/><meta name="og:title" content="GEM"/><meta name="twitter:card" content="summary_large_image"/><title>GEM ASSET</title><link rel="preload" href="/_next/static/css/2786522978a02f025205.css" as="style"/><link rel="stylesheet" href="/_next/static/css/2786522978a02f025205.css" data-n-g=""/><link rel="preload" href="/_next/static/css/f2fce7b83fe6ca04479b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f2fce7b83fe6ca04479b.css" data-n-p=""/><noscript data-n-css="true"></noscript><link rel="preload" href="/_next/static/chunks/main-47bc8f80085b54a800da.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.baa41d4dbf5d52db897c.js" as="script"/><link rel="preload" href="/_next/static/chunks/76fa31df43514ea5aaf6b0525de03ac0f65eec9e.4a36a385313236c59b19.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-a9ae7a6d1de4e51a7ab6.js" as="script"/><link rel="preload" href="/_next/static/chunks/cb1608f2.c3a9f0eb95374ca4919a.js" as="script"/><link rel="preload" href="/_next/static/chunks/3a03639cec9cbb3837db9d33dd6cd155d0d21f09.13fa0eac69555f2fe80c.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/data_cards/%5Bid%5D-e1361c13d80af81deda6.js" as="script"/></head><body><div id="__next"><div class="layout_background__1AVEa undefined"><header class="layout_header__2rhWq"><div class="navbar_navwrapper__15zia"><div class="navbar_gradbar__1Xi5u"></div><nav class="navbar_navbar__3gnco"><span class="utils_headingLg__de7p0 navbar_navbarlogo__PLEwr"><a href="/">GEM BENCHMARK</a></span><div class="navbar_menutoggle__358pJ" id="mobile-menu"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars fa-w-14 navbar_bar__QVPSR" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"></path></svg></div><ul><li class="navbar_navitem__3ICSG navbar_pushright__3G2DM"><a href="/resources">Resources</a></li><li class="navbar_navitem__3ICSG"><a href="/data_cards">Data Cards</a></li><li class="navbar_navitem__3ICSG"><a href="/model_cards">Model Cards</a></li><li class="navbar_navitem__3ICSG"><a href="/tutorials">tutorials</a></li><li class="navbar_navitem__3ICSG"><a href="/results">Results</a></li><li class="navbar_navitem__3ICSG"><a href="/papers">Papers</a></li><li class="navbar_navitem__3ICSG"><a href="/team">Team</a></li><li class="navbar_navitem__3ICSG"><a href="/nl_augmenter">NL-Augmenter</a></li><li class="navbar_navitem__3ICSG"><a href="/hackathon">Hackathon</a></li></ul></nav></div></header><div class="layout_container__2t4v2"><main><article><span class="utils_headingXl__1XecN">ASSET</span><span class="utils_smallSpace__375iy"></span><span class="utils_lightText__12Ckm">Simplification</span><div><h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#dataset-description">Dataset Description</a>
<ul>
<li><a href="#dataset-and-task-summary">Dataset and Task Summary</a></li>
<li><a href="#why-is-this-dataset-part-of-gem">Why is this dataset part of GEM?</a></li>
<li><a href="#languages">Languages</a></li>
</ul>
</li>
<li><a href="#meta-information">Meta Information</a>
<ul>
<li><a href="#dataset-curators">Dataset Curators</a></li>
<li><a href="#licensing-information">Licensing Information</a></li>
<li><a href="#citation-information">Citation Information</a></li>
<li><a href="#leaderboard">Leaderboard</a></li>
</ul>
</li>
<li><a href="#dataset-structure">Dataset Structure</a>
<ul>
<li><a href="#data-instances">Data Instances</a></li>
<li><a href="#data-fields">Data Fields</a></li>
<li><a href="#data-statistics">Data Statistics</a></li>
</ul>
</li>
<li><a href="#dataset-creation">Dataset Creation</a>
<ul>
<li><a href="#curation-rationale">Curation Rationale</a></li>
<li><a href="#communicative-goal">Communicative Goal</a></li>
<li><a href="#source-data">Source Data</a>
<ul>
<li><a href="#initial-data-collection-and-normalization">Initial Data Collection and Normalization</a></li>
<li><a href="#who-are-the-source-language-producers">Who are the source language producers?</a></li>
</ul>
</li>
<li><a href="#annotations">Annotations</a>
<ul>
<li><a href="#annotation-process">Annotation process</a></li>
<li><a href="#who-are-the-annotators">Who are the annotators?</a></li>
</ul>
</li>
<li><a href="#personal-and-sensitive-information">Personal and Sensitive Information</a></li>
</ul>
</li>
<li><a href="#changes-to-the-original-dataset-for-gem">Changes to the Original Dataset for GEM</a>
<ul>
<li><a href="#special-test-sets">Special test sets</a>
<ul>
<li><a href="#subpopulations">Subpopulations</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#considerations-for-using-the-data">Considerations for Using the Data</a>
<ul>
<li><a href="#social-impact-of-the-dataset">Social Impact of the Dataset</a></li>
<li><a href="#impact-on-underserved-communities">Impact on Underserved Communities</a></li>
<li><a href="#discussion-of-biases">Discussion of Biases</a></li>
<li><a href="#other-known-limitations">Other Known Limitations</a></li>
</ul>
</li>
<li><a href="#getting-started-with-in-depth-research-on-the-task">Getting started with in-depth research on the task</a></li>
</ul>
<h2 id="dataset-description">Dataset Description</h2>
<ul>
<li><strong>Homepage:</strong> None (See <strong>Repository</strong>)</li>
<li><strong>Repository:</strong> <a href="https://github.com/facebookresearch/asset">ASSET repository</a></li>
<li><strong>Paper:</strong> <a href="https://www.aclweb.org/anthology/2020.acl-main.424.pdf">ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations</a></li>
<li><strong>Point of Contact:</strong> <a href="mailto:f.alva@sheffield.ac.uk">Fernando Alva-Manchego</a>, <a href="mailto:louismartincs@gmail.com">Louis Martin</a></li>
</ul>
<h3 id="dataset-and-task-summary">Dataset and Task Summary</h3>
<p><a href="https://github.com/facebookresearch/asset">ASSET</a> <a href="https://www.aclweb.org/anthology/2020.acl-main.424.pdf">(Alva-Manchego et al., 2020)</a> is multi-reference dataset for the evaluation of sentence simplification in English. The dataset uses the same 2,359 sentences from <a href="https://github.com/cocoxu/simplification/">TurkCorpus</a> <a href="https://www.aclweb.org/anthology/Q16-1029.pdf">(Xu et al., 2016)</a> and each sentence is associated with 10 crowdsourced simplifications. Unlike previous simplification datasets, which contain a single transformation (e.g., lexical paraphrasing in TurkCorpus or sentence
splitting in <a href="https://www.aclweb.org/anthology/D18-1081.pdf">HSplit</a>), the simplifications in ASSET encompass a variety of rewriting transformations.</p>
<h3 id="why-is-this-dataset-part-of-gem">Why is this dataset part of GEM?</h3>
<p>ASSET is a high quality simplification dataset where each source (not simple) sentence is associated with 10 human-written simplifications. It is one of the two datasets for the text simplification task in GEM. It acts as the validation and test set.</p>
<h3 id="languages">Languages</h3>
<p>ASSET contains English text only (BCP-47: <code>en</code>).</p>
<h2 id="meta-information">Meta Information</h2>
<h3 id="dataset-curators">Dataset Curators</h3>
<p>ASSET was developed by researchers at the University of Sheffield, Inria,
Facebook AI Research, and Imperial College London. The work was partly supported by Benoît Sagot's chair in the PRAIRIE institute, funded by the French National Research Agency (ANR) as part of the "Investissements d’avenir" program (reference ANR-19-P3IA-0001).</p>
<h3 id="licensing-information">Licensing Information</h3>
<p><a href="https://creativecommons.org/licenses/by-nc/4.0/">Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)</a></p>
<h3 id="citation-information">Citation Information</h3>
<pre><code>@inproceedings{alva-manchego-etal-2020-asset,
    title = "{ASSET}: {A} Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations",
    author = "Alva-Manchego, Fernando  and
      Martin, Louis  and
      Bordes, Antoine  and
      Scarton, Carolina  and
      Sagot, Beno{\^\i}t  and
      Specia, Lucia",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.424",
    pages = "4668--4679",
}
</code></pre>
<h3 id="leaderboard">Leaderboard</h3>
<p>There is no official leaderboard associated with ASSET.</p>
<h2 id="dataset-structure">Dataset Structure</h2>
<h3 id="data-instances">Data Instances</h3>
<ul>
<li><code>simplification</code> configuration: an instance consists in an original sentence and 10 possible reference simplifications.</li>
<li><code>ratings</code> configuration: a data instance consists in an original sentence, a simplification obtained by an automated system, and a judgment of quality along one of three axes by a crowd worker.</li>
</ul>
<h3 id="data-fields">Data Fields</h3>
<ul>
<li><code>original</code>: an original sentence from the source datasets</li>
<li><code>simplifications</code>: in the <code>simplification</code> config, a set of reference simplifications produced by crowd workers.</li>
<li><code>simplification</code>: in the <code>ratings</code> config, a simplification of the original obtained by an automated system</li>
<li><code>aspect</code>: in the <code>ratings</code> config, the aspect on which the simplification is evaluated, one of <code>meaning</code>, <code>fluency</code>, <code>simplicity</code></li>
<li><code>rating</code>: a quality rating between 0 and 100</li>
</ul>
<h3 id="data-statistics">Data Statistics</h3>
<p>ASSET does not contain a training set; many models use <a href="https://github.com/XingxingZhang/dress">WikiLarge</a> (Zhang and Lapata, 2017) for training. For GEM, <a href="https://github.com/chaojiang06/wiki-auto">Wiki-Auto</a> will be used for training the model.</p>
<p>Each input sentence has 10 associated reference simplified sentences. The statistics of ASSET are given below.</p>
<table>
<thead>
<tr>
<th></th>
<th>Dev</th>
<th>Test</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input Sentences</td>
<td>2000</td>
<td>359</td>
<td>2359</td>
</tr>
<tr>
<td>Reference Simplifications</td>
<td>20000</td>
<td>3590</td>
<td>23590</td>
</tr>
</tbody>
</table>
<p>The test and validation sets are the same as those of <a href="https://github.com/cocoxu/simplification/">TurkCorpus</a>. The split was random.</p>
<p>There are 19.04 tokens per reference on average (lower than 21.29 and 25.49 for TurkCorpus and HSplit, respectively). Most (17,245) of the referece sentences do not involve sentence splitting.</p>
<h2 id="dataset-creation">Dataset Creation</h2>
<h3 id="curation-rationale">Curation Rationale</h3>
<p>ASSET was created in order to improve the evaluation of sentence simplification. It uses the same input sentences as the <a href="https://github.com/cocoxu/simplification/">TurkCorpus</a> dataset from <a href="https://www.aclweb.org/anthology/Q16-1029.pdf">(Xu et al., 2016)</a>. The 2,359 input sentences of TurkCorpus are a sample of "standard" (not simple) sentences from the <a href="https://www.informatik.tu-darmstadt.de/ukp/research_6/data/sentence_simplification/simple_complex_sentence_pairs/index.en.jsp">Parallel Wikipedia Simplification (PWKP)</a> dataset <a href="https://www.aclweb.org/anthology/C10-1152.pdf">(Zhu et al., 2010)</a>, which come from the August 22, 2009 version of Wikipedia. The sentences of TurkCorpus were chosen to be of similar length <a href="https://www.aclweb.org/anthology/Q16-1029.pdf">(Xu et al., 2016)</a>. No further information is provided on the sampling strategy.</p>
<p>The TurkCorpus dataset was developed in order to overcome some of the problems with sentence pairs from Standard and Simple Wikipedia: a large fraction of sentences were misaligned, or not actually simpler <a href="https://www.aclweb.org/anthology/Q16-1029.pdf">(Xu et al., 2016)</a>. However, TurkCorpus mainly focused on <em>lexical paraphrasing</em>, and so cannot be used to evaluate simplifications involving <em>compression</em> (deletion) or <em>sentence splitting</em>. HSplit <a href="https://www.aclweb.org/anthology/D18-1081.pdf">(Sulem et al., 2018)</a>, on the other hand, can only be used to evaluate sentence splitting. The reference sentences in ASSET include a wider variety of sentence rewriting strategies, combining splitting, compression and paraphrasing. Annotators were given examples of each kind of transformation individually, as well as all three transformations used at once, but were allowed to decide which transformations to use for any given sentence.</p>
<p>An example illustrating the differences between TurkCorpus, HSplit and ASSET is given below:</p>
<blockquote>
<p><strong>Original:</strong> He settled in London, devoting himself chiefly to practical teaching.</p>
<p><strong>TurkCorpus:</strong> He rooted in London, devoting himself mainly to practical teaching.</p>
<p><strong>HSplit:</strong> He settled in London. He devoted himself chiefly to practical teaching.</p>
<p><strong>ASSET:</strong> He lived in London. He was a teacher.</p>
</blockquote>
<h3 id="communicative-goal">Communicative Goal</h3>
<p>The goal is to communicate the main ideas of source sentence in a way that is easier to understand by non-native speakers of English. This could be done by replacing complex words with simpler synonyms (i.e. paraphrasing), deleting unimportant information (i.e. compression), and/or splitting a long complex sentence into several simpler ones.</p>
<h3 id="source-data">Source Data</h3>
<h4 id="initial-data-collection-and-normalization">Initial Data Collection and Normalization</h4>
<p>Not applicable since ASSET uses the same 2,359 sentences from <a href="https://github.com/cocoxu/simplification/">TurkCorpus</a> <a href="https://www.aclweb.org/anthology/Q16-1029.pdf">(Xu et al., 2016)</a></p>
<h4 id="who-are-the-source-language-producers">Who are the source language producers?</h4>
<p>The dataset uses language from English Wikipedia (August 22, 2009 version): some demographic information is provided <a href="https://en.wikipedia.org/wiki/Wikipedia:Who_writes_Wikipedia%3F">here</a>.</p>
<h3 id="annotations">Annotations</h3>
<h4 id="annotation-process">Annotation process</h4>
<p>The instructions given to the annotators are available <a href="https://github.com/facebookresearch/asset/blob/master/crowdsourcing/AMT_AnnotationInstructions.pdf">here</a>.</p>
<h4 id="who-are-the-annotators">Who are the annotators?</h4>
<p>Reference sentences were written by 42 workers on Amazon Mechanical Turk (AMT). The requirements for being an annotator were:</p>
<ul>
<li>Passing a Qualification Test (appropriately simplifying sentences). Out of 100 workers, 42 passed the test.</li>
<li>Being a resident of the United States, United Kingdom or Canada.</li>
<li>Having a HIT approval rate over 95%, and over 1000 HITs approved.</li>
</ul>
<p>No other demographic or compensation information is provided in the ASSET paper.</p>
<h3 id="personal-and-sensitive-information">Personal and Sensitive Information</h3>
<p>Since the dataset is created from English Wikipedia (August 22, 2009 version), all the information contained in the dataset is already in the public domain.</p>
<h2 id="changes-to-the-original-dataset-for-gem">Changes to the Original Dataset for GEM</h2>
<p>No change is made to the original dataset.</p>
<h3 id="special-test-sets">Special test sets</h3>
<h4 id="subpopulations">Subpopulations</h4>
<p>The goal was to assess performance when simplifying source sentences with different syntactic structure and complexity. To this end, we split the original test set according to syntactic complexity of the source sentences. To characterize sentence syntactic complexity, we use the 8-level developmental level (d-level) scale proposed by <a href="https://www.researchgate.net/publication/254033869_How_complex_is_that_sentence_A_proposed_revision_of_the_Rosenberg_and_Abbeduto_D-Level_Scale">Covington et al. (2006)</a> and the implementation of <a href="https://www.jbe-platform.com/content/journals/10.1075/ijcl.15.4.02lu">Lu, Xiaofei (2010)</a>.
We thus split the original test set into 8 subsets corresponding to the 8 d-levels assigned to source sentences. We obtain the following number of instances per level and average d-level of the dataset:</p>
<table>
<thead>
<tr>
<th>Total nb. sentences</th>
<th>L0</th>
<th>L1</th>
<th>L2</th>
<th>L3</th>
<th>L4</th>
<th>L5</th>
<th>L6</th>
<th>L7</th>
<th>Mean Level</th>
</tr>
</thead>
<tbody>
<tr>
<td>359</td>
<td>166</td>
<td>0</td>
<td>58</td>
<td>32</td>
<td>5</td>
<td>28</td>
<td>7</td>
<td>63</td>
<td>2.38</td>
</tr>
</tbody>
</table>
<h2 id="considerations-for-using-the-data">Considerations for Using the Data</h2>
<h3 id="social-impact-of-the-dataset">Social Impact of the Dataset</h3>
<p>The dataset helps move forward the research towards text simplification by creating a higher quality validation and test dataset. Progress in text simplification in turn has the potential to increase the accessibility of written documents to wider audiences.</p>
<h3 id="impact-on-underserved-communities">Impact on Underserved Communities</h3>
<p>The dataset is in English, a language with an abundance of existing resources.</p>
<h3 id="discussion-of-biases">Discussion of Biases</h3>
<p>The dataset may contain some social biases, as the input sentences are based on Wikipedia. Studies have shown that the English Wikipedia contains both gender biases <a href="https://research.tudelft.nl/en/publications/is-wikipedia-succeeding-in-reducing-gender-bias-assessing-changes">(Schmahl et al., 2020)</a> and racial biases <a href="https://journals.sagepub.com/doi/pdf/10.1177/2378023118823946">(Adams et al., 2019)</a>.</p>
<h3 id="other-known-limitations">Other Known Limitations</h3>
<p>Since the dataset contains only 2,359 sentences that are derived from Wikipedia, it is limited to a small subset of topics present on Wikipedia.</p>
<h2 id="getting-started-with-in-depth-research-on-the-task">Getting started with in-depth research on the task</h2>
<p>The dataset can be downloaded from the original repository <a href="https://github.com/facebookresearch/asset">(here)</a> by the authors or can also be used via <a href="https://huggingface.co/datasets/asset">HuggingFace</a> and <a href="https://www.tensorflow.org/datasets/overview">TFDS</a>.</p>
<p>There are recent supervised (<a href="https://arxiv.org/abs/1910.02677">Martin et al., 2019</a>, <a href="https://www.aclweb.org/anthology/N19-1317/">Kriz et al., 2019</a>, <a href="https://www.aclweb.org/anthology/P19-1331/">Dong et al., 2019</a>, <a href="https://www.aclweb.org/anthology/D17-1062/">Zhang and Lapata, 2017</a>) and unsupervised (<a href="https://arxiv.org/abs/2005.00352v1">Martin et al., 2020</a>, <a href="https://www.aclweb.org/anthology/2020.acl-main.707/">Kumar et al., 2020</a>, <a href="https://www.aclweb.org/anthology/P19-1198/">Surya et al., 2019</a>) text simplification models that can be used as baselines.</p>
<p>The common metric used for automatic evaluation is SARI <a href="https://www.aclweb.org/anthology/Q16-1029/">(Xu et al., 2016)</a>.</p>
</div></article></main><div class="layout_push__1J9g0"></div></div><footer class="layout_footer__127N0 utils_eggshell__Njxsh"><span class="layout_backToHome__1vZsp"><a href="/">← Home</a></span><span>If you have any questions, please join our <a href="https://groups.google.com/g/gem-benchmark" target="_blank" class="utils_accentUnderline__k083p">google group</a> for support.</span></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"taskData":{"id":"ASSET","contentHtml":"\u003ch2 id=\"table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#dataset-description\"\u003eDataset Description\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#dataset-and-task-summary\"\u003eDataset and Task Summary\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#why-is-this-dataset-part-of-gem\"\u003eWhy is this dataset part of GEM?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#languages\"\u003eLanguages\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#meta-information\"\u003eMeta Information\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#dataset-curators\"\u003eDataset Curators\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#licensing-information\"\u003eLicensing Information\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#citation-information\"\u003eCitation Information\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#leaderboard\"\u003eLeaderboard\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dataset-structure\"\u003eDataset Structure\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#data-instances\"\u003eData Instances\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#data-fields\"\u003eData Fields\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#data-statistics\"\u003eData Statistics\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dataset-creation\"\u003eDataset Creation\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#curation-rationale\"\u003eCuration Rationale\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#communicative-goal\"\u003eCommunicative Goal\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#source-data\"\u003eSource Data\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#initial-data-collection-and-normalization\"\u003eInitial Data Collection and Normalization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#who-are-the-source-language-producers\"\u003eWho are the source language producers?\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#annotations\"\u003eAnnotations\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#annotation-process\"\u003eAnnotation process\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#who-are-the-annotators\"\u003eWho are the annotators?\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#personal-and-sensitive-information\"\u003ePersonal and Sensitive Information\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#changes-to-the-original-dataset-for-gem\"\u003eChanges to the Original Dataset for GEM\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#special-test-sets\"\u003eSpecial test sets\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#subpopulations\"\u003eSubpopulations\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#considerations-for-using-the-data\"\u003eConsiderations for Using the Data\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#social-impact-of-the-dataset\"\u003eSocial Impact of the Dataset\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#impact-on-underserved-communities\"\u003eImpact on Underserved Communities\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#discussion-of-biases\"\u003eDiscussion of Biases\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#other-known-limitations\"\u003eOther Known Limitations\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#getting-started-with-in-depth-research-on-the-task\"\u003eGetting started with in-depth research on the task\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"dataset-description\"\u003eDataset Description\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHomepage:\u003c/strong\u003e None (See \u003cstrong\u003eRepository\u003c/strong\u003e)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRepository:\u003c/strong\u003e \u003ca href=\"https://github.com/facebookresearch/asset\"\u003eASSET repository\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePaper:\u003c/strong\u003e \u003ca href=\"https://www.aclweb.org/anthology/2020.acl-main.424.pdf\"\u003eASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoint of Contact:\u003c/strong\u003e \u003ca href=\"mailto:f.alva@sheffield.ac.uk\"\u003eFernando Alva-Manchego\u003c/a\u003e, \u003ca href=\"mailto:louismartincs@gmail.com\"\u003eLouis Martin\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"dataset-and-task-summary\"\u003eDataset and Task Summary\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/facebookresearch/asset\"\u003eASSET\u003c/a\u003e \u003ca href=\"https://www.aclweb.org/anthology/2020.acl-main.424.pdf\"\u003e(Alva-Manchego et al., 2020)\u003c/a\u003e is multi-reference dataset for the evaluation of sentence simplification in English. The dataset uses the same 2,359 sentences from \u003ca href=\"https://github.com/cocoxu/simplification/\"\u003eTurkCorpus\u003c/a\u003e \u003ca href=\"https://www.aclweb.org/anthology/Q16-1029.pdf\"\u003e(Xu et al., 2016)\u003c/a\u003e and each sentence is associated with 10 crowdsourced simplifications. Unlike previous simplification datasets, which contain a single transformation (e.g., lexical paraphrasing in TurkCorpus or sentence\nsplitting in \u003ca href=\"https://www.aclweb.org/anthology/D18-1081.pdf\"\u003eHSplit\u003c/a\u003e), the simplifications in ASSET encompass a variety of rewriting transformations.\u003c/p\u003e\n\u003ch3 id=\"why-is-this-dataset-part-of-gem\"\u003eWhy is this dataset part of GEM?\u003c/h3\u003e\n\u003cp\u003eASSET is a high quality simplification dataset where each source (not simple) sentence is associated with 10 human-written simplifications. It is one of the two datasets for the text simplification task in GEM. It acts as the validation and test set.\u003c/p\u003e\n\u003ch3 id=\"languages\"\u003eLanguages\u003c/h3\u003e\n\u003cp\u003eASSET contains English text only (BCP-47: \u003ccode\u003een\u003c/code\u003e).\u003c/p\u003e\n\u003ch2 id=\"meta-information\"\u003eMeta Information\u003c/h2\u003e\n\u003ch3 id=\"dataset-curators\"\u003eDataset Curators\u003c/h3\u003e\n\u003cp\u003eASSET was developed by researchers at the University of Sheffield, Inria,\nFacebook AI Research, and Imperial College London. The work was partly supported by Benoît Sagot's chair in the PRAIRIE institute, funded by the French National Research Agency (ANR) as part of the \"Investissements d’avenir\" program (reference ANR-19-P3IA-0001).\u003c/p\u003e\n\u003ch3 id=\"licensing-information\"\u003eLicensing Information\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://creativecommons.org/licenses/by-nc/4.0/\"\u003eAttribution-NonCommercial 4.0 International (CC BY-NC 4.0)\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"citation-information\"\u003eCitation Information\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{alva-manchego-etal-2020-asset,\n    title = \"{ASSET}: {A} Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations\",\n    author = \"Alva-Manchego, Fernando  and\n      Martin, Louis  and\n      Bordes, Antoine  and\n      Scarton, Carolina  and\n      Sagot, Beno{\\^\\i}t  and\n      Specia, Lucia\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-main.424\",\n    pages = \"4668--4679\",\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"leaderboard\"\u003eLeaderboard\u003c/h3\u003e\n\u003cp\u003eThere is no official leaderboard associated with ASSET.\u003c/p\u003e\n\u003ch2 id=\"dataset-structure\"\u003eDataset Structure\u003c/h2\u003e\n\u003ch3 id=\"data-instances\"\u003eData Instances\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esimplification\u003c/code\u003e configuration: an instance consists in an original sentence and 10 possible reference simplifications.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eratings\u003c/code\u003e configuration: a data instance consists in an original sentence, a simplification obtained by an automated system, and a judgment of quality along one of three axes by a crowd worker.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"data-fields\"\u003eData Fields\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eoriginal\u003c/code\u003e: an original sentence from the source datasets\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esimplifications\u003c/code\u003e: in the \u003ccode\u003esimplification\u003c/code\u003e config, a set of reference simplifications produced by crowd workers.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esimplification\u003c/code\u003e: in the \u003ccode\u003eratings\u003c/code\u003e config, a simplification of the original obtained by an automated system\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003easpect\u003c/code\u003e: in the \u003ccode\u003eratings\u003c/code\u003e config, the aspect on which the simplification is evaluated, one of \u003ccode\u003emeaning\u003c/code\u003e, \u003ccode\u003efluency\u003c/code\u003e, \u003ccode\u003esimplicity\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003erating\u003c/code\u003e: a quality rating between 0 and 100\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"data-statistics\"\u003eData Statistics\u003c/h3\u003e\n\u003cp\u003eASSET does not contain a training set; many models use \u003ca href=\"https://github.com/XingxingZhang/dress\"\u003eWikiLarge\u003c/a\u003e (Zhang and Lapata, 2017) for training. For GEM, \u003ca href=\"https://github.com/chaojiang06/wiki-auto\"\u003eWiki-Auto\u003c/a\u003e will be used for training the model.\u003c/p\u003e\n\u003cp\u003eEach input sentence has 10 associated reference simplified sentences. The statistics of ASSET are given below.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eDev\u003c/th\u003e\n\u003cth\u003eTest\u003c/th\u003e\n\u003cth\u003eTotal\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eInput Sentences\u003c/td\u003e\n\u003ctd\u003e2000\u003c/td\u003e\n\u003ctd\u003e359\u003c/td\u003e\n\u003ctd\u003e2359\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eReference Simplifications\u003c/td\u003e\n\u003ctd\u003e20000\u003c/td\u003e\n\u003ctd\u003e3590\u003c/td\u003e\n\u003ctd\u003e23590\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eThe test and validation sets are the same as those of \u003ca href=\"https://github.com/cocoxu/simplification/\"\u003eTurkCorpus\u003c/a\u003e. The split was random.\u003c/p\u003e\n\u003cp\u003eThere are 19.04 tokens per reference on average (lower than 21.29 and 25.49 for TurkCorpus and HSplit, respectively). Most (17,245) of the referece sentences do not involve sentence splitting.\u003c/p\u003e\n\u003ch2 id=\"dataset-creation\"\u003eDataset Creation\u003c/h2\u003e\n\u003ch3 id=\"curation-rationale\"\u003eCuration Rationale\u003c/h3\u003e\n\u003cp\u003eASSET was created in order to improve the evaluation of sentence simplification. It uses the same input sentences as the \u003ca href=\"https://github.com/cocoxu/simplification/\"\u003eTurkCorpus\u003c/a\u003e dataset from \u003ca href=\"https://www.aclweb.org/anthology/Q16-1029.pdf\"\u003e(Xu et al., 2016)\u003c/a\u003e. The 2,359 input sentences of TurkCorpus are a sample of \"standard\" (not simple) sentences from the \u003ca href=\"https://www.informatik.tu-darmstadt.de/ukp/research_6/data/sentence_simplification/simple_complex_sentence_pairs/index.en.jsp\"\u003eParallel Wikipedia Simplification (PWKP)\u003c/a\u003e dataset \u003ca href=\"https://www.aclweb.org/anthology/C10-1152.pdf\"\u003e(Zhu et al., 2010)\u003c/a\u003e, which come from the August 22, 2009 version of Wikipedia. The sentences of TurkCorpus were chosen to be of similar length \u003ca href=\"https://www.aclweb.org/anthology/Q16-1029.pdf\"\u003e(Xu et al., 2016)\u003c/a\u003e. No further information is provided on the sampling strategy.\u003c/p\u003e\n\u003cp\u003eThe TurkCorpus dataset was developed in order to overcome some of the problems with sentence pairs from Standard and Simple Wikipedia: a large fraction of sentences were misaligned, or not actually simpler \u003ca href=\"https://www.aclweb.org/anthology/Q16-1029.pdf\"\u003e(Xu et al., 2016)\u003c/a\u003e. However, TurkCorpus mainly focused on \u003cem\u003elexical paraphrasing\u003c/em\u003e, and so cannot be used to evaluate simplifications involving \u003cem\u003ecompression\u003c/em\u003e (deletion) or \u003cem\u003esentence splitting\u003c/em\u003e. HSplit \u003ca href=\"https://www.aclweb.org/anthology/D18-1081.pdf\"\u003e(Sulem et al., 2018)\u003c/a\u003e, on the other hand, can only be used to evaluate sentence splitting. The reference sentences in ASSET include a wider variety of sentence rewriting strategies, combining splitting, compression and paraphrasing. Annotators were given examples of each kind of transformation individually, as well as all three transformations used at once, but were allowed to decide which transformations to use for any given sentence.\u003c/p\u003e\n\u003cp\u003eAn example illustrating the differences between TurkCorpus, HSplit and ASSET is given below:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eOriginal:\u003c/strong\u003e He settled in London, devoting himself chiefly to practical teaching.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTurkCorpus:\u003c/strong\u003e He rooted in London, devoting himself mainly to practical teaching.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHSplit:\u003c/strong\u003e He settled in London. He devoted himself chiefly to practical teaching.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eASSET:\u003c/strong\u003e He lived in London. He was a teacher.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"communicative-goal\"\u003eCommunicative Goal\u003c/h3\u003e\n\u003cp\u003eThe goal is to communicate the main ideas of source sentence in a way that is easier to understand by non-native speakers of English. This could be done by replacing complex words with simpler synonyms (i.e. paraphrasing), deleting unimportant information (i.e. compression), and/or splitting a long complex sentence into several simpler ones.\u003c/p\u003e\n\u003ch3 id=\"source-data\"\u003eSource Data\u003c/h3\u003e\n\u003ch4 id=\"initial-data-collection-and-normalization\"\u003eInitial Data Collection and Normalization\u003c/h4\u003e\n\u003cp\u003eNot applicable since ASSET uses the same 2,359 sentences from \u003ca href=\"https://github.com/cocoxu/simplification/\"\u003eTurkCorpus\u003c/a\u003e \u003ca href=\"https://www.aclweb.org/anthology/Q16-1029.pdf\"\u003e(Xu et al., 2016)\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"who-are-the-source-language-producers\"\u003eWho are the source language producers?\u003c/h4\u003e\n\u003cp\u003eThe dataset uses language from English Wikipedia (August 22, 2009 version): some demographic information is provided \u003ca href=\"https://en.wikipedia.org/wiki/Wikipedia:Who_writes_Wikipedia%3F\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"annotations\"\u003eAnnotations\u003c/h3\u003e\n\u003ch4 id=\"annotation-process\"\u003eAnnotation process\u003c/h4\u003e\n\u003cp\u003eThe instructions given to the annotators are available \u003ca href=\"https://github.com/facebookresearch/asset/blob/master/crowdsourcing/AMT_AnnotationInstructions.pdf\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch4 id=\"who-are-the-annotators\"\u003eWho are the annotators?\u003c/h4\u003e\n\u003cp\u003eReference sentences were written by 42 workers on Amazon Mechanical Turk (AMT). The requirements for being an annotator were:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePassing a Qualification Test (appropriately simplifying sentences). Out of 100 workers, 42 passed the test.\u003c/li\u003e\n\u003cli\u003eBeing a resident of the United States, United Kingdom or Canada.\u003c/li\u003e\n\u003cli\u003eHaving a HIT approval rate over 95%, and over 1000 HITs approved.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNo other demographic or compensation information is provided in the ASSET paper.\u003c/p\u003e\n\u003ch3 id=\"personal-and-sensitive-information\"\u003ePersonal and Sensitive Information\u003c/h3\u003e\n\u003cp\u003eSince the dataset is created from English Wikipedia (August 22, 2009 version), all the information contained in the dataset is already in the public domain.\u003c/p\u003e\n\u003ch2 id=\"changes-to-the-original-dataset-for-gem\"\u003eChanges to the Original Dataset for GEM\u003c/h2\u003e\n\u003cp\u003eNo change is made to the original dataset.\u003c/p\u003e\n\u003ch3 id=\"special-test-sets\"\u003eSpecial test sets\u003c/h3\u003e\n\u003ch4 id=\"subpopulations\"\u003eSubpopulations\u003c/h4\u003e\n\u003cp\u003eThe goal was to assess performance when simplifying source sentences with different syntactic structure and complexity. To this end, we split the original test set according to syntactic complexity of the source sentences. To characterize sentence syntactic complexity, we use the 8-level developmental level (d-level) scale proposed by \u003ca href=\"https://www.researchgate.net/publication/254033869_How_complex_is_that_sentence_A_proposed_revision_of_the_Rosenberg_and_Abbeduto_D-Level_Scale\"\u003eCovington et al. (2006)\u003c/a\u003e and the implementation of \u003ca href=\"https://www.jbe-platform.com/content/journals/10.1075/ijcl.15.4.02lu\"\u003eLu, Xiaofei (2010)\u003c/a\u003e.\nWe thus split the original test set into 8 subsets corresponding to the 8 d-levels assigned to source sentences. We obtain the following number of instances per level and average d-level of the dataset:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eTotal nb. sentences\u003c/th\u003e\n\u003cth\u003eL0\u003c/th\u003e\n\u003cth\u003eL1\u003c/th\u003e\n\u003cth\u003eL2\u003c/th\u003e\n\u003cth\u003eL3\u003c/th\u003e\n\u003cth\u003eL4\u003c/th\u003e\n\u003cth\u003eL5\u003c/th\u003e\n\u003cth\u003eL6\u003c/th\u003e\n\u003cth\u003eL7\u003c/th\u003e\n\u003cth\u003eMean Level\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e359\u003c/td\u003e\n\u003ctd\u003e166\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e58\u003c/td\u003e\n\u003ctd\u003e32\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e28\u003c/td\u003e\n\u003ctd\u003e7\u003c/td\u003e\n\u003ctd\u003e63\u003c/td\u003e\n\u003ctd\u003e2.38\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"considerations-for-using-the-data\"\u003eConsiderations for Using the Data\u003c/h2\u003e\n\u003ch3 id=\"social-impact-of-the-dataset\"\u003eSocial Impact of the Dataset\u003c/h3\u003e\n\u003cp\u003eThe dataset helps move forward the research towards text simplification by creating a higher quality validation and test dataset. Progress in text simplification in turn has the potential to increase the accessibility of written documents to wider audiences.\u003c/p\u003e\n\u003ch3 id=\"impact-on-underserved-communities\"\u003eImpact on Underserved Communities\u003c/h3\u003e\n\u003cp\u003eThe dataset is in English, a language with an abundance of existing resources.\u003c/p\u003e\n\u003ch3 id=\"discussion-of-biases\"\u003eDiscussion of Biases\u003c/h3\u003e\n\u003cp\u003eThe dataset may contain some social biases, as the input sentences are based on Wikipedia. Studies have shown that the English Wikipedia contains both gender biases \u003ca href=\"https://research.tudelft.nl/en/publications/is-wikipedia-succeeding-in-reducing-gender-bias-assessing-changes\"\u003e(Schmahl et al., 2020)\u003c/a\u003e and racial biases \u003ca href=\"https://journals.sagepub.com/doi/pdf/10.1177/2378023118823946\"\u003e(Adams et al., 2019)\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"other-known-limitations\"\u003eOther Known Limitations\u003c/h3\u003e\n\u003cp\u003eSince the dataset contains only 2,359 sentences that are derived from Wikipedia, it is limited to a small subset of topics present on Wikipedia.\u003c/p\u003e\n\u003ch2 id=\"getting-started-with-in-depth-research-on-the-task\"\u003eGetting started with in-depth research on the task\u003c/h2\u003e\n\u003cp\u003eThe dataset can be downloaded from the original repository \u003ca href=\"https://github.com/facebookresearch/asset\"\u003e(here)\u003c/a\u003e by the authors or can also be used via \u003ca href=\"https://huggingface.co/datasets/asset\"\u003eHuggingFace\u003c/a\u003e and \u003ca href=\"https://www.tensorflow.org/datasets/overview\"\u003eTFDS\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThere are recent supervised (\u003ca href=\"https://arxiv.org/abs/1910.02677\"\u003eMartin et al., 2019\u003c/a\u003e, \u003ca href=\"https://www.aclweb.org/anthology/N19-1317/\"\u003eKriz et al., 2019\u003c/a\u003e, \u003ca href=\"https://www.aclweb.org/anthology/P19-1331/\"\u003eDong et al., 2019\u003c/a\u003e, \u003ca href=\"https://www.aclweb.org/anthology/D17-1062/\"\u003eZhang and Lapata, 2017\u003c/a\u003e) and unsupervised (\u003ca href=\"https://arxiv.org/abs/2005.00352v1\"\u003eMartin et al., 2020\u003c/a\u003e, \u003ca href=\"https://www.aclweb.org/anthology/2020.acl-main.707/\"\u003eKumar et al., 2020\u003c/a\u003e, \u003ca href=\"https://www.aclweb.org/anthology/P19-1198/\"\u003eSurya et al., 2019\u003c/a\u003e) text simplification models that can be used as baselines.\u003c/p\u003e\n\u003cp\u003eThe common metric used for automatic evaluation is SARI \u003ca href=\"https://www.aclweb.org/anthology/Q16-1029/\"\u003e(Xu et al., 2016)\u003c/a\u003e.\u003c/p\u003e\n","title":"ASSET","type":"Simplification","motivation":"ASSET is a high quality simplification dataset where each source (not simple) sentence is associated with 10 human-written simplifications."}},"__N_SSG":true},"page":"/data_cards/[id]","query":{"id":"ASSET"},"buildId":"XLtxCgtRfLh8hV5kAG4wa","nextExport":false,"isFallback":false,"gsp":true,"head":[["meta",{"name":"viewport","content":"width=device-width"}],["meta",{"charSet":"utf-8"}],["link",{"rel":"icon","href":"/favicon.ico"}],["meta",{"name":"description","content":"Benchmark natural language generation systems with GEM."}],["meta",{"property":"og:image","content":"https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light\u0026md=1\u0026fontSize=100px\u0026images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"}],["meta",{"name":"og:title","content":"GEM"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["title",{"children":"GEM ASSET"}]]}</script><script nomodule="" src="/_next/static/chunks/polyfills-e935b474c81cf901504d.js"></script><script src="/_next/static/chunks/main-47bc8f80085b54a800da.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.baa41d4dbf5d52db897c.js" async=""></script><script src="/_next/static/chunks/76fa31df43514ea5aaf6b0525de03ac0f65eec9e.4a36a385313236c59b19.js" async=""></script><script src="/_next/static/chunks/pages/_app-a9ae7a6d1de4e51a7ab6.js" async=""></script><script src="/_next/static/chunks/cb1608f2.c3a9f0eb95374ca4919a.js" async=""></script><script src="/_next/static/chunks/3a03639cec9cbb3837db9d33dd6cd155d0d21f09.13fa0eac69555f2fe80c.js" async=""></script><script src="/_next/static/chunks/pages/data_cards/%5Bid%5D-e1361c13d80af81deda6.js" async=""></script><script src="/_next/static/XLtxCgtRfLh8hV5kAG4wa/_buildManifest.js" async=""></script><script src="/_next/static/XLtxCgtRfLh8hV5kAG4wa/_ssgManifest.js" async=""></script></body></html>