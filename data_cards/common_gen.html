<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="Benchmark natural language generation systems with GEM."/><meta property="og:image" content="https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"/><meta name="og:title" content="GEM"/><meta name="twitter:card" content="summary_large_image"/><title>GEM <!-- -->common_gen</title><meta name="next-head-count" content="8"/><link rel="preload" href="/_next/static/css/42fe94e3e660903d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/42fe94e3e660903d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/93c336621cfc84eb.css" as="style"/><link rel="stylesheet" href="/_next/static/css/93c336621cfc84eb.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-635a834dfd7d0dc2.js" defer=""></script><script src="/_next/static/chunks/framework-7a7e500878b44665.js" defer=""></script><script src="/_next/static/chunks/main-a56c17dda72126ba.js" defer=""></script><script src="/_next/static/chunks/pages/_app-da8862f0ec3a97c1.js" defer=""></script><script src="/_next/static/chunks/c16184b3-ddb1b99b5e568a2a.js" defer=""></script><script src="/_next/static/chunks/50-3dccc3616b494db8.js" defer=""></script><script src="/_next/static/chunks/pages/data_cards/%5Bid%5D-14d2b498c38b49da.js" defer=""></script><script src="/_next/static/456T-gjsiPfCS_Gwi0APa/_buildManifest.js" defer=""></script><script src="/_next/static/456T-gjsiPfCS_Gwi0APa/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="layout_background__oCFQX undefined"><header class="layout_header__SFlEE"><div class="navbar_navwrapper__RkXSe"><div class="navbar_gradbar__Vli6s"></div><nav class="navbar_navbar__vdWdK"><span class="utils_headingLg__RYtYb navbar_navbarlogo__u28NK"><a href="/">GEM BENCHMARK</a></span><div class="navbar_menutoggle__4Urrc" id="mobile-menu"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars navbar_bar__f8cyd" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg></div><ul><li class="navbar_navitem__15TsF navbar_pushright___9_8s"><a href="/resources">Resources</a></li><li class="navbar_navitem__15TsF"><a href="/data_cards">Data Cards</a></li><li class="navbar_navitem__15TsF"><a href="/model_cards">Model Cards</a></li><li class="navbar_navitem__15TsF"><a href="/tutorials">tutorials</a></li><li class="navbar_navitem__15TsF"><a href="/results">Results</a></li><li class="navbar_navitem__15TsF"><a href="/workshop">Workshop</a></li></ul></nav></div></header><div class="layout_container__FUycR layout_wideContainer__IUVFY"><main><article><a href="/data_cards"><a><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="arrow-left" class="svg-inline--fa fa-arrow-left utils_icon__AiQ5I" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path></svg></a></a><span class="utils_spacer__a__NY"></span><span class="utils_headingXl__zlq1q">common_gen</span><span class="utils_smallSpace__dcJPu"></span><span class="utils_lightText__B_gv3">Reasoning</span><div class="datacard-wrapper"><div class="datacard">
  <section class="datacard-section">
    <div class="datacard-summary">
      <h2>common_gen</h2>
      <div class="summary-content">
        <p>CommonGen is an English text generation task to explicitly test machines for the ability of generative
          commonsense reasoning. Given a set of common concepts, the task is to generate a coherent sentence describing
          an everyday scenario using these concepts. CommonGen is challenging because it inherently requires 1)
          relational reasoning using background commonsense knowledge, and 2) compositional generalization ability to
          work on unseen concept combinations. The dataset, constructed through a combination of crowd-sourcing from AMT
          and existing caption corpora, consists of 30k concept-sets and 50k sentences in total. Note that the CommonGen
          test set is private and requires submission to the external leaderboard.</p>
        <p>You can load the dataset via:</p>

        <div class="code-wrapper">
          <div class="toolbar">
            <div class="copy-icon" title="Click to copy code block"></div>
            <div class="expand-modal-icon" title="Click to expand code block"></div>
          </div>
          <pre><code>import datasets
data = datasets.load_dataset('GEM/common_gen')
</code></pre>
        </div>

        <p>The data loader can be found <a href="https://huggingface.co/datasets/GEM/common_gen">here</a>.</p>
      </div>
    </div>

    <div class="datacard-field-wrapper">

      <div class="datacard-field">

        <h5>website

        </h5>

        <p><a href="https://inklab.usc.edu/CommonGen/">link</a></p>
      </div>

      <div class="datacard-field">

        <h5>paper

        </h5>

        <p><a href="https://aclanthology.org/2020.findings-emnlp.165">Link</a></p>
      </div>

      <div class="datacard-field">

        <h5>authors

        </h5>

        <p>Bill Yuchen Lin (USC), Wangchunshu Zhou (USC), Ming Shen (USC), Pei Zhou (USC), Chandra Bhagavatula
          (AllenAI), Yejin Choi (AllenAI + UW), Xiang Ren (USC)</p>
      </div>
    </div>

  </section>

  <section class="datacard-section quick">
    <h3 class="section-title">Quick-Use</h3>

    <div class="datacard-field-wrapper">

      <div class="datacard-field periscope">

        <h5>Contact Name

          <div class="tooltip">
            <div class="tooltip-icon"></div>
            <div class="tooltip-text">
              <p>If known, provide the name of at least one person the reader can contact for questions about the
                dataset.</p>
            </div>
          </div>

        </h5>

        <p>Bill Yuchen Lin</p>
      </div>

      <div class="datacard-field telescope">

        <h5>Multilingual?

          <div class="tooltip">
            <div class="tooltip-icon"></div>
            <div class="tooltip-text">
              <p>Is the dataset multilingual?</p>
            </div>
          </div>

        </h5>

        <p>no</p>
      </div>

      <div class="datacard-field telescope">

        <h5>Covered Languages

          <div class="tooltip">
            <div class="tooltip-icon"></div>
            <div class="tooltip-text">
              <p>What languages/dialects are covered in the dataset?</p>
            </div>
          </div>

        </h5>

        <p><code>English</code></p>
      </div>

      <div class="datacard-field telescope">

        <h5>License

          <div class="tooltip">
            <div class="tooltip-icon"></div>
            <div class="tooltip-text">
              <p>What is the license of the dataset?</p>
            </div>
          </div>

        </h5>

        <p>mit: MIT License</p>
      </div>

      <div class="datacard-field periscope">

        <h5>Communicative Goal

          <div class="tooltip">
            <div class="tooltip-icon"></div>
            <div class="tooltip-text">
              <p>Provide a short description of the communicative goal of a model trained for this task on this dataset.
              </p>
            </div>
          </div>

        </h5>

        <p>The speaker is required to produce a <em>coherent</em> sentence which mentions all of the source concepts,
          and which describes a <em>likely</em> situation that could be captured in a picture or video.</p>
      </div>

      <div class="datacard-field telescope">

        <h5>Additional Annotations?

          <div class="tooltip">
            <div class="tooltip-icon"></div>
            <div class="tooltip-text">
              <p>Does the dataset have additional annotations for each instance?</p>
            </div>
          </div>

        </h5>

        <p>none</p>
      </div>

      <div class="datacard-field telescope">

        <h5>Contains PII?

          <div class="tooltip">
            <div class="tooltip-icon"></div>
            <div class="tooltip-text">
              <p>Does the source language data likely contain Personal Identifying Information about the data creators
                or subjects?</p>
            </div>
          </div>

        </h5>

        <p>no PII</p>
      </div>
    </div>

  </section>


  <section class="datacard-section open">

    <div class="datacard-section-preview">
      <h3>Dataset Overview

        <div class="tooltip">
          <div class="tooltip-icon"></div>
          <div class="tooltip-text">
            <ul>
              <li>
                <h4>Where to find the Data and its Documentation</h4>
              </li>
              <li>
                <h4>Languages and Intended Use</h4>
              </li>
              <li>
                <h4>Credit</h4>
              </li>
              <li>
                <h4>Dataset Structure</h4>
              </li>
            </ul>
          </div>
        </div>

      </h3>
      <button class="expand-button">
        <svg fill="#3c4f50" height="24px" viewBox="0 0 24 24" width="24px" xmlns="http://www.w3.org/2000/svg">
          <path d="M0 0h24v24H0z" fill="none"></path>
          <path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z">
          </path>
        </svg>
      </button>
    </div>

    <div class="datacard-collapsible">



      <div class="datacard-subsection">
        <h4>Where to find the Data and its Documentation</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field telescope">

            <h5>Webpage

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What is the webpage for the dataset (if it exists)?</p>
                </div>
              </div>

            </h5>

            <p><a href="https://inklab.usc.edu/CommonGen/">link</a></p>
          </div>

          <div class="datacard-field telescope">

            <h5>Download

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What is the link to where the original dataset is hosted?</p>
                </div>
              </div>

            </h5>

            <p><a href="https://github.com/INK-USC/CommonGen">Link</a></p>
          </div>

          <div class="datacard-field telescope">

            <h5>Paper

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What is the link to the paper describing the dataset (open access preferred)?</p>
                </div>
              </div>

            </h5>

            <p><a href="https://aclanthology.org/2020.findings-emnlp.165">Link</a></p>
          </div>

          <div class="datacard-field microscope">

            <h5>BibTex

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Provide the BibTex-formatted reference for the dataset. Please use the correct published version
                    (ACL anthology, etc.) instead of google scholar created Bibtex.</p>
                </div>
              </div>

            </h5>


            <div class="code-wrapper">
              <div class="toolbar">
                <div class="copy-icon" title="Click to copy code block"></div>
                <div class="expand-modal-icon" title="Click to expand code block"></div>
              </div>
              <pre><code>@inproceedings{lin-etal-2020-commongen,
title = "{C}ommon{G}en: A Constrained Text Generation Challenge for Generative Commonsense Reasoning",
author = "Lin, Bill Yuchen  and
Zhou, Wangchunshu  and
Shen, Ming  and
Zhou, Pei  and
Bhagavatula, Chandra  and
Choi, Yejin  and
Ren, Xiang",
booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
month = nov,
year = "2020",
address = "Online",
publisher = "Association for Computational Linguistics",
url = "https://www.aclweb.org/anthology/2020.findings-emnlp.165",
pages = "1823--1840",
}
</code></pre>
            </div>

          </div>

          <div class="datacard-field periscope">

            <h5>Contact Name

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>If known, provide the name of at least one person the reader can contact for questions about the
                    dataset.</p>
                </div>
              </div>

            </h5>

            <p>Bill Yuchen Lin</p>
          </div>

          <div class="datacard-field periscope">

            <h5>Contact Email

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>If known, provide the email of at least one person the reader can contact for questions about the
                    dataset.</p>
                </div>
              </div>

            </h5>

            <p><a href="mailto:yuchen.lin@usc.edu">yuchen.lin@usc.edu</a></p>
          </div>

          <div class="datacard-field telescope">

            <h5>Has a Leaderboard?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Does the dataset have an active leaderboard?</p>
                </div>
              </div>

            </h5>

            <p>yes</p>
          </div>

          <div class="datacard-field periscope">

            <h5>Leaderboard Link

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Provide a link to the leaderboard.</p>
                </div>
              </div>

            </h5>

            <p><a href="https://inklab.usc.edu/CommonGen/leaderboard.html">Link</a></p>
          </div>

          <div class="datacard-field microscope">

            <h5>Leaderboard Details

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Briefly describe how the leaderboard evaluates models.</p>
                </div>
              </div>

            </h5>

            <p>The model outputs are evaluated against the crowdsourced references, and ranked by SPICE score. The
              leaderboard also reports BLEU-4 and CIDEr scores.</p>
          </div>
        </div>

      </div>

      <div class="datacard-subsection">
        <h4>Languages and Intended Use</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field telescope">

            <h5>Multilingual?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Is the dataset multilingual?</p>
                </div>
              </div>

            </h5>

            <p>no</p>
          </div>

          <div class="datacard-field periscope">

            <h5>Covered Dialects

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What dialects are covered? Are there multiple dialects per language?</p>
                </div>
              </div>

            </h5>

            <p>No information is provided on regional restrictions and we thus assume that the covered dialects are
              those spoken by raters on Mechanical Turk.</p>
          </div>

          <div class="datacard-field telescope">

            <h5>Covered Languages

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What languages/dialects are covered in the dataset?</p>
                </div>
              </div>

            </h5>

            <p><code>English</code></p>
          </div>

          <div class="datacard-field periscope">

            <h5>Whose Language?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Whose language is in the dataset?</p>
                </div>
              </div>

            </h5>

            <p>The concepts were extracted from multiple English image captioning datasets and the data was collected
              via Amazon Mechanical Turk. No information on regional restrictions is provided.</p>
          </div>

          <div class="datacard-field telescope">

            <h5>License

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What is the license of the dataset?</p>
                </div>
              </div>

            </h5>

            <p>mit: MIT License</p>
          </div>

          <div class="datacard-field microscope">

            <h5>Intended Use

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What is the intended use of the dataset?</p>
                </div>
              </div>

            </h5>

            <p>CommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test
              machines for the ability of generative commonsense reasoning.</p>
          </div>

          <div class="datacard-field telescope">

            <h5>Primary Task

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What primary task does the dataset support?</p>
                </div>
              </div>

            </h5>

            <p>Reasoning</p>
          </div>

          <div class="datacard-field periscope">

            <h5>Communicative Goal

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Provide a short description of the communicative goal of a model trained for this task on this
                    dataset.</p>
                </div>
              </div>

            </h5>

            <p>The speaker is required to produce a <em>coherent</em> sentence which mentions all of the source
              concepts, and which describes a <em>likely</em> situation that could be captured in a picture or video.
            </p>
          </div>
        </div>

      </div>

      <div class="datacard-subsection">
        <h4>Credit</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field telescope">

            <h5>Curation Organization Type(s)

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>In what kind of organization did the dataset curation happen?</p>
                </div>
              </div>

            </h5>

            <p><code>academic</code>, <code>independent</code></p>
          </div>

          <div class="datacard-field periscope">

            <h5>Curation Organization(s)

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Name the organization(s).</p>
                </div>
              </div>

            </h5>

            <p>The dataset was curated by a joint team of researchers from the University of Southern California and
              Allen Institute for Artificial Intelligence.</p>
          </div>

          <div class="datacard-field microscope">

            <h5>Dataset Creators

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Who created the original dataset? List the people involved in collecting the dataset and their
                    affiliation(s).</p>
                </div>
              </div>

            </h5>

            <p>Bill Yuchen Lin (USC), Wangchunshu Zhou (USC), Ming Shen (USC), Pei Zhou (USC), Chandra Bhagavatula
              (AllenAI), Yejin Choi (AllenAI + UW), Xiang Ren (USC)</p>
          </div>

          <div class="datacard-field microscope">

            <h5>Funding

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Who funded the data creation?</p>
                </div>
              </div>

            </h5>

            <p>The research is based upon work supported in part by the Office of the Director of National Intelligence
              (ODNI), Intelligence Advanced Research Projects Activity (IARPA), the DARPA MCS program, and NSF SMA
              18-29268.</p>
          </div>

          <div class="datacard-field microscope">

            <h5>Who added the Dataset to GEM?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Who contributed to the data card and adding the dataset to GEM? List the people+affiliations
                    involved in creating this data card and who helped integrate this dataset into GEM.</p>
                </div>
              </div>

            </h5>

            <p>Yacine Jernite created the initial data card. It was later extended by Simon Mille. Sebastian Gehrmann
              migrated it to the GEMv2 format.</p>
          </div>
        </div>

      </div>

      <div class="datacard-subsection">
        <h4>Dataset Structure</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field telescope">

            <h5>Data Fields

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>List and describe the fields present in the dataset.</p>
                </div>
              </div>

            </h5>

            <p>A data instance has the following fields:</p>
            <ul>
              <li><code>concepts</code>: a <code>list</code> of <code>string</code> values denoting the concept the
                system should write about. Has 3 to 5 items, constitutes the <code>input</code> of the task.</li>
              <li><code>target</code>: a sentence <code>string</code> mentioning all of the above mentioned
                <code>concepts</code>. Constitutes the desired <code>output</code> of the task.</li>
            </ul>
          </div>

          <div class="datacard-field periscope">

            <h5>Example Instance

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Provide a JSON formatted example of a typical instance in the dataset.</p>
                </div>
              </div>

            </h5>


            <div class="code-wrapper">
              <div class="toolbar">
                <div class="copy-icon" title="Click to copy code block"></div>
                <div class="expand-modal-icon" title="Click to expand code block"></div>
              </div>
              <pre><code>[
{
"concepts": ['ski', 'mountain', 'skier'],
"target": 'Skier skis down the mountain',
},
{
"concepts": ['ski', 'mountain', 'skier'],
"target": 'Three skiers are skiing on a snowy mountain.',
},
]
</code></pre>
            </div>

          </div>

          <div class="datacard-field periscope">

            <h5>Data Splits

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Describe and name the splits in the dataset if there are more than one.</p>
                </div>
              </div>

            </h5>

            <p>Each example in the dataset consists of a set of 3 to 5 concepts denoted by a single noun, verb, or
              adjective (the input), and a sentence using these concepts (the output). The dataset provides several such
              sentences for each such concept.</p>

            <div class="table-wrapper">
              <div class="toolbar">
                <div class="expand-modal-icon" title="Click to expand table"></div>
              </div>
              <table>
                <thead>
                  <tr>
                    <th></th>
                    <th>Train</th>
                    <th>Dev</th>
                    <th>Test</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><strong>Total concept-sets</strong></td>
                    <td>32,651</td>
                    <td>993</td>
                    <td>1,497</td>
                  </tr>
                  <tr>
                    <td><strong>Total sentences</strong></td>
                    <td>67,389</td>
                    <td>4,018</td>
                    <td>6,042</td>
                  </tr>
                  <tr>
                    <td><strong>Average sentence length</strong></td>
                    <td>10.54</td>
                    <td>11.55</td>
                    <td>13.34</td>
                  </tr>
                </tbody>
              </table>
            </div>

          </div>

          <div class="datacard-field microscope">

            <h5>Splitting Criteria

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Describe any criteria for splitting the data, if used. If there are differences between the splits
                    (e.g., if the training annotations are machine-generated and the dev and test ones are created by
                    humans, or if different numbers of annotators contributed to each example), describe them here.</p>
                </div>
              </div>

            </h5>

            <p>The dev and test set were created by sampling sets of concepts of size 4 or 5 (and as many of size 3 for
              the dev set) present in the source captioning datasets and having crowd-workers write reference sentences
              using these concepts.</p>
            <p>Conversely, the training set has more concept sets of size 3 than of size 4 and 5, and uses the original
              captions from the source datasets as references.</p>
            <p>The authors also ensured that the training, dev and test set have different combinations of unique
              concepts to ensure compositionality (details in <a href="https://arxiv.org/pdf/1911.03705v3.pdf">Table
                1</a>).</p>
          </div>
        </div>

      </div>
    </div>
  </section>

  <section class="datacard-section open">

    <div class="datacard-section-preview">
      <h3>Dataset in GEM

        <div class="tooltip">
          <div class="tooltip-icon"></div>
          <div class="tooltip-text">
            <ul>
              <li>
                <h4>Rationale for Inclusion in GEM</h4>
              </li>
              <li>
                <h4>GEM-Specific Curation</h4>
              </li>
              <li>
                <h4>Getting Started with the Task</h4>
              </li>
            </ul>
          </div>
        </div>

      </h3>
      <button class="expand-button">
        <svg fill="#3c4f50" height="24px" viewBox="0 0 24 24" width="24px" xmlns="http://www.w3.org/2000/svg">
          <path d="M0 0h24v24H0z" fill="none"></path>
          <path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z">
          </path>
        </svg>
      </button>
    </div>

    <div class="datacard-collapsible">



      <div class="datacard-subsection">
        <h4>Rationale for Inclusion in GEM</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field microscope">

            <h5>Why is the Dataset in GEM?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What does this dataset contribute toward better generation evaluation and why is it part of GEM?
                  </p>
                </div>
              </div>

            </h5>

            <p>CommonGen is a medium sized corpus with a unique reasoning challenge and interesting evaluation
              possibilities.</p>
          </div>

          <div class="datacard-field telescope">

            <h5>Similar Datasets

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Do other datasets for the high level task exist?</p>
                </div>
              </div>

            </h5>

            <p>no</p>
          </div>

          <div class="datacard-field periscope">

            <h5>Ability that the Dataset measures

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What aspect of model ability can be measured with this dataset?</p>
                </div>
              </div>

            </h5>

            <p>Commonsense reasoning</p>
          </div>
        </div>

      </div>

      <div class="datacard-subsection">
        <h4>GEM-Specific Curation</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field telescope">

            <h5>Modificatied for GEM?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Has the GEM version of the dataset been modified in any way (data, processing, splits) from the
                    original curated data?</p>
                </div>
              </div>

            </h5>

            <p>yes</p>
          </div>

          <div class="datacard-field periscope">

            <h5>GEM Modifications

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What changes have been made to he original dataset?</p>
                </div>
              </div>

            </h5>

            <p><code>other</code></p>
          </div>

          <div class="datacard-field microscope">

            <h5>Modification Details

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>For each of these changes, described them in more details and provided the intended purpose of the
                    modification</p>
                </div>
              </div>

            </h5>

            <p>4 challenge sets for CommenGen were added to the GEM evaluation suite.</p>
          </div>

          <div class="datacard-field telescope">

            <h5>Additional Splits?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Does GEM provide additional splits to the dataset?</p>
                </div>
              </div>

            </h5>

            <p>yes</p>
          </div>

          <div class="datacard-field periscope">

            <h5>Split Information

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Describe how the new splits were created</p>
                </div>
              </div>

            </h5>

            <ol>
              <li>Data Shift</li>
            </ol>
            <p>We created subsets of the training and development sets of ~500 randomly selected inputs each.</p>
            <ol start="2">
              <li>Transformations</li>
            </ol>
            <p>We applied input scrambling on a subset of 500 randomly selected test instances; the order of the
              concepts was randomly reassigned.</p>
            <ol start="3">
              <li>Subpopulations</li>
            </ol>
            <p>We created a subpopulation based on input length, taking into account the number of concepts the input
              test structures. By comparing inputs of different lengths, we can see to what extent systems are able to
              handle different input sizes</p>

            <div class="table-wrapper">
              <div class="toolbar">
                <div class="expand-modal-icon" title="Click to expand table"></div>
              </div>
              <table>
                <thead>
                  <tr>
                    <th>Concept number</th>
                    <th>Frequency English</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>4</td>
                    <td>747</td>
                  </tr>
                  <tr>
                    <td>5</td>
                    <td>750</td>
                  </tr>
                </tbody>
              </table>
            </div>

          </div>

          <div class="datacard-field periscope">

            <h5>Split Motivation

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What aspects of the model's generation capacities were the splits created to test?</p>
                </div>
              </div>

            </h5>

            <p>Generalization and Robustness</p>
          </div>
        </div>

      </div>

      <div class="datacard-subsection">
        <h4>Getting Started with the Task</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field microscope">

            <h5>Pointers to Resources

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Getting started with in-depth research on the task. Add relevant pointers to resources that
                    researchers can consult when they want to get started digging deeper into the task.</p>
                </div>
              </div>

            </h5>

            <ul>
              <li>Two variants of <a href="https://arxiv.org/abs/1910.13461">BART</a>, <a
                  href="https://arxiv.org/abs/2009.12677">Knowledge Graph augemnted-BART</a> and <a
                  href="https://arxiv.org/abs/2012.00366">Enhanced Knowledge Injection Model for Commonsense
                  Generation</a>, hold the top two spots on the leaderboard, followed by a fine-tuned <a
                  href="https://arxiv.org/abs/1910.10683">T5 model</a>.</li>
              <li>The following script shows how to download and load the data, fine-tune, and evaluate a model using
                the ROUGE, BLEU, and METEOR metrics: <a
                  href="https://github.com/GEM-benchmark/GEM-baseline-models/blob/main/examples/GEM-common_gen.ipynb">GEM
                  sample script</a>.</li>
            </ul>
          </div>
        </div>

      </div>
    </div>
  </section>

  <section class="datacard-section open">

    <div class="datacard-section-preview">
      <h3>Previous Results

        <div class="tooltip">
          <div class="tooltip-icon"></div>
          <div class="tooltip-text">
            <ul>
              <li>
                <h4>Previous Results</h4>
              </li>
            </ul>
          </div>
        </div>

      </h3>
      <button class="expand-button">
        <svg fill="#3c4f50" height="24px" viewBox="0 0 24 24" width="24px" xmlns="http://www.w3.org/2000/svg">
          <path d="M0 0h24v24H0z" fill="none"></path>
          <path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z">
          </path>
        </svg>
      </button>
    </div>

    <div class="datacard-collapsible">



      <div class="datacard-subsection">
        <h4>Previous Results</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field telescope">

            <h5>Measured Model Abilities

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What aspect of model ability can be measured with this dataset?</p>
                </div>
              </div>

            </h5>

            <p>Commonsense Reasoning</p>
          </div>

          <div class="datacard-field periscope">

            <h5>Metrics

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What metrics are typically used for this task?</p>
                </div>
              </div>

            </h5>

            <p><code>Other: Other Metrics</code>, <code>BLEU</code>, <code>ROUGE</code>, <code>METEOR</code></p>
          </div>

          <div class="datacard-field periscope">

            <h5>Other Metrics

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Definitions of other metrics</p>
                </div>
              </div>

            </h5>

            <ul>
              <li>SPICE: An evaluation metric for image captioning that is defined over scene graphs</li>
              <li>CIDEr: An n-gram overlap metric based on cosine similarity between the TF-IDF weighted ngram counts
              </li>
            </ul>
          </div>

          <div class="datacard-field microscope">

            <h5>Proposed Evaluation

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>List and describe the purpose of the metrics and evaluation methodology (including human
                    evaluation) that the dataset creators used when introducing this task.</p>
                </div>
              </div>

            </h5>

            <p>The main metrics are captioning metrics since the original concept lists were extracted from captioning
              datasets. A human subject study with five graduate students was conducted and they were asked to rank the
              "commonsense plausibility" of two models at a time.</p>
          </div>

          <div class="datacard-field telescope">

            <h5>Previous results available?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Are previous results available?</p>
                </div>
              </div>

            </h5>

            <p>yes</p>
          </div>

          <div class="datacard-field periscope">

            <h5>Other Evaluation Approaches

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What evaluation approaches have others used?</p>
                </div>
              </div>

            </h5>

            <p>The currently best performing model KFCNet (<a
                href="https://aclanthology.org/2021.findings-emnlp.249/">https://aclanthology.org/2021.findings-emnlp.249/</a>)
              uses the same automatic evaluation but does not conduct any human evaluation.</p>
          </div>

          <div class="datacard-field microscope">

            <h5>Relevant Previous Results

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What are the most relevant previous results for this task/dataset?</p>
                </div>
              </div>

            </h5>

            <p>The most relevant results can be seen on the <a
                href="https://inklab.usc.edu/CommonGen/leaderboard.html">leaderboard</a></p>
          </div>
        </div>

      </div>
    </div>
  </section>

  <section class="datacard-section open">

    <div class="datacard-section-preview">
      <h3>Dataset Curation

        <div class="tooltip">
          <div class="tooltip-icon"></div>
          <div class="tooltip-text">
            <ul>
              <li>
                <h4>Original Curation</h4>
              </li>
              <li>
                <h4>Language Data</h4>
              </li>
              <li>
                <h4>Structured Annotations</h4>
              </li>
              <li>
                <h4>Consent</h4>
              </li>
              <li>
                <h4>Private Identifying Information (PII)</h4>
              </li>
              <li>
                <h4>Maintenance</h4>
              </li>
            </ul>
          </div>
        </div>

      </h3>
      <button class="expand-button">
        <svg fill="#3c4f50" height="24px" viewBox="0 0 24 24" width="24px" xmlns="http://www.w3.org/2000/svg">
          <path d="M0 0h24v24H0z" fill="none"></path>
          <path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z">
          </path>
        </svg>
      </button>
    </div>

    <div class="datacard-collapsible">



      <div class="datacard-subsection">
        <h4>Original Curation</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field telescope">

            <h5>Original Curation Rationale

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Original curation rationale</p>
                </div>
              </div>

            </h5>

            <p>The dataset creators selected sets of concepts that appeared in image and video captions (as identified
              by a POS tagger) to ensure that a likely real-world scenario including the set could be imagined and
              constructed. Section 3.1 of the <a href="https://arxiv.org/pdf/1911.03705v3.pdf">paper</a> describes a
              sampling scheme which encourages diversity of sets while selecting common concepts.</p>
          </div>

          <div class="datacard-field periscope">

            <h5>Communicative Goal

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What was the communicative goal?</p>
                </div>
              </div>

            </h5>

            <p>The speaker is required to produce a <em>coherent</em> sentence which mentions all of the source
              concepts, and which describes a <em>likely</em> situation that could be captured in a picture or video.
            </p>
          </div>

          <div class="datacard-field telescope">

            <h5>Sourced from Different Sources

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Is the dataset aggregated from different data sources?</p>
                </div>
              </div>

            </h5>

            <p>yes</p>
          </div>

          <div class="datacard-field periscope">

            <h5>Source Details

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>List the sources (one per line)</p>
                </div>
              </div>

            </h5>

            <ul>
              <li><a href="https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00166">Flickr30k</a></li>
              <li><a href="https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48">MSCOCO</a></li>
              <li><a href="https://www.aclweb.org/anthology/P18-1238/">Conceptual Captions</a></li>
              <li>Video captioning datasets:
                <ul>
                  <li><a href="https://link.springer.com/article/10.1007/s11263-016-0987-1">LSMDC</a></li>
                  <li><a
                      href="https://openaccess.thecvf.com/content_iccv_2017/html/Krishna_Dense-Captioning_Events_in_ICCV_2017_paper.html">ActivityNet</a>
                  </li>
                  <li><a
                      href="https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_VaTeX_A_Large-Scale_High-Quality_Multilingual_Dataset_for_Video-and-Language_Research_ICCV_2019_paper.html">VaTeX</a>
                  </li>
                </ul>
              </li>
            </ul>
          </div>
        </div>

      </div>

      <div class="datacard-subsection">
        <h4>Language Data</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field telescope">

            <h5>How was Language Data Obtained?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>How was the language data obtained?</p>
                </div>
              </div>

            </h5>

            <p><code>Crowdsourced</code></p>
          </div>

          <div class="datacard-field periscope">

            <h5>Where was it crowdsourced?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>If crowdsourced, where from?</p>
                </div>
              </div>

            </h5>

            <p><code>Amazon Mechanical Turk</code></p>
          </div>

          <div class="datacard-field microscope">

            <h5>Language Producers

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What further information do we have on the language producers?</p>
                </div>
              </div>

            </h5>

            <p>The training data consists of concept sets and captions for the source datasets. The concept sets are the
              sets of labels of the images or videos, selected with a heuristic to maximize diversity while ensuring
              that they represent likely scenarios.</p>
            <p>The dev and test set sentences were created by Amazon Mechanical Turk crowd workers. The workers were
              shown an example generation and a set of 4 or 5 concept names along with their part-of-speech and asked to
              write:</p>
            <ol>
              <li>One sentence mentioning all of the concepts</li>
              <li>A rationale explaining how the sentence connects the concept</li>
            </ol>
            <p>A screenshot of the interface is provided in Figure 7 of the <a
                href="https://arxiv.org/pdf/1911.03705v3.pdf">Appendix</a>.</p>
          </div>

          <div class="datacard-field periscope">

            <h5>Topics Covered

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Does the language in the dataset focus on specific topics? How would you describe them?</p>
                </div>
              </div>

            </h5>

            <p>Information was not provided.</p>
          </div>

          <div class="datacard-field telescope">

            <h5>Data Validation

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Was the text validated by a different worker or a data curator?</p>
                </div>
              </div>

            </h5>

            <p>validated by data curator</p>
          </div>

          <div class="datacard-field telescope">

            <h5>Was Data Filtered?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Were text instances selected or filtered?</p>
                </div>
              </div>

            </h5>

            <p>algorithmically</p>
          </div>

          <div class="datacard-field microscope">

            <h5>Filter Criteria

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What were the selection criteria?</p>
                </div>
              </div>

            </h5>

            <p>During the data collection, workers who provided rationales that were too short, failed to have good
              coverage of the input in their sentences, or workers whose output had a high perplexity under a GPT-2
              model were disqualified from the pool and replaced with newcomers.</p>
          </div>
        </div>

      </div>

      <div class="datacard-subsection">
        <h4>Structured Annotations</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field telescope">

            <h5>Additional Annotations?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Does the dataset have additional annotations for each instance?</p>
                </div>
              </div>

            </h5>

            <p>none</p>
          </div>

          <div class="datacard-field telescope">

            <h5>Annotation Service?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Was an annotation service used?</p>
                </div>
              </div>

            </h5>

            <p>no</p>
          </div>
        </div>

      </div>

      <div class="datacard-subsection">
        <h4>Consent</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field telescope">

            <h5>Any Consent Policy?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Was there a consent policy involved when gathering the data?</p>
                </div>
              </div>

            </h5>

            <p>no</p>
          </div>

          <div class="datacard-field microscope">

            <h5>Justification for Using the Data

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>If not, what is the justification for reusing the data?</p>
                </div>
              </div>

            </h5>

            <p>The data was sourced from Mechanical Turk which means that raters were aware that their annotations may
              be publicly released for research purposes.</p>
          </div>
        </div>

      </div>

      <div class="datacard-subsection">
        <h4>Private Identifying Information (PII)</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field telescope">

            <h5>Contains PII?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Does the source language data likely contain Personal Identifying Information about the data
                    creators or subjects?</p>
                </div>
              </div>

            </h5>

            <p>no PII</p>
          </div>

          <div class="datacard-field periscope">

            <h5>Justification for no PII

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Provide a justification for selecting <code>no PII</code> above.</p>
                </div>
              </div>

            </h5>

            <p>The concepts are restricted to verbs, adjectives, and common nouns, and no personal information is given
              in the captions.</p>
          </div>
        </div>

      </div>

      <div class="datacard-subsection">
        <h4>Maintenance</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field telescope">

            <h5>Any Maintenance Plan?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Does the original dataset have a maintenance plan?</p>
                </div>
              </div>

            </h5>

            <p>no</p>
          </div>
        </div>

      </div>
    </div>
  </section>

  <section class="datacard-section open">

    <div class="datacard-section-preview">
      <h3>Broader Social Context

        <div class="tooltip">
          <div class="tooltip-icon"></div>
          <div class="tooltip-text">
            <ul>
              <li>
                <h4>Previous Work on the Social Impact of the Dataset</h4>
              </li>
              <li>
                <h4>Impact on Under-Served Communities</h4>
              </li>
              <li>
                <h4>Discussion of Biases</h4>
              </li>
            </ul>
          </div>
        </div>

      </h3>
      <button class="expand-button">
        <svg fill="#3c4f50" height="24px" viewBox="0 0 24 24" width="24px" xmlns="http://www.w3.org/2000/svg">
          <path d="M0 0h24v24H0z" fill="none"></path>
          <path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z">
          </path>
        </svg>
      </button>
    </div>

    <div class="datacard-collapsible">



      <div class="datacard-subsection">
        <h4>Previous Work on the Social Impact of the Dataset</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field telescope">

            <h5>Usage of Models based on the Data

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Are you aware of cases where models trained on the task featured in this dataset ore related tasks
                    have been used in automated systems?</p>
                </div>
              </div>

            </h5>

            <p>no</p>
          </div>
        </div>

      </div>

      <div class="datacard-subsection">
        <h4>Impact on Under-Served Communities</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field telescope">

            <h5>Addresses needs of underserved Communities?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Does this dataset address the needs of communities that are traditionally underserved in language
                    technology, and particularly language generation technology? Communities may be underserved for
                    exemple because their language, language variety, or social or geographical context is
                    underepresented in NLP and NLG resources (datasets and models).</p>
                </div>
              </div>

            </h5>

            <p>no</p>
          </div>
        </div>

      </div>

      <div class="datacard-subsection">
        <h4>Discussion of Biases</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field telescope">

            <h5>Any Documented Social Biases?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Are there documented social biases in the dataset? Biases in this context are variations in the
                    ways members of different social categories are represented that can have harmful downstream
                    consequences for members of the more disadvantaged group.</p>
                </div>
              </div>

            </h5>

            <p>no</p>
          </div>

          <div class="datacard-field periscope">

            <h5>Are the Language Producers Representative of the Language?

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Does the distribution of language producers in the dataset accurately represent the full
                    distribution of speakers of the language world-wide? If not, how does it differ?</p>
                </div>
              </div>

            </h5>

            <p>The dataset is created using data from image captioning systems and might inherit some of the social
              biases represented therein (see e.g. <a href="https://arxiv.org/abs/2006.08315">Tang et al. 2020</a>).</p>
            <p>Another related concern is the exposure bias introduced by the initial selection of pictures and video,
              which are likely to over-represent situations that are common in the US at the expense of other parts of
              the world (Flickr, for example, is a US-based company founded in Canada). For more discussion of the
              potential impacts of exposure bias, see e.g. <a href="https://www.aclweb.org/anthology/P16-2096.pdf">The
                Social Impact of Natural Language Processing</a>.</p>
          </div>
        </div>

      </div>
    </div>
  </section>

  <section class="datacard-section open">

    <div class="datacard-section-preview">
      <h3>Considerations for Using the Data

        <div class="tooltip">
          <div class="tooltip-icon"></div>
          <div class="tooltip-text">
            <ul>
              <li>
                <h4>PII Risks and Liability</h4>
              </li>
              <li>
                <h4>Licenses</h4>
              </li>
              <li>
                <h4>Known Technical Limitations</h4>
              </li>
            </ul>
          </div>
        </div>

      </h3>
      <button class="expand-button">
        <svg fill="#3c4f50" height="24px" viewBox="0 0 24 24" width="24px" xmlns="http://www.w3.org/2000/svg">
          <path d="M0 0h24v24H0z" fill="none"></path>
          <path d="M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z">
          </path>
        </svg>
      </button>
    </div>

    <div class="datacard-collapsible">



      <div class="datacard-subsection">
        <h4>PII Risks and Liability</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field microscope">

            <h5>Potential PII Risk

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Considering your answers to the PII part of the Data Curation Section, describe any potential
                    privacy to the data subjects and creators risks when using the dataset.</p>
                </div>
              </div>

            </h5>

            <p>The concepts are restricted to verbs, adjectives, and common nouns, and no personal information is given
              in the captions.</p>
          </div>
        </div>

      </div>

      <div class="datacard-subsection">
        <h4>Licenses</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field periscope">

            <h5>Copyright Restrictions on the Dataset

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Based on your answers in the Intended Use part of the Data Overview Section, which of the following
                    best describe the copyright and licensing status of the dataset?</p>
                </div>
              </div>

            </h5>

            <p><code>open license - commercial use allowed</code></p>
          </div>

          <div class="datacard-field periscope">

            <h5>Copyright Restrictions on the Language Data

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Based on your answers in the Language part of the Data Curation Section, which of the following
                    best describe the copyright and licensing status of the underlying language data?</p>
                </div>
              </div>

            </h5>

            <p><code>open license - commercial use allowed</code></p>
          </div>
        </div>

      </div>

      <div class="datacard-subsection">
        <h4>Known Technical Limitations</h4>


        <div class="datacard-field-wrapper">

          <div class="datacard-field microscope">

            <h5>Technical Limitations

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>Describe any known technical limitations, such as spurrious correlations, train/test overlap,
                    annotation biases, or mis-annotations, and cite the works that first identified these limitations
                    when possible.</p>
                </div>
              </div>

            </h5>

            <p>The dataset is in English, a language with an abundance of existing resources.</p>
            <p>The use of GPT-2 to validate development ant test sentences <a
                href="https://www.aclweb.org/anthology/D19-1339.pdf">might be cause for similar concern</a>, but we do
              note that the authors only use the model to discount very high perplexity sequences which is less likely
              to surface those biases.</p>
            <p>The language in the development and test set is crowdsourced, which means that it was written by workers
              whose main goal was speed. This is likely to impact the quality and variety of the targets. The population
              of crowdsource workers is also not identically distributed as the the base population of the locations the
              workers come from, which may lead to different representation of situations or underlying expectations of
              what these situations are.</p>
          </div>

          <div class="datacard-field microscope">

            <h5>Unsuited Applications

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>When using a model trained on this dataset in a setting where users or the public may interact with
                    its predictions, what are some pitfalls to look out for? In particular, describe some applications
                    of the general task featured in this dataset that its curation or properties make it less suitable
                    for.</p>
                </div>
              </div>

            </h5>

            <p>Due to the overrepresentation of US-situations, the system may not work for users across the world.
              Moreover, only limited information on the dataset quality are provided and the system may fail as a result
              of unknown issues.</p>
          </div>

          <div class="datacard-field microscope">

            <h5>Discouraged Use Cases

              <div class="tooltip">
                <div class="tooltip-icon"></div>
                <div class="tooltip-text">
                  <p>What are some discouraged use cases of a model trained to maximize the proposed metrics on this
                    dataset? In particular, think about settings where decisions made by a model that performs
                    reasonably well on the metric my still have strong negative consequences for user or members of the
                    public.</p>
                </div>
              </div>

            </h5>

            <p>Any system needs to be evaluated on a broader set of unseen concepts then provided in the dataset. Since
              the references for the test set are private, it is not known how well findings generalize beyond the
              collection methodology.</p>
          </div>
        </div>

      </div>
    </div>
  </section>
</div></div></article></main><div class="layout_push__lpoMK"></div></div><footer class="layout_footer__WlhMu utils_eggshell__3hbbY"><span class="layout_backToHome__D9QFr"><a href="/"> Home</a></span><span>If you have any questions, please join our <a href="https://groups.google.com/g/gem-benchmark" target="_blank" class="utils_accentUnderline__VG89l">google group</a> for support.</span></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"taskData":{"id":"common_gen","contentHtml":"\u003cdiv class=\"datacard\"\u003e\r\n  \u003csection class=\"datacard-section\"\u003e\r\n    \u003cdiv class=\"datacard-summary\"\u003e\r\n      \u003ch2\u003ecommon_gen\u003c/h2\u003e\r\n      \u003cdiv class=\"summary-content\"\u003e\r\n        \u003cp\u003eCommonGen is an English text generation task to explicitly test machines for the ability of generative\r\n          commonsense reasoning. Given a set of common concepts, the task is to generate a coherent sentence describing\r\n          an everyday scenario using these concepts. CommonGen is challenging because it inherently requires 1)\r\n          relational reasoning using background commonsense knowledge, and 2) compositional generalization ability to\r\n          work on unseen concept combinations. The dataset, constructed through a combination of crowd-sourcing from AMT\r\n          and existing caption corpora, consists of 30k concept-sets and 50k sentences in total. Note that the CommonGen\r\n          test set is private and requires submission to the external leaderboard.\u003c/p\u003e\r\n        \u003cp\u003eYou can load the dataset via:\u003c/p\u003e\r\n\r\n        \u003cdiv class=\"code-wrapper\"\u003e\r\n          \u003cdiv class=\"toolbar\"\u003e\r\n            \u003cdiv class=\"copy-icon\" title=\"Click to copy code block\"\u003e\u003c/div\u003e\r\n            \u003cdiv class=\"expand-modal-icon\" title=\"Click to expand code block\"\u003e\u003c/div\u003e\r\n          \u003c/div\u003e\r\n          \u003cpre\u003e\u003ccode\u003eimport datasets\r\ndata = datasets.load_dataset('GEM/common_gen')\r\n\u003c/code\u003e\u003c/pre\u003e\r\n        \u003c/div\u003e\r\n\r\n        \u003cp\u003eThe data loader can be found \u003ca href=\"https://huggingface.co/datasets/GEM/common_gen\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\r\n      \u003c/div\u003e\r\n    \u003c/div\u003e\r\n\r\n    \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n      \u003cdiv class=\"datacard-field\"\u003e\r\n\r\n        \u003ch5\u003ewebsite\r\n\r\n        \u003c/h5\u003e\r\n\r\n        \u003cp\u003e\u003ca href=\"https://inklab.usc.edu/CommonGen/\"\u003elink\u003c/a\u003e\u003c/p\u003e\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-field\"\u003e\r\n\r\n        \u003ch5\u003epaper\r\n\r\n        \u003c/h5\u003e\r\n\r\n        \u003cp\u003e\u003ca href=\"https://aclanthology.org/2020.findings-emnlp.165\"\u003eLink\u003c/a\u003e\u003c/p\u003e\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-field\"\u003e\r\n\r\n        \u003ch5\u003eauthors\r\n\r\n        \u003c/h5\u003e\r\n\r\n        \u003cp\u003eBill Yuchen Lin (USC), Wangchunshu Zhou (USC), Ming Shen (USC), Pei Zhou (USC), Chandra Bhagavatula\r\n          (AllenAI), Yejin Choi (AllenAI + UW), Xiang Ren (USC)\u003c/p\u003e\r\n      \u003c/div\u003e\r\n    \u003c/div\u003e\r\n\r\n  \u003c/section\u003e\r\n\r\n  \u003csection class=\"datacard-section quick\"\u003e\r\n    \u003ch3 class=\"section-title\"\u003eQuick-Use\u003c/h3\u003e\r\n\r\n    \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n      \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n        \u003ch5\u003eContact Name\r\n\r\n          \u003cdiv class=\"tooltip\"\u003e\r\n            \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n            \u003cdiv class=\"tooltip-text\"\u003e\r\n              \u003cp\u003eIf known, provide the name of at least one person the reader can contact for questions about the\r\n                dataset.\u003c/p\u003e\r\n            \u003c/div\u003e\r\n          \u003c/div\u003e\r\n\r\n        \u003c/h5\u003e\r\n\r\n        \u003cp\u003eBill Yuchen Lin\u003c/p\u003e\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n        \u003ch5\u003eMultilingual?\r\n\r\n          \u003cdiv class=\"tooltip\"\u003e\r\n            \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n            \u003cdiv class=\"tooltip-text\"\u003e\r\n              \u003cp\u003eIs the dataset multilingual?\u003c/p\u003e\r\n            \u003c/div\u003e\r\n          \u003c/div\u003e\r\n\r\n        \u003c/h5\u003e\r\n\r\n        \u003cp\u003eno\u003c/p\u003e\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n        \u003ch5\u003eCovered Languages\r\n\r\n          \u003cdiv class=\"tooltip\"\u003e\r\n            \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n            \u003cdiv class=\"tooltip-text\"\u003e\r\n              \u003cp\u003eWhat languages/dialects are covered in the dataset?\u003c/p\u003e\r\n            \u003c/div\u003e\r\n          \u003c/div\u003e\r\n\r\n        \u003c/h5\u003e\r\n\r\n        \u003cp\u003e\u003ccode\u003eEnglish\u003c/code\u003e\u003c/p\u003e\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n        \u003ch5\u003eLicense\r\n\r\n          \u003cdiv class=\"tooltip\"\u003e\r\n            \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n            \u003cdiv class=\"tooltip-text\"\u003e\r\n              \u003cp\u003eWhat is the license of the dataset?\u003c/p\u003e\r\n            \u003c/div\u003e\r\n          \u003c/div\u003e\r\n\r\n        \u003c/h5\u003e\r\n\r\n        \u003cp\u003emit: MIT License\u003c/p\u003e\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n        \u003ch5\u003eCommunicative Goal\r\n\r\n          \u003cdiv class=\"tooltip\"\u003e\r\n            \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n            \u003cdiv class=\"tooltip-text\"\u003e\r\n              \u003cp\u003eProvide a short description of the communicative goal of a model trained for this task on this dataset.\r\n              \u003c/p\u003e\r\n            \u003c/div\u003e\r\n          \u003c/div\u003e\r\n\r\n        \u003c/h5\u003e\r\n\r\n        \u003cp\u003eThe speaker is required to produce a \u003cem\u003ecoherent\u003c/em\u003e sentence which mentions all of the source concepts,\r\n          and which describes a \u003cem\u003elikely\u003c/em\u003e situation that could be captured in a picture or video.\u003c/p\u003e\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n        \u003ch5\u003eAdditional Annotations?\r\n\r\n          \u003cdiv class=\"tooltip\"\u003e\r\n            \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n            \u003cdiv class=\"tooltip-text\"\u003e\r\n              \u003cp\u003eDoes the dataset have additional annotations for each instance?\u003c/p\u003e\r\n            \u003c/div\u003e\r\n          \u003c/div\u003e\r\n\r\n        \u003c/h5\u003e\r\n\r\n        \u003cp\u003enone\u003c/p\u003e\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n        \u003ch5\u003eContains PII?\r\n\r\n          \u003cdiv class=\"tooltip\"\u003e\r\n            \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n            \u003cdiv class=\"tooltip-text\"\u003e\r\n              \u003cp\u003eDoes the source language data likely contain Personal Identifying Information about the data creators\r\n                or subjects?\u003c/p\u003e\r\n            \u003c/div\u003e\r\n          \u003c/div\u003e\r\n\r\n        \u003c/h5\u003e\r\n\r\n        \u003cp\u003eno PII\u003c/p\u003e\r\n      \u003c/div\u003e\r\n    \u003c/div\u003e\r\n\r\n  \u003c/section\u003e\r\n\r\n\r\n  \u003csection class=\"datacard-section open\"\u003e\r\n\r\n    \u003cdiv class=\"datacard-section-preview\"\u003e\r\n      \u003ch3\u003eDataset Overview\r\n\r\n        \u003cdiv class=\"tooltip\"\u003e\r\n          \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n          \u003cdiv class=\"tooltip-text\"\u003e\r\n            \u003cul\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eWhere to find the Data and its Documentation\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eLanguages and Intended Use\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eCredit\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eDataset Structure\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n            \u003c/ul\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/h3\u003e\r\n      \u003cbutton class=\"expand-button\"\u003e\r\n        \u003csvg fill=\"#3c4f50\" height=\"24px\" viewBox=\"0 0 24 24\" width=\"24px\" xmlns=\"http://www.w3.org/2000/svg\"\u003e\r\n          \u003cpath d=\"M0 0h24v24H0z\" fill=\"none\"\u003e\u003c/path\u003e\r\n          \u003cpath d=\"M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z\"\u003e\r\n          \u003c/path\u003e\r\n        \u003c/svg\u003e\r\n      \u003c/button\u003e\r\n    \u003c/div\u003e\r\n\r\n    \u003cdiv class=\"datacard-collapsible\"\u003e\r\n\r\n\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eWhere to find the Data and its Documentation\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eWebpage\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat is the webpage for the dataset (if it exists)?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003e\u003ca href=\"https://inklab.usc.edu/CommonGen/\"\u003elink\u003c/a\u003e\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eDownload\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat is the link to where the original dataset is hosted?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003e\u003ca href=\"https://github.com/INK-USC/CommonGen\"\u003eLink\u003c/a\u003e\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003ePaper\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat is the link to the paper describing the dataset (open access preferred)?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003e\u003ca href=\"https://aclanthology.org/2020.findings-emnlp.165\"\u003eLink\u003c/a\u003e\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eBibTex\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eProvide the BibTex-formatted reference for the dataset. Please use the correct published version\r\n                    (ACL anthology, etc.) instead of google scholar created Bibtex.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n\r\n            \u003cdiv class=\"code-wrapper\"\u003e\r\n              \u003cdiv class=\"toolbar\"\u003e\r\n                \u003cdiv class=\"copy-icon\" title=\"Click to copy code block\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"expand-modal-icon\" title=\"Click to expand code block\"\u003e\u003c/div\u003e\r\n              \u003c/div\u003e\r\n              \u003cpre\u003e\u003ccode\u003e@inproceedings{lin-etal-2020-commongen,\r\ntitle = \"{C}ommon{G}en: A Constrained Text Generation Challenge for Generative Commonsense Reasoning\",\r\nauthor = \"Lin, Bill Yuchen  and\r\nZhou, Wangchunshu  and\r\nShen, Ming  and\r\nZhou, Pei  and\r\nBhagavatula, Chandra  and\r\nChoi, Yejin  and\r\nRen, Xiang\",\r\nbooktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\r\nmonth = nov,\r\nyear = \"2020\",\r\naddress = \"Online\",\r\npublisher = \"Association for Computational Linguistics\",\r\nurl = \"https://www.aclweb.org/anthology/2020.findings-emnlp.165\",\r\npages = \"1823--1840\",\r\n}\r\n\u003c/code\u003e\u003c/pre\u003e\r\n            \u003c/div\u003e\r\n\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eContact Name\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eIf known, provide the name of at least one person the reader can contact for questions about the\r\n                    dataset.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eBill Yuchen Lin\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eContact Email\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eIf known, provide the email of at least one person the reader can contact for questions about the\r\n                    dataset.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003e\u003ca href=\"mailto:yuchen.lin@usc.edu\"\u003eyuchen.lin@usc.edu\u003c/a\u003e\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eHas a Leaderboard?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eDoes the dataset have an active leaderboard?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eyes\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eLeaderboard Link\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eProvide a link to the leaderboard.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003e\u003ca href=\"https://inklab.usc.edu/CommonGen/leaderboard.html\"\u003eLink\u003c/a\u003e\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eLeaderboard Details\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eBriefly describe how the leaderboard evaluates models.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe model outputs are evaluated against the crowdsourced references, and ranked by SPICE score. The\r\n              leaderboard also reports BLEU-4 and CIDEr scores.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eLanguages and Intended Use\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eMultilingual?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eIs the dataset multilingual?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eno\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eCovered Dialects\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat dialects are covered? Are there multiple dialects per language?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eNo information is provided on regional restrictions and we thus assume that the covered dialects are\r\n              those spoken by raters on Mechanical Turk.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eCovered Languages\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat languages/dialects are covered in the dataset?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003e\u003ccode\u003eEnglish\u003c/code\u003e\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eWhose Language?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhose language is in the dataset?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe concepts were extracted from multiple English image captioning datasets and the data was collected\r\n              via Amazon Mechanical Turk. No information on regional restrictions is provided.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eLicense\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat is the license of the dataset?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003emit: MIT License\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eIntended Use\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat is the intended use of the dataset?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eCommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test\r\n              machines for the ability of generative commonsense reasoning.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003ePrimary Task\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat primary task does the dataset support?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eReasoning\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eCommunicative Goal\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eProvide a short description of the communicative goal of a model trained for this task on this\r\n                    dataset.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe speaker is required to produce a \u003cem\u003ecoherent\u003c/em\u003e sentence which mentions all of the source\r\n              concepts, and which describes a \u003cem\u003elikely\u003c/em\u003e situation that could be captured in a picture or video.\r\n            \u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eCredit\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eCuration Organization Type(s)\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eIn what kind of organization did the dataset curation happen?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003e\u003ccode\u003eacademic\u003c/code\u003e, \u003ccode\u003eindependent\u003c/code\u003e\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eCuration Organization(s)\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eName the organization(s).\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe dataset was curated by a joint team of researchers from the University of Southern California and\r\n              Allen Institute for Artificial Intelligence.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eDataset Creators\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWho created the original dataset? List the people involved in collecting the dataset and their\r\n                    affiliation(s).\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eBill Yuchen Lin (USC), Wangchunshu Zhou (USC), Ming Shen (USC), Pei Zhou (USC), Chandra Bhagavatula\r\n              (AllenAI), Yejin Choi (AllenAI + UW), Xiang Ren (USC)\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eFunding\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWho funded the data creation?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe research is based upon work supported in part by the Office of the Director of National Intelligence\r\n              (ODNI), Intelligence Advanced Research Projects Activity (IARPA), the DARPA MCS program, and NSF SMA\r\n              18-29268.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eWho added the Dataset to GEM?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWho contributed to the data card and adding the dataset to GEM? List the people+affiliations\r\n                    involved in creating this data card and who helped integrate this dataset into GEM.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eYacine Jernite created the initial data card. It was later extended by Simon Mille. Sebastian Gehrmann\r\n              migrated it to the GEMv2 format.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eDataset Structure\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eData Fields\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eList and describe the fields present in the dataset.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eA data instance has the following fields:\u003c/p\u003e\r\n            \u003cul\u003e\r\n              \u003cli\u003e\u003ccode\u003econcepts\u003c/code\u003e: a \u003ccode\u003elist\u003c/code\u003e of \u003ccode\u003estring\u003c/code\u003e values denoting the concept the\r\n                system should write about. Has 3 to 5 items, constitutes the \u003ccode\u003einput\u003c/code\u003e of the task.\u003c/li\u003e\r\n              \u003cli\u003e\u003ccode\u003etarget\u003c/code\u003e: a sentence \u003ccode\u003estring\u003c/code\u003e mentioning all of the above mentioned\r\n                \u003ccode\u003econcepts\u003c/code\u003e. Constitutes the desired \u003ccode\u003eoutput\u003c/code\u003e of the task.\u003c/li\u003e\r\n            \u003c/ul\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eExample Instance\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eProvide a JSON formatted example of a typical instance in the dataset.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n\r\n            \u003cdiv class=\"code-wrapper\"\u003e\r\n              \u003cdiv class=\"toolbar\"\u003e\r\n                \u003cdiv class=\"copy-icon\" title=\"Click to copy code block\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"expand-modal-icon\" title=\"Click to expand code block\"\u003e\u003c/div\u003e\r\n              \u003c/div\u003e\r\n              \u003cpre\u003e\u003ccode\u003e[\r\n{\r\n\"concepts\": ['ski', 'mountain', 'skier'],\r\n\"target\": 'Skier skis down the mountain',\r\n},\r\n{\r\n\"concepts\": ['ski', 'mountain', 'skier'],\r\n\"target\": 'Three skiers are skiing on a snowy mountain.',\r\n},\r\n]\r\n\u003c/code\u003e\u003c/pre\u003e\r\n            \u003c/div\u003e\r\n\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eData Splits\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eDescribe and name the splits in the dataset if there are more than one.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eEach example in the dataset consists of a set of 3 to 5 concepts denoted by a single noun, verb, or\r\n              adjective (the input), and a sentence using these concepts (the output). The dataset provides several such\r\n              sentences for each such concept.\u003c/p\u003e\r\n\r\n            \u003cdiv class=\"table-wrapper\"\u003e\r\n              \u003cdiv class=\"toolbar\"\u003e\r\n                \u003cdiv class=\"expand-modal-icon\" title=\"Click to expand table\"\u003e\u003c/div\u003e\r\n              \u003c/div\u003e\r\n              \u003ctable\u003e\r\n                \u003cthead\u003e\r\n                  \u003ctr\u003e\r\n                    \u003cth\u003e\u003c/th\u003e\r\n                    \u003cth\u003eTrain\u003c/th\u003e\r\n                    \u003cth\u003eDev\u003c/th\u003e\r\n                    \u003cth\u003eTest\u003c/th\u003e\r\n                  \u003c/tr\u003e\r\n                \u003c/thead\u003e\r\n                \u003ctbody\u003e\r\n                  \u003ctr\u003e\r\n                    \u003ctd\u003e\u003cstrong\u003eTotal concept-sets\u003c/strong\u003e\u003c/td\u003e\r\n                    \u003ctd\u003e32,651\u003c/td\u003e\r\n                    \u003ctd\u003e993\u003c/td\u003e\r\n                    \u003ctd\u003e1,497\u003c/td\u003e\r\n                  \u003c/tr\u003e\r\n                  \u003ctr\u003e\r\n                    \u003ctd\u003e\u003cstrong\u003eTotal sentences\u003c/strong\u003e\u003c/td\u003e\r\n                    \u003ctd\u003e67,389\u003c/td\u003e\r\n                    \u003ctd\u003e4,018\u003c/td\u003e\r\n                    \u003ctd\u003e6,042\u003c/td\u003e\r\n                  \u003c/tr\u003e\r\n                  \u003ctr\u003e\r\n                    \u003ctd\u003e\u003cstrong\u003eAverage sentence length\u003c/strong\u003e\u003c/td\u003e\r\n                    \u003ctd\u003e10.54\u003c/td\u003e\r\n                    \u003ctd\u003e11.55\u003c/td\u003e\r\n                    \u003ctd\u003e13.34\u003c/td\u003e\r\n                  \u003c/tr\u003e\r\n                \u003c/tbody\u003e\r\n              \u003c/table\u003e\r\n            \u003c/div\u003e\r\n\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eSplitting Criteria\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eDescribe any criteria for splitting the data, if used. If there are differences between the splits\r\n                    (e.g., if the training annotations are machine-generated and the dev and test ones are created by\r\n                    humans, or if different numbers of annotators contributed to each example), describe them here.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe dev and test set were created by sampling sets of concepts of size 4 or 5 (and as many of size 3 for\r\n              the dev set) present in the source captioning datasets and having crowd-workers write reference sentences\r\n              using these concepts.\u003c/p\u003e\r\n            \u003cp\u003eConversely, the training set has more concept sets of size 3 than of size 4 and 5, and uses the original\r\n              captions from the source datasets as references.\u003c/p\u003e\r\n            \u003cp\u003eThe authors also ensured that the training, dev and test set have different combinations of unique\r\n              concepts to ensure compositionality (details in \u003ca href=\"https://arxiv.org/pdf/1911.03705v3.pdf\"\u003eTable\r\n                1\u003c/a\u003e).\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n    \u003c/div\u003e\r\n  \u003c/section\u003e\r\n\r\n  \u003csection class=\"datacard-section open\"\u003e\r\n\r\n    \u003cdiv class=\"datacard-section-preview\"\u003e\r\n      \u003ch3\u003eDataset in GEM\r\n\r\n        \u003cdiv class=\"tooltip\"\u003e\r\n          \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n          \u003cdiv class=\"tooltip-text\"\u003e\r\n            \u003cul\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eRationale for Inclusion in GEM\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eGEM-Specific Curation\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eGetting Started with the Task\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n            \u003c/ul\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/h3\u003e\r\n      \u003cbutton class=\"expand-button\"\u003e\r\n        \u003csvg fill=\"#3c4f50\" height=\"24px\" viewBox=\"0 0 24 24\" width=\"24px\" xmlns=\"http://www.w3.org/2000/svg\"\u003e\r\n          \u003cpath d=\"M0 0h24v24H0z\" fill=\"none\"\u003e\u003c/path\u003e\r\n          \u003cpath d=\"M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z\"\u003e\r\n          \u003c/path\u003e\r\n        \u003c/svg\u003e\r\n      \u003c/button\u003e\r\n    \u003c/div\u003e\r\n\r\n    \u003cdiv class=\"datacard-collapsible\"\u003e\r\n\r\n\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eRationale for Inclusion in GEM\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eWhy is the Dataset in GEM?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat does this dataset contribute toward better generation evaluation and why is it part of GEM?\r\n                  \u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eCommonGen is a medium sized corpus with a unique reasoning challenge and interesting evaluation\r\n              possibilities.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eSimilar Datasets\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eDo other datasets for the high level task exist?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eno\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eAbility that the Dataset measures\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat aspect of model ability can be measured with this dataset?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eCommonsense reasoning\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eGEM-Specific Curation\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eModificatied for GEM?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eHas the GEM version of the dataset been modified in any way (data, processing, splits) from the\r\n                    original curated data?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eyes\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eGEM Modifications\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat changes have been made to he original dataset?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003e\u003ccode\u003eother\u003c/code\u003e\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eModification Details\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eFor each of these changes, described them in more details and provided the intended purpose of the\r\n                    modification\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003e4 challenge sets for CommenGen were added to the GEM evaluation suite.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eAdditional Splits?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eDoes GEM provide additional splits to the dataset?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eyes\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eSplit Information\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eDescribe how the new splits were created\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003col\u003e\r\n              \u003cli\u003eData Shift\u003c/li\u003e\r\n            \u003c/ol\u003e\r\n            \u003cp\u003eWe created subsets of the training and development sets of ~500 randomly selected inputs each.\u003c/p\u003e\r\n            \u003col start=\"2\"\u003e\r\n              \u003cli\u003eTransformations\u003c/li\u003e\r\n            \u003c/ol\u003e\r\n            \u003cp\u003eWe applied input scrambling on a subset of 500 randomly selected test instances; the order of the\r\n              concepts was randomly reassigned.\u003c/p\u003e\r\n            \u003col start=\"3\"\u003e\r\n              \u003cli\u003eSubpopulations\u003c/li\u003e\r\n            \u003c/ol\u003e\r\n            \u003cp\u003eWe created a subpopulation based on input length, taking into account the number of concepts the input\r\n              test structures. By comparing inputs of different lengths, we can see to what extent systems are able to\r\n              handle different input sizes\u003c/p\u003e\r\n\r\n            \u003cdiv class=\"table-wrapper\"\u003e\r\n              \u003cdiv class=\"toolbar\"\u003e\r\n                \u003cdiv class=\"expand-modal-icon\" title=\"Click to expand table\"\u003e\u003c/div\u003e\r\n              \u003c/div\u003e\r\n              \u003ctable\u003e\r\n                \u003cthead\u003e\r\n                  \u003ctr\u003e\r\n                    \u003cth\u003eConcept number\u003c/th\u003e\r\n                    \u003cth\u003eFrequency English\u003c/th\u003e\r\n                  \u003c/tr\u003e\r\n                \u003c/thead\u003e\r\n                \u003ctbody\u003e\r\n                  \u003ctr\u003e\r\n                    \u003ctd\u003e4\u003c/td\u003e\r\n                    \u003ctd\u003e747\u003c/td\u003e\r\n                  \u003c/tr\u003e\r\n                  \u003ctr\u003e\r\n                    \u003ctd\u003e5\u003c/td\u003e\r\n                    \u003ctd\u003e750\u003c/td\u003e\r\n                  \u003c/tr\u003e\r\n                \u003c/tbody\u003e\r\n              \u003c/table\u003e\r\n            \u003c/div\u003e\r\n\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eSplit Motivation\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat aspects of the model's generation capacities were the splits created to test?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eGeneralization and Robustness\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eGetting Started with the Task\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003ePointers to Resources\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eGetting started with in-depth research on the task. Add relevant pointers to resources that\r\n                    researchers can consult when they want to get started digging deeper into the task.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cul\u003e\r\n              \u003cli\u003eTwo variants of \u003ca href=\"https://arxiv.org/abs/1910.13461\"\u003eBART\u003c/a\u003e, \u003ca\r\n                  href=\"https://arxiv.org/abs/2009.12677\"\u003eKnowledge Graph augemnted-BART\u003c/a\u003e and \u003ca\r\n                  href=\"https://arxiv.org/abs/2012.00366\"\u003eEnhanced Knowledge Injection Model for Commonsense\r\n                  Generation\u003c/a\u003e, hold the top two spots on the leaderboard, followed by a fine-tuned \u003ca\r\n                  href=\"https://arxiv.org/abs/1910.10683\"\u003eT5 model\u003c/a\u003e.\u003c/li\u003e\r\n              \u003cli\u003eThe following script shows how to download and load the data, fine-tune, and evaluate a model using\r\n                the ROUGE, BLEU, and METEOR metrics: \u003ca\r\n                  href=\"https://github.com/GEM-benchmark/GEM-baseline-models/blob/main/examples/GEM-common_gen.ipynb\"\u003eGEM\r\n                  sample script\u003c/a\u003e.\u003c/li\u003e\r\n            \u003c/ul\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n    \u003c/div\u003e\r\n  \u003c/section\u003e\r\n\r\n  \u003csection class=\"datacard-section open\"\u003e\r\n\r\n    \u003cdiv class=\"datacard-section-preview\"\u003e\r\n      \u003ch3\u003ePrevious Results\r\n\r\n        \u003cdiv class=\"tooltip\"\u003e\r\n          \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n          \u003cdiv class=\"tooltip-text\"\u003e\r\n            \u003cul\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003ePrevious Results\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n            \u003c/ul\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/h3\u003e\r\n      \u003cbutton class=\"expand-button\"\u003e\r\n        \u003csvg fill=\"#3c4f50\" height=\"24px\" viewBox=\"0 0 24 24\" width=\"24px\" xmlns=\"http://www.w3.org/2000/svg\"\u003e\r\n          \u003cpath d=\"M0 0h24v24H0z\" fill=\"none\"\u003e\u003c/path\u003e\r\n          \u003cpath d=\"M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z\"\u003e\r\n          \u003c/path\u003e\r\n        \u003c/svg\u003e\r\n      \u003c/button\u003e\r\n    \u003c/div\u003e\r\n\r\n    \u003cdiv class=\"datacard-collapsible\"\u003e\r\n\r\n\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003ePrevious Results\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eMeasured Model Abilities\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat aspect of model ability can be measured with this dataset?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eCommonsense Reasoning\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eMetrics\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat metrics are typically used for this task?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003e\u003ccode\u003eOther: Other Metrics\u003c/code\u003e, \u003ccode\u003eBLEU\u003c/code\u003e, \u003ccode\u003eROUGE\u003c/code\u003e, \u003ccode\u003eMETEOR\u003c/code\u003e\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eOther Metrics\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eDefinitions of other metrics\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cul\u003e\r\n              \u003cli\u003eSPICE: An evaluation metric for image captioning that is defined over scene graphs\u003c/li\u003e\r\n              \u003cli\u003eCIDEr: An n-gram overlap metric based on cosine similarity between the TF-IDF weighted ngram counts\r\n              \u003c/li\u003e\r\n            \u003c/ul\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eProposed Evaluation\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eList and describe the purpose of the metrics and evaluation methodology (including human\r\n                    evaluation) that the dataset creators used when introducing this task.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe main metrics are captioning metrics since the original concept lists were extracted from captioning\r\n              datasets. A human subject study with five graduate students was conducted and they were asked to rank the\r\n              \"commonsense plausibility\" of two models at a time.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003ePrevious results available?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eAre previous results available?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eyes\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eOther Evaluation Approaches\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat evaluation approaches have others used?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe currently best performing model KFCNet (\u003ca\r\n                href=\"https://aclanthology.org/2021.findings-emnlp.249/\"\u003ehttps://aclanthology.org/2021.findings-emnlp.249/\u003c/a\u003e)\r\n              uses the same automatic evaluation but does not conduct any human evaluation.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eRelevant Previous Results\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat are the most relevant previous results for this task/dataset?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe most relevant results can be seen on the \u003ca\r\n                href=\"https://inklab.usc.edu/CommonGen/leaderboard.html\"\u003eleaderboard\u003c/a\u003e\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n    \u003c/div\u003e\r\n  \u003c/section\u003e\r\n\r\n  \u003csection class=\"datacard-section open\"\u003e\r\n\r\n    \u003cdiv class=\"datacard-section-preview\"\u003e\r\n      \u003ch3\u003eDataset Curation\r\n\r\n        \u003cdiv class=\"tooltip\"\u003e\r\n          \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n          \u003cdiv class=\"tooltip-text\"\u003e\r\n            \u003cul\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eOriginal Curation\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eLanguage Data\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eStructured Annotations\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eConsent\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003ePrivate Identifying Information (PII)\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eMaintenance\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n            \u003c/ul\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/h3\u003e\r\n      \u003cbutton class=\"expand-button\"\u003e\r\n        \u003csvg fill=\"#3c4f50\" height=\"24px\" viewBox=\"0 0 24 24\" width=\"24px\" xmlns=\"http://www.w3.org/2000/svg\"\u003e\r\n          \u003cpath d=\"M0 0h24v24H0z\" fill=\"none\"\u003e\u003c/path\u003e\r\n          \u003cpath d=\"M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z\"\u003e\r\n          \u003c/path\u003e\r\n        \u003c/svg\u003e\r\n      \u003c/button\u003e\r\n    \u003c/div\u003e\r\n\r\n    \u003cdiv class=\"datacard-collapsible\"\u003e\r\n\r\n\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eOriginal Curation\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eOriginal Curation Rationale\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eOriginal curation rationale\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe dataset creators selected sets of concepts that appeared in image and video captions (as identified\r\n              by a POS tagger) to ensure that a likely real-world scenario including the set could be imagined and\r\n              constructed. Section 3.1 of the \u003ca href=\"https://arxiv.org/pdf/1911.03705v3.pdf\"\u003epaper\u003c/a\u003e describes a\r\n              sampling scheme which encourages diversity of sets while selecting common concepts.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eCommunicative Goal\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat was the communicative goal?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe speaker is required to produce a \u003cem\u003ecoherent\u003c/em\u003e sentence which mentions all of the source\r\n              concepts, and which describes a \u003cem\u003elikely\u003c/em\u003e situation that could be captured in a picture or video.\r\n            \u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eSourced from Different Sources\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eIs the dataset aggregated from different data sources?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eyes\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eSource Details\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eList the sources (one per line)\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cul\u003e\r\n              \u003cli\u003e\u003ca href=\"https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00166\"\u003eFlickr30k\u003c/a\u003e\u003c/li\u003e\r\n              \u003cli\u003e\u003ca href=\"https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48\"\u003eMSCOCO\u003c/a\u003e\u003c/li\u003e\r\n              \u003cli\u003e\u003ca href=\"https://www.aclweb.org/anthology/P18-1238/\"\u003eConceptual Captions\u003c/a\u003e\u003c/li\u003e\r\n              \u003cli\u003eVideo captioning datasets:\r\n                \u003cul\u003e\r\n                  \u003cli\u003e\u003ca href=\"https://link.springer.com/article/10.1007/s11263-016-0987-1\"\u003eLSMDC\u003c/a\u003e\u003c/li\u003e\r\n                  \u003cli\u003e\u003ca\r\n                      href=\"https://openaccess.thecvf.com/content_iccv_2017/html/Krishna_Dense-Captioning_Events_in_ICCV_2017_paper.html\"\u003eActivityNet\u003c/a\u003e\r\n                  \u003c/li\u003e\r\n                  \u003cli\u003e\u003ca\r\n                      href=\"https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_VaTeX_A_Large-Scale_High-Quality_Multilingual_Dataset_for_Video-and-Language_Research_ICCV_2019_paper.html\"\u003eVaTeX\u003c/a\u003e\r\n                  \u003c/li\u003e\r\n                \u003c/ul\u003e\r\n              \u003c/li\u003e\r\n            \u003c/ul\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eLanguage Data\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eHow was Language Data Obtained?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eHow was the language data obtained?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003e\u003ccode\u003eCrowdsourced\u003c/code\u003e\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eWhere was it crowdsourced?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eIf crowdsourced, where from?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003e\u003ccode\u003eAmazon Mechanical Turk\u003c/code\u003e\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eLanguage Producers\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat further information do we have on the language producers?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe training data consists of concept sets and captions for the source datasets. The concept sets are the\r\n              sets of labels of the images or videos, selected with a heuristic to maximize diversity while ensuring\r\n              that they represent likely scenarios.\u003c/p\u003e\r\n            \u003cp\u003eThe dev and test set sentences were created by Amazon Mechanical Turk crowd workers. The workers were\r\n              shown an example generation and a set of 4 or 5 concept names along with their part-of-speech and asked to\r\n              write:\u003c/p\u003e\r\n            \u003col\u003e\r\n              \u003cli\u003eOne sentence mentioning all of the concepts\u003c/li\u003e\r\n              \u003cli\u003eA rationale explaining how the sentence connects the concept\u003c/li\u003e\r\n            \u003c/ol\u003e\r\n            \u003cp\u003eA screenshot of the interface is provided in Figure 7 of the \u003ca\r\n                href=\"https://arxiv.org/pdf/1911.03705v3.pdf\"\u003eAppendix\u003c/a\u003e.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eTopics Covered\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eDoes the language in the dataset focus on specific topics? How would you describe them?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eInformation was not provided.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eData Validation\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWas the text validated by a different worker or a data curator?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003evalidated by data curator\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eWas Data Filtered?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWere text instances selected or filtered?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003ealgorithmically\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eFilter Criteria\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat were the selection criteria?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eDuring the data collection, workers who provided rationales that were too short, failed to have good\r\n              coverage of the input in their sentences, or workers whose output had a high perplexity under a GPT-2\r\n              model were disqualified from the pool and replaced with newcomers.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eStructured Annotations\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eAdditional Annotations?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eDoes the dataset have additional annotations for each instance?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003enone\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eAnnotation Service?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWas an annotation service used?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eno\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eConsent\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eAny Consent Policy?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWas there a consent policy involved when gathering the data?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eno\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eJustification for Using the Data\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eIf not, what is the justification for reusing the data?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe data was sourced from Mechanical Turk which means that raters were aware that their annotations may\r\n              be publicly released for research purposes.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003ePrivate Identifying Information (PII)\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eContains PII?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eDoes the source language data likely contain Personal Identifying Information about the data\r\n                    creators or subjects?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eno PII\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eJustification for no PII\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eProvide a justification for selecting \u003ccode\u003eno PII\u003c/code\u003e above.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe concepts are restricted to verbs, adjectives, and common nouns, and no personal information is given\r\n              in the captions.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eMaintenance\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eAny Maintenance Plan?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eDoes the original dataset have a maintenance plan?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eno\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n    \u003c/div\u003e\r\n  \u003c/section\u003e\r\n\r\n  \u003csection class=\"datacard-section open\"\u003e\r\n\r\n    \u003cdiv class=\"datacard-section-preview\"\u003e\r\n      \u003ch3\u003eBroader Social Context\r\n\r\n        \u003cdiv class=\"tooltip\"\u003e\r\n          \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n          \u003cdiv class=\"tooltip-text\"\u003e\r\n            \u003cul\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003ePrevious Work on the Social Impact of the Dataset\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eImpact on Under-Served Communities\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eDiscussion of Biases\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n            \u003c/ul\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/h3\u003e\r\n      \u003cbutton class=\"expand-button\"\u003e\r\n        \u003csvg fill=\"#3c4f50\" height=\"24px\" viewBox=\"0 0 24 24\" width=\"24px\" xmlns=\"http://www.w3.org/2000/svg\"\u003e\r\n          \u003cpath d=\"M0 0h24v24H0z\" fill=\"none\"\u003e\u003c/path\u003e\r\n          \u003cpath d=\"M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z\"\u003e\r\n          \u003c/path\u003e\r\n        \u003c/svg\u003e\r\n      \u003c/button\u003e\r\n    \u003c/div\u003e\r\n\r\n    \u003cdiv class=\"datacard-collapsible\"\u003e\r\n\r\n\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003ePrevious Work on the Social Impact of the Dataset\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eUsage of Models based on the Data\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eAre you aware of cases where models trained on the task featured in this dataset ore related tasks\r\n                    have been used in automated systems?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eno\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eImpact on Under-Served Communities\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eAddresses needs of underserved Communities?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eDoes this dataset address the needs of communities that are traditionally underserved in language\r\n                    technology, and particularly language generation technology? Communities may be underserved for\r\n                    exemple because their language, language variety, or social or geographical context is\r\n                    underepresented in NLP and NLG resources (datasets and models).\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eno\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eDiscussion of Biases\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field telescope\"\u003e\r\n\r\n            \u003ch5\u003eAny Documented Social Biases?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eAre there documented social biases in the dataset? Biases in this context are variations in the\r\n                    ways members of different social categories are represented that can have harmful downstream\r\n                    consequences for members of the more disadvantaged group.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eno\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eAre the Language Producers Representative of the Language?\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eDoes the distribution of language producers in the dataset accurately represent the full\r\n                    distribution of speakers of the language world-wide? If not, how does it differ?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe dataset is created using data from image captioning systems and might inherit some of the social\r\n              biases represented therein (see e.g. \u003ca href=\"https://arxiv.org/abs/2006.08315\"\u003eTang et al. 2020\u003c/a\u003e).\u003c/p\u003e\r\n            \u003cp\u003eAnother related concern is the exposure bias introduced by the initial selection of pictures and video,\r\n              which are likely to over-represent situations that are common in the US at the expense of other parts of\r\n              the world (Flickr, for example, is a US-based company founded in Canada). For more discussion of the\r\n              potential impacts of exposure bias, see e.g. \u003ca href=\"https://www.aclweb.org/anthology/P16-2096.pdf\"\u003eThe\r\n                Social Impact of Natural Language Processing\u003c/a\u003e.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n    \u003c/div\u003e\r\n  \u003c/section\u003e\r\n\r\n  \u003csection class=\"datacard-section open\"\u003e\r\n\r\n    \u003cdiv class=\"datacard-section-preview\"\u003e\r\n      \u003ch3\u003eConsiderations for Using the Data\r\n\r\n        \u003cdiv class=\"tooltip\"\u003e\r\n          \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n          \u003cdiv class=\"tooltip-text\"\u003e\r\n            \u003cul\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003ePII Risks and Liability\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eLicenses\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n              \u003cli\u003e\r\n                \u003ch4\u003eKnown Technical Limitations\u003c/h4\u003e\r\n              \u003c/li\u003e\r\n            \u003c/ul\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/h3\u003e\r\n      \u003cbutton class=\"expand-button\"\u003e\r\n        \u003csvg fill=\"#3c4f50\" height=\"24px\" viewBox=\"0 0 24 24\" width=\"24px\" xmlns=\"http://www.w3.org/2000/svg\"\u003e\r\n          \u003cpath d=\"M0 0h24v24H0z\" fill=\"none\"\u003e\u003c/path\u003e\r\n          \u003cpath d=\"M16.59 8.59L12 13.17 7.41 8.59 6 10l6 6 6-6z\"\u003e\r\n          \u003c/path\u003e\r\n        \u003c/svg\u003e\r\n      \u003c/button\u003e\r\n    \u003c/div\u003e\r\n\r\n    \u003cdiv class=\"datacard-collapsible\"\u003e\r\n\r\n\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003ePII Risks and Liability\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003ePotential PII Risk\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eConsidering your answers to the PII part of the Data Curation Section, describe any potential\r\n                    privacy to the data subjects and creators risks when using the dataset.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe concepts are restricted to verbs, adjectives, and common nouns, and no personal information is given\r\n              in the captions.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eLicenses\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eCopyright Restrictions on the Dataset\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eBased on your answers in the Intended Use part of the Data Overview Section, which of the following\r\n                    best describe the copyright and licensing status of the dataset?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003e\u003ccode\u003eopen license - commercial use allowed\u003c/code\u003e\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field periscope\"\u003e\r\n\r\n            \u003ch5\u003eCopyright Restrictions on the Language Data\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eBased on your answers in the Language part of the Data Curation Section, which of the following\r\n                    best describe the copyright and licensing status of the underlying language data?\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003e\u003ccode\u003eopen license - commercial use allowed\u003c/code\u003e\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n\r\n      \u003cdiv class=\"datacard-subsection\"\u003e\r\n        \u003ch4\u003eKnown Technical Limitations\u003c/h4\u003e\r\n\r\n\r\n        \u003cdiv class=\"datacard-field-wrapper\"\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eTechnical Limitations\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eDescribe any known technical limitations, such as spurrious correlations, train/test overlap,\r\n                    annotation biases, or mis-annotations, and cite the works that first identified these limitations\r\n                    when possible.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eThe dataset is in English, a language with an abundance of existing resources.\u003c/p\u003e\r\n            \u003cp\u003eThe use of GPT-2 to validate development ant test sentences \u003ca\r\n                href=\"https://www.aclweb.org/anthology/D19-1339.pdf\"\u003emight be cause for similar concern\u003c/a\u003e, but we do\r\n              note that the authors only use the model to discount very high perplexity sequences which is less likely\r\n              to surface those biases.\u003c/p\u003e\r\n            \u003cp\u003eThe language in the development and test set is crowdsourced, which means that it was written by workers\r\n              whose main goal was speed. This is likely to impact the quality and variety of the targets. The population\r\n              of crowdsource workers is also not identically distributed as the the base population of the locations the\r\n              workers come from, which may lead to different representation of situations or underlying expectations of\r\n              what these situations are.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eUnsuited Applications\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhen using a model trained on this dataset in a setting where users or the public may interact with\r\n                    its predictions, what are some pitfalls to look out for? In particular, describe some applications\r\n                    of the general task featured in this dataset that its curation or properties make it less suitable\r\n                    for.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eDue to the overrepresentation of US-situations, the system may not work for users across the world.\r\n              Moreover, only limited information on the dataset quality are provided and the system may fail as a result\r\n              of unknown issues.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n\r\n          \u003cdiv class=\"datacard-field microscope\"\u003e\r\n\r\n            \u003ch5\u003eDiscouraged Use Cases\r\n\r\n              \u003cdiv class=\"tooltip\"\u003e\r\n                \u003cdiv class=\"tooltip-icon\"\u003e\u003c/div\u003e\r\n                \u003cdiv class=\"tooltip-text\"\u003e\r\n                  \u003cp\u003eWhat are some discouraged use cases of a model trained to maximize the proposed metrics on this\r\n                    dataset? In particular, think about settings where decisions made by a model that performs\r\n                    reasonably well on the metric my still have strong negative consequences for user or members of the\r\n                    public.\u003c/p\u003e\r\n                \u003c/div\u003e\r\n              \u003c/div\u003e\r\n\r\n            \u003c/h5\u003e\r\n\r\n            \u003cp\u003eAny system needs to be evaluated on a broader set of unseen concepts then provided in the dataset. Since\r\n              the references for the test set are private, it is not known how well findings generalize beyond the\r\n              collection methodology.\u003c/p\u003e\r\n          \u003c/div\u003e\r\n        \u003c/div\u003e\r\n\r\n      \u003c/div\u003e\r\n    \u003c/div\u003e\r\n  \u003c/section\u003e\r\n\u003c/div\u003e","title":"common_gen","type":"Reasoning","languages":"English","summary":"CommonGen is an English text generation task to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts, the task is to generate a coherent sentence describing an everyday scenario using these concepts. CommonGen is challenging because it inherently requires 1) relational reasoning using background commonsense knowledge, and 2) compositional generalization ability to work on unseen concept combinations. The dataset, constructed through a combination of crowd-sourcing from AMT and existing caption corpora, consists of 30k concept-sets and 50k sentences in total. Note that the CommonGen test set is private and requires submission to the external leaderboard."}},"__N_SSG":true},"page":"/data_cards/[id]","query":{"id":"common_gen"},"buildId":"456T-gjsiPfCS_Gwi0APa","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>