<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="Benchmark natural language generation systems with GEM."/><meta property="og:image" content="https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"/><meta name="og:title" content="GEM"/><meta name="twitter:card" content="summary_large_image"/><title>GEM WebNLG</title><link rel="preload" href="/_next/static/css/2786522978a02f025205.css" as="style"/><link rel="stylesheet" href="/_next/static/css/2786522978a02f025205.css" data-n-g=""/><link rel="preload" href="/_next/static/css/f2fce7b83fe6ca04479b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f2fce7b83fe6ca04479b.css" data-n-p=""/><noscript data-n-css="true"></noscript><link rel="preload" href="/_next/static/chunks/main-47bc8f80085b54a800da.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.baa41d4dbf5d52db897c.js" as="script"/><link rel="preload" href="/_next/static/chunks/e70fad557dfa42f32a11d0d2c99fe8f6e8d1fa86.4a36a385313236c59b19.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-a9ae7a6d1de4e51a7ab6.js" as="script"/><link rel="preload" href="/_next/static/chunks/cb1608f2.c3a9f0eb95374ca4919a.js" as="script"/><link rel="preload" href="/_next/static/chunks/451c6be158cef50d8cc28b919cf08d1e5b9ff3fc.f0ec181e43727e8a893e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/data_cards/%5Bid%5D-e1361c13d80af81deda6.js" as="script"/></head><body><div id="__next"><div class="layout_background__1AVEa undefined"><header class="layout_header__2rhWq"><div class="navbar_navwrapper__15zia"><div class="navbar_gradbar__1Xi5u"></div><nav class="navbar_navbar__3gnco"><span class="utils_headingLg__de7p0 navbar_navbarlogo__PLEwr"><a href="/">GEM BENCHMARK</a></span><div class="navbar_menutoggle__358pJ" id="mobile-menu"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars fa-w-14 navbar_bar__QVPSR" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"></path></svg></div><ul><li class="navbar_navitem__3ICSG navbar_pushright__3G2DM"><a href="/resources">Resources</a></li><li class="navbar_navitem__3ICSG"><a href="/data_cards">Data Cards</a></li><li class="navbar_navitem__3ICSG"><a href="/model_cards">Model Cards</a></li><li class="navbar_navitem__3ICSG"><a href="/tutorials">tutorials</a></li><li class="navbar_navitem__3ICSG"><a href="/results">Results</a></li><li class="navbar_navitem__3ICSG"><a href="/papers">Papers</a></li><li class="navbar_navitem__3ICSG"><a href="/team">Team</a></li><li class="navbar_navitem__3ICSG"><a href="/nl_augmenter">NL-Augmenter</a></li><li class="navbar_navitem__3ICSG"><a href="/workshop">Workshop</a></li></ul></nav></div></header><div class="layout_container__2t4v2"><main><article><span class="utils_headingXl__1XecN">WebNLG</span><span class="utils_smallSpace__375iy"></span><span class="utils_lightText__12Ckm">Structure-to-text</span><div><h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#dataset-description">Dataset Description</a>
<ul>
<li><a href="#dataset-and-task-summary">Dataset and Task Summary</a></li>
<li><a href="#why-is-this-dataset-part-of-gem">Why is this dataset part of GEM?</a></li>
<li><a href="#languages">Languages</a></li>
</ul>
</li>
<li><a href="#meta-information">Meta Information</a>
<ul>
<li><a href="#dataset-curators">Dataset Curators</a></li>
<li><a href="#licensing-information">Licensing Information</a></li>
<li><a href="#citation-information">Citation Information</a></li>
<li><a href="#leaderboard">Leaderboard</a></li>
</ul>
</li>
<li><a href="#dataset-structure">Dataset Structure</a>
<ul>
<li><a href="#data-instances">Data Instances</a></li>
<li><a href="#data-fields">Data Fields</a></li>
<li><a href="#data-statistics">Data Statistics</a></li>
</ul>
</li>
<li><a href="#dataset-creation">Dataset Creation</a>
<ul>
<li><a href="#curation-rationale">Curation Rationale</a></li>
<li><a href="#communicative-goal">Communicative Goal</a></li>
<li><a href="#source-data">Source Data</a>
<ul>
<li><a href="#initial-data-collection-and-normalization">Initial Data Collection and Normalization</a></li>
<li><a href="#who-are-the-source-language-producers">Who are the source language producers?</a></li>
</ul>
</li>
<li><a href="#annotations">Annotations</a>
<ul>
<li><a href="#annotation-process">Annotation process</a></li>
<li><a href="#who-are-the-annotators">Who are the annotators?</a></li>
</ul>
</li>
<li><a href="#personal-and-sensitive-information">Personal and Sensitive Information</a></li>
</ul>
</li>
<li><a href="#changes-to-the-original-dataset-for-gem">Changes to the Original Dataset for GEM</a>
<ul>
<li><a href="#special-test-sets">Special test sets</a>
<ul>
<li><a href="#data-shift">Data shift</a></li>
<li><a href="#transformations">Transformations</a></li>
<li><a href="#subpopulations">Subpopulations</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#considerations-for-using-the-data">Considerations for Using the Data</a>
<ul>
<li><a href="#social-impact-of-the-dataset">Social Impact of the Dataset</a></li>
<li><a href="#impact-on-underserved-communities">Impact on Underserved Communities</a></li>
<li><a href="#discussion-of-biases">Discussion of Biases</a></li>
<li><a href="#other-known-limitations">Other Known Limitations</a></li>
</ul>
</li>
<li><a href="#getting-started-with-in-depth-research-on-the-task">Getting started with in-depth research on the task</a></li>
</ul>
<h2 id="dataset-description">Dataset Description</h2>
<ul>
<li><strong>Homepage:</strong> <a href="https://webnlg-challenge.loria.fr/">https://webnlg-challenge.loria.fr/</a></li>
<li><strong>Repository:</strong> <a href="https://gitlab.com/shimorina/webnlg-dataset">https://gitlab.com/shimorina/webnlg-dataset</a></li>
<li><strong>Paper:</strong> <a href="http://www.aclweb.org/anthology/P17-1017">First Dataset Release</a>, <a href="https://www.aclweb.org/anthology/W17-3518/">WebNLG Challenge 2017 Report</a>, <a href="https://webnlg-challenge.loria.fr/files/2020.webnlg-papers.7.pdf">WebNLG Challenge 2020 Report</a></li>
<li><strong>Point of Contact:</strong> <a href="mailto:webnlg-challenge@inria.fr">webnlg-challenge@inria.fr</a></li>
</ul>
<h3 id="dataset-and-task-summary">Dataset and Task Summary</h3>
<p>WebNLG is a bi-lingual dataset (English, Russian) of parallel DBpedia triple sets and short texts that cover about 450 different DBpedia properties. The WebNLG data was originally created to promote the development of RDF verbalisers able to generate short text and to handle micro-planning (i.e., sentence segmentation and ordering, referring expression generation, aggregation); the goal of the task is to generate texts starting from 1 to 7 input triples which have entities in common (so the input is actually a connected Knowledge Graph). The dataset contains about 17,000 triple sets and 45,000 crowdsourced texts in English, and 7,000 triples sets and 19,000 crowdsourced texts in Russian. A challenging test set section with entities and/or properties that have not been seen at training time is available.</p>
<h3 id="why-is-this-dataset-part-of-gem">Why is this dataset part of GEM?</h3>
<p>The WebNLG 2020 dataset (version 3.0) is a large bi-lingual dataset with crowdsourced reference texts and a rather large variety of knowledge in the inputs.</p>
<h3 id="languages">Languages</h3>
<p>English (<code>en</code>, all dataset versions), Russian (<code>ru</code>, from the version 3.0 on).</p>
<h2 id="meta-information">Meta Information</h2>
<h3 id="dataset-curators">Dataset Curators</h3>
<p>The principle curator of the dataset is Anastasia Shimorina (Universit√© de Lorraine / LORIA, France). Throughout the WebNLG releases, several people contributed to their construction: Claire Gardent (CNRS / LORIA, France), Shashi Narayan (Google, UK), Laura Perez-Beltrachini (University of Edinburgh, UK), Elena Khasanova, and Thiago Castro Ferreira (Federal University of Minas Gerais, Brazil).
The dataset construction was funded by the French National Research Agency (ANR).</p>
<h3 id="licensing-information">Licensing Information</h3>
<p><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC Attribution-Noncommercial-Share Alike 4.0 International</a>.</p>
<h3 id="citation-information">Citation Information</h3>
<p>Initial release of the dataset:</p>
<pre><code>@inproceedings{gardent2017creating,
  author = 	"Gardent, Claire
		and Shimorina, Anastasia
		and Narayan, Shashi
		and Perez-Beltrachini, Laura",
  title = 	"Creating Training Corpora for NLG Micro-Planners",
  booktitle = 	"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"179--188",
  location = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/P17-1017",
  url = 	"http://www.aclweb.org/anthology/P17-1017"
}
</code></pre>
<p>The latest version 3.0:</p>
<pre><code>@inproceedings{castro-ferreira20:bilin-bi-direc-webnl-shared,
  title={The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task Overview and Evaluation Results (WebNLG+ 2020)},
  author={Castro Ferreira, Thiago and
                  Gardent, Claire and
		  Ilinykh, Nikolai and
		  van der Lee, Chris and
		  Mille, Simon and
		  Moussallem, Diego and
		  Shimorina, Anastasia},
  booktitle = {Proceedings of the 3rd WebNLG Workshop on Natural Language Generation from the Semantic Web (WebNLG+ 2020)},
    pages = "55--76",
  year = 	 2020,
  address = 	 {Dublin, Ireland (Virtual)},
  publisher = {Association for Computational Linguistics}}
</code></pre>
<h3 id="leaderboard">Leaderboard</h3>
<p>The dataset supports an active leaderboard, the best results are tracked here: <a href="https://beng.dice-research.org/gerbil/">https://beng.dice-research.org/gerbil/</a>. The model outputs are evaluated against the crowdsourced references; the leaderboard reports BLEU-4, METEOR, chrF++, TER, BERTScore and BLEURT scores.</p>
<h2 id="dataset-structure">Dataset Structure</h2>
<h3 id="data-instances">Data Instances</h3>
<p>English instance:</p>
<pre><code>{
"entry": {
	"category": "Company",
	"size": "4",
	"shape": "(X (X) (X) (X) (X))",
	"shape_type": "sibling",
	"eid": "Id21",
	"lexs": [
	    {
		"comment": "good",
		"lex": "Trane, which was founded on January 1st 1913 in La Crosse, Wisconsin, is based in Ireland. It has 29,000 employees.",
		"lid": "Id1"
	    }
	],
	"modifiedtripleset": [
	    {
		"subject": "Trane",
		"property": "foundingDate",
		"object": "1913-01-01"
	    },
	    {
		"subject": "Trane",
		"property": "location",
		"object": "Ireland"
	    },
	    {
		"subject": "Trane",
		"property": "foundationPlace",
		"object": "La_Crosse,_Wisconsin"
	    },
	    {
		"subject": "Trane",
		"property": "numberOfEmployees",
		"object": "29000"
	    }

	],
	"originaltriplesets": {
	    "originaltripleset": [
		    {
			"subject": "Trane",
			"property": "foundingDate",
			"object": "1913-01-01"
		    },
		    {
			"subject": "Trane",
			"property": "location",
			"object": "Ireland"
		    },
		    {
			"subject": "Trane",
			"property": "foundationPlace",
			"object": "La_Crosse,_Wisconsin"
		    },
		    {
			"subject": "Trane",
			"property": "numberOfEmployees",
			"object": "29000"
		    }
	    ]
	}

	}
}
</code></pre>
<p>The XML-formatted example is <a href="https://webnlg-challenge.loria.fr/docs/#example">here</a>.</p>
<p>Russian instance:</p>
<pre><code>{
"entry": {
	"category": "Building",
	"size": "3",
	"shape": "(X (X (X (X))))",
	"shape_type": "chain",
	"eid": "Id2",
	"lexs": [
	    {
		"lang": "en",
		"lex": "Andrew Mitchell is a leader in Birmingham where the architect John Madin who designed 103 Colmore Row was born.",
		"lid": "Id2"
	    },
	    {
		"lang": "ru",
		"lex": "–≠–Ω–¥—Ä—é –ú–∏—Ç—á–µ–ª–ª - –ª–∏–¥–µ—Ä –≤ –ë–∏—Ä–º–∏–Ω–≥–µ–º–µ, –≥–¥–µ —Ä–æ–¥–∏–ª—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä –î–∂–æ–Ω –ú–∞–¥–∏–Ω, –∫–æ—Ç–æ—Ä—ã–π —Å–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–ª –ö–æ–ª–º–æ—Ä-—Ä–æ—É 103.",
		"lid": "Id2"
	    }
	],
	"modifiedtripleset": [
		    {
			"subject": "103_Colmore_Row",
			"property": "architect",
			"object": "John_Madin"
		    },
		    {
			"subject": "John_Madin",
			"property": "birthPlace",
			"object": "Birmingham"
		    },
		    {
			"subject": "Birmingham",
			"property": "leaderName",
			"object": "Andrew_Mitchell"
		    }

	],
	"originaltriplesets": {
	    "originaltripleset": [
		    {
			"subject": "103_Colmore_Row",
			"property": "architect",
			"object": "John_Madin"
		    },
		    {
			"subject": "John_Madin",
			"property": "birthPlace",
			"object": "Birmingham"
		    },
		    {
			"subject": "Birmingham",
			"property": "leaderName",
			"object": "Andrew_Mitchell"
		    }
	    ]
	},

	"dbpedialinks" : [
		    {
			"subject": "Birmingham",
			"property": "sameAs",
			"object": "–ë–∏—Ä–º–∏–Ω–≥–µ–º"
		    }
	],

	"links" : [
		    {
			"subject": "Andrew Mitchell",
			"property": "sameAs",
			"object": "–≠–Ω–¥—Ä—é –ú–∏—Ç—á–µ–ª–ª (–º—É–∂)"
		    },
		    {
			"subject": "John Madin",
			"property": "sameAs",
			"object": "–î–∂–æ–Ω –ú–∞–¥–∏–Ω (–º—É–∂)"
		    },
		    {
			"subject": "103 Colmore Row",
			"property": "sameAs",
			"object": "–ö–æ–ª–º–æ—Ä-—Ä–æ—É 103"
		    }
	]

	}
}
</code></pre>
<h3 id="data-fields">Data Fields</h3>
<p>See <a href="https://webnlg-challenge.loria.fr/docs/">official documentation</a>.</p>
<p><code>entry</code>: a data instance of the benchmark. Each entry has five attributes: a DBpedia category (<code>category</code>), entry ID (<code>eid</code>), shape, shape type, and triple set size (<code>size</code>).</p>
<ul>
<li>
<p><code>shape</code>: a string representation of the RDF tree with nested parentheses where <code>X</code> is a node (see <a href="https://en.wikipedia.org/wiki/Newick_format">Newick tree format</a>).</p>
</li>
<li>
<p><code>shape_type</code>: a type of the tree shape. We <a href="https://www.aclweb.org/anthology/C16-1141.pdf">identify</a> three types of tree shapes:</p>
<ul>
<li><code>chain</code> (the object of one triple is the subject of the other);</li>
<li><code>sibling</code> (triples with a shared subject);</li>
<li><code>mixed</code> (both <code>chain</code> and <code>sibling</code> types present).</li>
</ul>
</li>
<li>
<p><code>eid</code>: an entry ID. It is unique only within a category and a size.</p>
</li>
<li>
<p><code>category</code>: a DBpedia category (Astronaut, City, MusicalWork, Politician, etc.).</p>
</li>
<li>
<p><code>size</code>: the number of RDF triples in a set. Ranges from 1 to 7.</p>
</li>
</ul>
<p>Each <code>entry</code> has three fields: <code>originaltripleset</code>, <code>modifiedtripleset</code>, and <code>lexs</code>.</p>
<p><code>originaltripleset</code>: a set of RDF triples as extracted from <a href="https://wiki.dbpedia.org/">DBpedia</a>. Each set of RDF triples is a tree. Triples have the subject-predicate-object structure.</p>
<p><code>modifiedtripleset</code>: a set of RDF triples as presented to crowdworkers (for more details on modifications, see below).</p>
<p>Original and modified triples serve different purposes: the original triples ‚Äî to link data to a knowledge base (DBpedia), whereas the modified triples ‚Äî to ensure consistency and homogeneity throughout the data. To train models, the modified triples should be used.</p>
<p><code>lexs</code> (shortened for lexicalisations): a natural language text verbalising the triples. Each lexicalisation has two attributes: a comment (<code>comment</code>), and a lexicalisation ID (<code>lid</code>). By default, comments have the value <code>good</code>, except rare cases when they were manually marked as <code>toFix</code>. That was done during the corpus creation, when it was seen that a lexicalisation did not exactly match a triple set.</p>
<p>Russian data has additional optional fields comparing to English:</p>
<p><code>&#x3C;dbpedialinks></code>: RDF triples extracted from DBpedia between English and Russian entities by means of the property <code>sameAs</code>.</p>
<p><code>&#x3C;links></code>: RDF triples created manually for some entities to serve as pointers to translators. There are two types of them:</p>
<ul>
<li>
<p>with <code>sameAs</code> (<code>Spaniards | sameAs | –∏—Å–ø–∞–Ω—Ü—ã</code>)</p>
</li>
<li>
<p>with <code>includes</code> (<code>Tomatoes, guanciale, cheese, olive oil | includes | –≥—É–∞–Ω—á–∏–∞–ª–µ</code>). Those were mostly created for string literals to translate some parts of them.</p>
</li>
</ul>
<p>Lexicalisations in the Russian WebNLG have a new parameter <code>lang</code> (values: <code>en</code>, <code>ru</code>) because original English texts were kept in the Russian version (see the example above).</p>
<h3 id="data-statistics">Data Statistics</h3>
<table>
<thead>
<tr>
<th>English (v3.0)</th>
<th>Train</th>
<th>Dev</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>triple sets</strong></td>
<td>13,211</td>
<td>1,667</td>
<td>1,779</td>
</tr>
<tr>
<td><strong>texts</strong></td>
<td>35,426</td>
<td>4,464</td>
<td>5,150</td>
</tr>
<tr>
<td><strong>properties</strong></td>
<td>372</td>
<td>290</td>
<td>220</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Russian (v3.0)</th>
<th>Train</th>
<th>Dev</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>triple sets</strong></td>
<td>5,573</td>
<td>790</td>
<td>1,102</td>
</tr>
<tr>
<td><strong>texts</strong></td>
<td>14,239</td>
<td>2,026</td>
<td>2,780</td>
</tr>
<tr>
<td><strong>properties</strong></td>
<td>226</td>
<td>115</td>
<td>192</td>
</tr>
</tbody>
</table>
<h2 id="dataset-creation">Dataset Creation</h2>
<h3 id="curation-rationale">Curation Rationale</h3>
<p>The WebNLG dataset was created to promote the development (<em>i</em>) of RDF verbalisers and (<em>ii</em>) of microplanners able to handle a wide range of linguistic constructions. The dataset aims at covering knowledge in different domains ("categories"). The same properties and entities can appear in several categories.</p>
<h3 id="communicative-goal">Communicative Goal</h3>
<p>The systems are required to produce one text (one or more sentences) that verbalises all and only the input triples in a grammatical and natural way.</p>
<h3 id="source-data">Source Data</h3>
<p>The data was compiled from raw DBpedia triples. <a href="https://www.aclweb.org/anthology/C16-1141/">This paper</a> explains how the triples were selected.</p>
<h4 id="initial-data-collection-and-normalization">Initial Data Collection and Normalization</h4>
<p>Initial triples extracted from DBpedia were modified in several ways. See <a href="https://webnlg-challenge.loria.fr/docs/">official documentation</a> for the most frequent changes that have been made. An original tripleset and a modified tripleset usually represent a one-to-one mapping. However, there are cases with many-to-one mappings when several original triplesets are mapped to one modified tripleset.</p>
<p>Entities that served as roots of RDF trees are listed in <a href="https://gitlab.com/shimorina/webnlg-dataset/-/blob/master/supplementary/entities_dict.json">this file</a>.</p>
<p>The English WebNLG 2020 dataset (v3.0) for training comprises data-text pairs for 16 distinct DBpedia categories:</p>
<ul>
<li>The 10 seen categories used in the 2017 version: Airport, Astronaut, Building, City, ComicsCharacter, Food, Monument, SportsTeam, University, and WrittenWork.</li>
<li>The 5 unseen categories of 2017, which are now part of the seen data: Athlete, Artist, CelestialBody, MeanOfTransportation, Politician.</li>
<li>1 new category: Company.</li>
</ul>
<p>The Russian dataset (v3.0) comprises data-text pairs for 9 distinct categories: Airport, Astronaut, Building, CelestialBody, ComicsCharacter, Food, Monument, SportsTeam, and University.</p>
<h4 id="who-are-the-source-language-producers">Who are the source language producers?</h4>
<p>There are no source texts, all textual material was compiled during the annotation process.</p>
<h3 id="annotations">Annotations</h3>
<h4 id="annotation-process">Annotation process</h4>
<p>Annotators were first asked to create sentences that verbalise single triples. In a second round, annotators were asked to combine single-triple sentences together into sentences that cover 2 triples. And so on until 7 triples. Quality checks were performed to ensure the quality of the annotations. See Section 3.3 in <a href="https://www.aclweb.org/anthology/P17-1017.pdf">the dataset paper</a>.</p>
<p>Russian data was translated from English with an MT system and then was post-edited by crowdworkers. See Section 2.2 of <a href="https://webnlg-challenge.loria.fr/files/2020.webnlg-papers.7.pdf">this paper</a>.</p>
<h4 id="who-are-the-annotators">Who are the annotators?</h4>
<p>All references were collected through crowdsourcing platforms (CrowdFlower/Figure 8 and Amazon Mechanical Turk). For Russian, post-editing was done using the Yandex.Toloka crowdsourcing platform.</p>
<h3 id="personal-and-sensitive-information">Personal and Sensitive Information</h3>
<p>Neither the dataset as published or the annotation process involves the collection or sharing of any kind of personal / demographic information.</p>
<h2 id="changes-to-the-original-dataset-for-gem">Changes to the Original Dataset for GEM</h2>
<p>No changes to the main content of the dataset. The <a href="https://gitlab.com/shimorina/webnlg-dataset/-/tree/master/release_v3.0">version 3.0</a> of the dataset is used.</p>
<h3 id="special-test-sets">Special test sets</h3>
<p>23 special test sets for WebNLG were added to the GEM evaluation suite, 12 for English and 11 for Russian.</p>
<h4 id="data-shift">Data shift</h4>
<p>For both languages, we created subsets of the training and development sets of ~500 randomly selected inputs each. The inputs were sampled proportionnally from each category.</p>
<h4 id="transformations">Transformations</h4>
<p>Two types of transformations have been applied to WebNLG: (i) input scrambling (English and Russian) and (ii) numerical value replacements (English); in both cases, a subset of about 500 inputs was randomly selected. For (i), the order of the triples was randomly reassigned (each triple kept the same Subject-Property-Object internal order). For (ii), the change was performed respecting the format of the current cardinal value (e.g., alpha, integer, or floating-point) and replacing it with a new random value. The new number is lower-bounded between zero and upper bounded to be within to the highest power of 10 unit for the given value (e.g., replacing 54 would result in a random value between 0-100). Floating values maintain the degree of precision.</p>
<h4 id="subpopulations">Subpopulations</h4>
<p>For both languages, we did identify different subsets of the test set that we could compare to each other so that we would have a better understanding of the results. There are currently 8 selections that we have made:</p>
<p>Selection 1 (size): input length. This selection corresponds to the number of predicates in the input. By comparing inputs of different lengths, we can see to what extent NLG systems are able to handle different input sizes.  The table below provides the relevant frequencies. Please be aware that comparing selections with fewer than 100 items may result in unreliable comparisons.</p>
<table>
<thead>
<tr>
<th>Input length</th>
<th>Frequency English</th>
<th>Frequency Russian</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>369</td>
<td>254</td>
</tr>
<tr>
<td>2</td>
<td>349</td>
<td>200</td>
</tr>
<tr>
<td>3</td>
<td>350</td>
<td>214</td>
</tr>
<tr>
<td>4</td>
<td>305</td>
<td>214</td>
</tr>
<tr>
<td>5</td>
<td>213</td>
<td>159</td>
</tr>
<tr>
<td>6</td>
<td>114</td>
<td>32</td>
</tr>
<tr>
<td>7</td>
<td>79</td>
<td>29</td>
</tr>
</tbody>
</table>
<p>Selection 2 (frequency): seen/unseen single predicates. This selection corresponds to the inputs with only one predicate. We compare which predicates are seen/unseen in the training data. The table below provides the relevant frequencies. Note that the comparison is only valid for English. Not for Russian, since there is only one example of unseen single predicates.</p>
<table>
<thead>
<tr>
<th>_ in training</th>
<th>Frequency English</th>
<th>Frequency Russian</th>
</tr>
</thead>
<tbody>
<tr>
<td>Seen</td>
<td>297</td>
<td>253</td>
</tr>
<tr>
<td>Unseen</td>
<td>72</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Selection 3 (frequency): seen/unseen combinations of predicates. This selection checks for all combinations of predicates whether that combination has been seen in the training data. For example: if the combination of predicates A and B is seen, that means that there is an input in the training data consisting of two triples, where one triple uses predicate A and the other uses predicate B. If the combination is unseen, then the converse is true. The table below provides the relevant frequencies.</p>
<table>
<thead>
<tr>
<th>_ in training</th>
<th>Frequency English</th>
<th>Frequency Russian</th>
</tr>
</thead>
<tbody>
<tr>
<td>unseen</td>
<td>1295</td>
<td>354</td>
</tr>
<tr>
<td>seen</td>
<td>115</td>
<td>494</td>
</tr>
</tbody>
</table>
<p>Selection 4 (frequency): seen/unseen arguments. This selection checks for all input whether or not all arg1s and arg2s in the input have been seen during the training phase. For this selection, <em>Seen</em> is the default. Only if all arg1 instances for a particular input are unseen, do we count the arg1s of the input as unseen. The same holds for arg2. So "seen" here really means that at least some of the arg1s or arg2s are seen in the input. The table below provides the relevant frequencies. Note that the comparison is only valid for English. Not for Russian, since there are very few examples of unseen combinations of predicates.</p>
<table>
<thead>
<tr>
<th>Arguments seen in training?</th>
<th>Frequency English</th>
<th>Frequency Russian</th>
</tr>
</thead>
<tbody>
<tr>
<td>both_seen</td>
<td>518</td>
<td>1075</td>
</tr>
<tr>
<td>both_unseen</td>
<td>1177</td>
<td>4</td>
</tr>
<tr>
<td>arg1_unseen</td>
<td>56</td>
<td>19</td>
</tr>
<tr>
<td>arg2_unseen</td>
<td>28</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>Selection 5 (shape): repeated subjects. For this selection, the subsets are based on the times a subject is repeated in the input; it only takes into account the maximum number of times a subject is repeated, that is, if in one input a subject appears 3 times and a different subject 2 times, this input will be in the "3_subjects_same' split. Unique_subjects means all subjects are different.</p>
<table>
<thead>
<tr>
<th>Max num. of repeated subjects</th>
<th>Frequency English</th>
<th>Frequency Russian</th>
</tr>
</thead>
<tbody>
<tr>
<td>unique_subjects</td>
<td>453</td>
<td>339</td>
</tr>
<tr>
<td>2_subjects_same</td>
<td>414</td>
<td>316</td>
</tr>
<tr>
<td>3_subjects_same</td>
<td>382</td>
<td>217</td>
</tr>
<tr>
<td>4_subjects_same</td>
<td>251</td>
<td>143</td>
</tr>
<tr>
<td>5_subjects_same</td>
<td>158</td>
<td>56</td>
</tr>
<tr>
<td>6_subjects_same</td>
<td>80</td>
<td>19</td>
</tr>
<tr>
<td>7_subjects_same</td>
<td>41</td>
<td>12</td>
</tr>
</tbody>
</table>
<p>Selection 6 (shape): repeated objects. Same as for subjects above, but for objects. There are much less cases of repeated objects, so there are only two categories for this selection, unique_objects and some_objects_repeated; for the latter, we have up to 3 coreferring objects in English, and XXX in Russian.</p>
<table>
<thead>
<tr>
<th>Max num. of repeated objects</th>
<th>Frequency English</th>
<th>Frequency Russian</th>
</tr>
</thead>
<tbody>
<tr>
<td>unique_objects</td>
<td>1654</td>
<td>1099</td>
</tr>
<tr>
<td>some_objects_same</td>
<td>125</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>Selection 7 (shape): repeated properties. Same as for objects above, but for properties; up to two properties can be the same in English, up to XXX in Russian.</p>
<table>
<thead>
<tr>
<th>Max num. of repeated properties</th>
<th>Frequency English</th>
<th>Frequency Russian</th>
</tr>
</thead>
<tbody>
<tr>
<td>unique_properties</td>
<td>1510</td>
<td>986</td>
</tr>
<tr>
<td>some_properties_same</td>
<td>269</td>
<td>116</td>
</tr>
</tbody>
</table>
<p>Selection 8 (shape): entities that appear both as subject and object. For this selection, we grouped together the inputs in which no entity is found as both subject and object, and on the other side inputs in which one or more entity/ies appear both as subject and as object. We found up to two such entities per input in English, and up to XXX in Russian.</p>
<table>
<thead>
<tr>
<th>Max num. of objects and subjects in common</th>
<th>Frequency English</th>
<th>Frequency Russian</th>
</tr>
</thead>
<tbody>
<tr>
<td>unique_properties</td>
<td>1322</td>
<td>642</td>
</tr>
<tr>
<td>some_properties_same</td>
<td>457</td>
<td>460</td>
</tr>
</tbody>
</table>
<h2 id="considerations-for-using-the-data">Considerations for Using the Data</h2>
<h3 id="social-impact-of-the-dataset">Social Impact of the Dataset</h3>
<p>We do not foresee any negative social impact in particular from this dataset or task.</p>
<p>Positive outlooks: Being able to generate good quality text from RDF data would permit, e.g., making this data more accessible to lay users, enriching existing text with information drawn from knowledge bases such as DBpedia or describing, comparing and relating entities present in these knowledge bases.</p>
<h3 id="impact-on-underserved-communities">Impact on Underserved Communities</h3>
<p>N/A</p>
<h3 id="discussion-of-biases">Discussion of Biases</h3>
<p>This dataset is created using DBpedia RDF triples which naturally exhibit biases that have been found to exist in Wikipedia such as some forms of, e.g., gender bias.</p>
<p>The choice of <a href="https://gitlab.com/shimorina/webnlg-dataset/-/blob/master/supplementary/entities_dict.json">entities</a>, described by RDF trees, was not controlled. As such, they may contain gender biases; for instance, all the astronauts described by RDF triples are male. Hence, in texts, pronouns <em>he/him/his</em> occur more often. Similarly, entities can be related to the Western culture more often than to other cultures.</p>
<h3 id="other-known-limitations">Other Known Limitations</h3>
<p>The quality of the crowdsourced references is limited, in particular in terms of fluency/naturalness of the collected texts.</p>
<p>Russian data was machine-translated and then post-edited by crowdworkers, so some examples may still exhibit issues related to bad translations.</p>
<h2 id="getting-started-with-in-depth-research-on-the-task">Getting started with in-depth research on the task</h2>
<p>Dataset construction: <a href="https://www.aclweb.org/anthology/P17-1017/">main dataset paper</a>, <a href="https://www.aclweb.org/anthology/C16-1141/">RDF triple extraction</a>, <a href="https://www.aclweb.org/anthology/W19-3706/">Russian translation</a></p>
<p>WebNLG Challenge 2017: <a href="https://webnlg-challenge.loria.fr/challenge_2017/">webpage</a>, <a href="https://www.aclweb.org/anthology/W17-3518/">paper</a></p>
<p>WebNLG Challenge 2020: <a href="https://webnlg-challenge.loria.fr/challenge_2020/">webpage</a>, <a href="https://webnlg-challenge.loria.fr/files/2020.webnlg-papers.7.pdf">paper</a></p>
<p>Enriched version of WebNLG: <a href="https://github.com/ThiagoCF05/webnlg">repository</a>, <a href="https://www.aclweb.org/anthology/W18-6521/">paper</a></p>
<p>Related research papers: <a href="https://webnlg-challenge.loria.fr/research/">webpage</a></p>
</div></article></main><div class="layout_push__1J9g0"></div></div><footer class="layout_footer__127N0 utils_eggshell__Njxsh"><span class="layout_backToHome__1vZsp"><a href="/">‚Üê Home</a></span><span>If you have any questions, please join our <a href="https://groups.google.com/g/gem-benchmark" target="_blank" class="utils_accentUnderline__k083p">google group</a> for support.</span></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"taskData":{"id":"WebNLG","contentHtml":"\u003ch2 id=\"table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#dataset-description\"\u003eDataset Description\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#dataset-and-task-summary\"\u003eDataset and Task Summary\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#why-is-this-dataset-part-of-gem\"\u003eWhy is this dataset part of GEM?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#languages\"\u003eLanguages\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#meta-information\"\u003eMeta Information\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#dataset-curators\"\u003eDataset Curators\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#licensing-information\"\u003eLicensing Information\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#citation-information\"\u003eCitation Information\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#leaderboard\"\u003eLeaderboard\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dataset-structure\"\u003eDataset Structure\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#data-instances\"\u003eData Instances\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#data-fields\"\u003eData Fields\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#data-statistics\"\u003eData Statistics\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dataset-creation\"\u003eDataset Creation\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#curation-rationale\"\u003eCuration Rationale\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#communicative-goal\"\u003eCommunicative Goal\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#source-data\"\u003eSource Data\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#initial-data-collection-and-normalization\"\u003eInitial Data Collection and Normalization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#who-are-the-source-language-producers\"\u003eWho are the source language producers?\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#annotations\"\u003eAnnotations\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#annotation-process\"\u003eAnnotation process\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#who-are-the-annotators\"\u003eWho are the annotators?\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#personal-and-sensitive-information\"\u003ePersonal and Sensitive Information\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#changes-to-the-original-dataset-for-gem\"\u003eChanges to the Original Dataset for GEM\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#special-test-sets\"\u003eSpecial test sets\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#data-shift\"\u003eData shift\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#transformations\"\u003eTransformations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#subpopulations\"\u003eSubpopulations\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#considerations-for-using-the-data\"\u003eConsiderations for Using the Data\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#social-impact-of-the-dataset\"\u003eSocial Impact of the Dataset\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#impact-on-underserved-communities\"\u003eImpact on Underserved Communities\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#discussion-of-biases\"\u003eDiscussion of Biases\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#other-known-limitations\"\u003eOther Known Limitations\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#getting-started-with-in-depth-research-on-the-task\"\u003eGetting started with in-depth research on the task\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"dataset-description\"\u003eDataset Description\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHomepage:\u003c/strong\u003e \u003ca href=\"https://webnlg-challenge.loria.fr/\"\u003ehttps://webnlg-challenge.loria.fr/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRepository:\u003c/strong\u003e \u003ca href=\"https://gitlab.com/shimorina/webnlg-dataset\"\u003ehttps://gitlab.com/shimorina/webnlg-dataset\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePaper:\u003c/strong\u003e \u003ca href=\"http://www.aclweb.org/anthology/P17-1017\"\u003eFirst Dataset Release\u003c/a\u003e, \u003ca href=\"https://www.aclweb.org/anthology/W17-3518/\"\u003eWebNLG Challenge 2017 Report\u003c/a\u003e, \u003ca href=\"https://webnlg-challenge.loria.fr/files/2020.webnlg-papers.7.pdf\"\u003eWebNLG Challenge 2020 Report\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoint of Contact:\u003c/strong\u003e \u003ca href=\"mailto:webnlg-challenge@inria.fr\"\u003ewebnlg-challenge@inria.fr\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"dataset-and-task-summary\"\u003eDataset and Task Summary\u003c/h3\u003e\n\u003cp\u003eWebNLG is a bi-lingual dataset (English, Russian) of parallel DBpedia triple sets and short texts that cover about 450 different DBpedia properties. The WebNLG data was originally created to promote the development of RDF verbalisers able to generate short text and to handle micro-planning (i.e., sentence segmentation and ordering, referring expression generation, aggregation); the goal of the task is to generate texts starting from 1 to 7 input triples which have entities in common (so the input is actually a connected Knowledge Graph). The dataset contains about 17,000 triple sets and 45,000 crowdsourced texts in English, and 7,000 triples sets and 19,000 crowdsourced texts in Russian. A challenging test set section with entities and/or properties that have not been seen at training time is available.\u003c/p\u003e\n\u003ch3 id=\"why-is-this-dataset-part-of-gem\"\u003eWhy is this dataset part of GEM?\u003c/h3\u003e\n\u003cp\u003eThe WebNLG 2020 dataset (version 3.0) is a large bi-lingual dataset with crowdsourced reference texts and a rather large variety of knowledge in the inputs.\u003c/p\u003e\n\u003ch3 id=\"languages\"\u003eLanguages\u003c/h3\u003e\n\u003cp\u003eEnglish (\u003ccode\u003een\u003c/code\u003e, all dataset versions), Russian (\u003ccode\u003eru\u003c/code\u003e, from the version 3.0 on).\u003c/p\u003e\n\u003ch2 id=\"meta-information\"\u003eMeta Information\u003c/h2\u003e\n\u003ch3 id=\"dataset-curators\"\u003eDataset Curators\u003c/h3\u003e\n\u003cp\u003eThe principle curator of the dataset is Anastasia Shimorina (Universit√© de Lorraine / LORIA, France). Throughout the WebNLG releases, several people contributed to their construction: Claire Gardent (CNRS / LORIA, France), Shashi Narayan (Google, UK), Laura Perez-Beltrachini (University of Edinburgh, UK), Elena Khasanova, and Thiago Castro Ferreira (Federal University of Minas Gerais, Brazil).\nThe dataset construction was funded by the French National Research Agency (ANR).\u003c/p\u003e\n\u003ch3 id=\"licensing-information\"\u003eLicensing Information\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\"\u003eCC Attribution-Noncommercial-Share Alike 4.0 International\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"citation-information\"\u003eCitation Information\u003c/h3\u003e\n\u003cp\u003eInitial release of the dataset:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{gardent2017creating,\n  author = \t\"Gardent, Claire\n\t\tand Shimorina, Anastasia\n\t\tand Narayan, Shashi\n\t\tand Perez-Beltrachini, Laura\",\n  title = \t\"Creating Training Corpora for NLG Micro-Planners\",\n  booktitle = \t\"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n  year = \t\"2017\",\n  publisher = \t\"Association for Computational Linguistics\",\n  pages = \t\"179--188\",\n  location = \t\"Vancouver, Canada\",\n  doi = \t\"10.18653/v1/P17-1017\",\n  url = \t\"http://www.aclweb.org/anthology/P17-1017\"\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe latest version 3.0:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{castro-ferreira20:bilin-bi-direc-webnl-shared,\n  title={The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task Overview and Evaluation Results (WebNLG+ 2020)},\n  author={Castro Ferreira, Thiago and\n                  Gardent, Claire and\n\t\t  Ilinykh, Nikolai and\n\t\t  van der Lee, Chris and\n\t\t  Mille, Simon and\n\t\t  Moussallem, Diego and\n\t\t  Shimorina, Anastasia},\n  booktitle = {Proceedings of the 3rd WebNLG Workshop on Natural Language Generation from the Semantic Web (WebNLG+ 2020)},\n    pages = \"55--76\",\n  year = \t 2020,\n  address = \t {Dublin, Ireland (Virtual)},\n  publisher = {Association for Computational Linguistics}}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"leaderboard\"\u003eLeaderboard\u003c/h3\u003e\n\u003cp\u003eThe dataset supports an active leaderboard, the best results are tracked here: \u003ca href=\"https://beng.dice-research.org/gerbil/\"\u003ehttps://beng.dice-research.org/gerbil/\u003c/a\u003e. The model outputs are evaluated against the crowdsourced references; the leaderboard reports BLEU-4, METEOR, chrF++, TER, BERTScore and BLEURT scores.\u003c/p\u003e\n\u003ch2 id=\"dataset-structure\"\u003eDataset Structure\u003c/h2\u003e\n\u003ch3 id=\"data-instances\"\u003eData Instances\u003c/h3\u003e\n\u003cp\u003eEnglish instance:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e{\n\"entry\": {\n\t\"category\": \"Company\",\n\t\"size\": \"4\",\n\t\"shape\": \"(X (X) (X) (X) (X))\",\n\t\"shape_type\": \"sibling\",\n\t\"eid\": \"Id21\",\n\t\"lexs\": [\n\t    {\n\t\t\"comment\": \"good\",\n\t\t\"lex\": \"Trane, which was founded on January 1st 1913 in La Crosse, Wisconsin, is based in Ireland. It has 29,000 employees.\",\n\t\t\"lid\": \"Id1\"\n\t    }\n\t],\n\t\"modifiedtripleset\": [\n\t    {\n\t\t\"subject\": \"Trane\",\n\t\t\"property\": \"foundingDate\",\n\t\t\"object\": \"1913-01-01\"\n\t    },\n\t    {\n\t\t\"subject\": \"Trane\",\n\t\t\"property\": \"location\",\n\t\t\"object\": \"Ireland\"\n\t    },\n\t    {\n\t\t\"subject\": \"Trane\",\n\t\t\"property\": \"foundationPlace\",\n\t\t\"object\": \"La_Crosse,_Wisconsin\"\n\t    },\n\t    {\n\t\t\"subject\": \"Trane\",\n\t\t\"property\": \"numberOfEmployees\",\n\t\t\"object\": \"29000\"\n\t    }\n\n\t],\n\t\"originaltriplesets\": {\n\t    \"originaltripleset\": [\n\t\t    {\n\t\t\t\"subject\": \"Trane\",\n\t\t\t\"property\": \"foundingDate\",\n\t\t\t\"object\": \"1913-01-01\"\n\t\t    },\n\t\t    {\n\t\t\t\"subject\": \"Trane\",\n\t\t\t\"property\": \"location\",\n\t\t\t\"object\": \"Ireland\"\n\t\t    },\n\t\t    {\n\t\t\t\"subject\": \"Trane\",\n\t\t\t\"property\": \"foundationPlace\",\n\t\t\t\"object\": \"La_Crosse,_Wisconsin\"\n\t\t    },\n\t\t    {\n\t\t\t\"subject\": \"Trane\",\n\t\t\t\"property\": \"numberOfEmployees\",\n\t\t\t\"object\": \"29000\"\n\t\t    }\n\t    ]\n\t}\n\n\t}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe XML-formatted example is \u003ca href=\"https://webnlg-challenge.loria.fr/docs/#example\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eRussian instance:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e{\n\"entry\": {\n\t\"category\": \"Building\",\n\t\"size\": \"3\",\n\t\"shape\": \"(X (X (X (X))))\",\n\t\"shape_type\": \"chain\",\n\t\"eid\": \"Id2\",\n\t\"lexs\": [\n\t    {\n\t\t\"lang\": \"en\",\n\t\t\"lex\": \"Andrew Mitchell is a leader in Birmingham where the architect John Madin who designed 103 Colmore Row was born.\",\n\t\t\"lid\": \"Id2\"\n\t    },\n\t    {\n\t\t\"lang\": \"ru\",\n\t\t\"lex\": \"–≠–Ω–¥—Ä—é –ú–∏—Ç—á–µ–ª–ª - –ª–∏–¥–µ—Ä –≤ –ë–∏—Ä–º–∏–Ω–≥–µ–º–µ, –≥–¥–µ —Ä–æ–¥–∏–ª—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä –î–∂–æ–Ω –ú–∞–¥–∏–Ω, –∫–æ—Ç–æ—Ä—ã–π —Å–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–ª –ö–æ–ª–º–æ—Ä-—Ä–æ—É 103.\",\n\t\t\"lid\": \"Id2\"\n\t    }\n\t],\n\t\"modifiedtripleset\": [\n\t\t    {\n\t\t\t\"subject\": \"103_Colmore_Row\",\n\t\t\t\"property\": \"architect\",\n\t\t\t\"object\": \"John_Madin\"\n\t\t    },\n\t\t    {\n\t\t\t\"subject\": \"John_Madin\",\n\t\t\t\"property\": \"birthPlace\",\n\t\t\t\"object\": \"Birmingham\"\n\t\t    },\n\t\t    {\n\t\t\t\"subject\": \"Birmingham\",\n\t\t\t\"property\": \"leaderName\",\n\t\t\t\"object\": \"Andrew_Mitchell\"\n\t\t    }\n\n\t],\n\t\"originaltriplesets\": {\n\t    \"originaltripleset\": [\n\t\t    {\n\t\t\t\"subject\": \"103_Colmore_Row\",\n\t\t\t\"property\": \"architect\",\n\t\t\t\"object\": \"John_Madin\"\n\t\t    },\n\t\t    {\n\t\t\t\"subject\": \"John_Madin\",\n\t\t\t\"property\": \"birthPlace\",\n\t\t\t\"object\": \"Birmingham\"\n\t\t    },\n\t\t    {\n\t\t\t\"subject\": \"Birmingham\",\n\t\t\t\"property\": \"leaderName\",\n\t\t\t\"object\": \"Andrew_Mitchell\"\n\t\t    }\n\t    ]\n\t},\n\n\t\"dbpedialinks\" : [\n\t\t    {\n\t\t\t\"subject\": \"Birmingham\",\n\t\t\t\"property\": \"sameAs\",\n\t\t\t\"object\": \"–ë–∏—Ä–º–∏–Ω–≥–µ–º\"\n\t\t    }\n\t],\n\n\t\"links\" : [\n\t\t    {\n\t\t\t\"subject\": \"Andrew Mitchell\",\n\t\t\t\"property\": \"sameAs\",\n\t\t\t\"object\": \"–≠–Ω–¥—Ä—é –ú–∏—Ç—á–µ–ª–ª (–º—É–∂)\"\n\t\t    },\n\t\t    {\n\t\t\t\"subject\": \"John Madin\",\n\t\t\t\"property\": \"sameAs\",\n\t\t\t\"object\": \"–î–∂–æ–Ω –ú–∞–¥–∏–Ω (–º—É–∂)\"\n\t\t    },\n\t\t    {\n\t\t\t\"subject\": \"103 Colmore Row\",\n\t\t\t\"property\": \"sameAs\",\n\t\t\t\"object\": \"–ö–æ–ª–º–æ—Ä-—Ä–æ—É 103\"\n\t\t    }\n\t]\n\n\t}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"data-fields\"\u003eData Fields\u003c/h3\u003e\n\u003cp\u003eSee \u003ca href=\"https://webnlg-challenge.loria.fr/docs/\"\u003eofficial documentation\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eentry\u003c/code\u003e: a data instance of the benchmark. Each entry has five attributes: a DBpedia category (\u003ccode\u003ecategory\u003c/code\u003e), entry ID (\u003ccode\u003eeid\u003c/code\u003e), shape, shape type, and triple set size (\u003ccode\u003esize\u003c/code\u003e).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eshape\u003c/code\u003e: a string representation of the RDF tree with nested parentheses where \u003ccode\u003eX\u003c/code\u003e is a node (see \u003ca href=\"https://en.wikipedia.org/wiki/Newick_format\"\u003eNewick tree format\u003c/a\u003e).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eshape_type\u003c/code\u003e: a type of the tree shape. We \u003ca href=\"https://www.aclweb.org/anthology/C16-1141.pdf\"\u003eidentify\u003c/a\u003e three types of tree shapes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003echain\u003c/code\u003e (the object of one triple is the subject of the other);\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esibling\u003c/code\u003e (triples with a shared subject);\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emixed\u003c/code\u003e (both \u003ccode\u003echain\u003c/code\u003e and \u003ccode\u003esibling\u003c/code\u003e types present).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eeid\u003c/code\u003e: an entry ID. It is unique only within a category and a size.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ecategory\u003c/code\u003e: a DBpedia category (Astronaut, City, MusicalWork, Politician, etc.).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003esize\u003c/code\u003e: the number of RDF triples in a set. Ranges from 1 to 7.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach \u003ccode\u003eentry\u003c/code\u003e has three fields: \u003ccode\u003eoriginaltripleset\u003c/code\u003e, \u003ccode\u003emodifiedtripleset\u003c/code\u003e, and \u003ccode\u003elexs\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eoriginaltripleset\u003c/code\u003e: a set of RDF triples as extracted from \u003ca href=\"https://wiki.dbpedia.org/\"\u003eDBpedia\u003c/a\u003e. Each set of RDF triples is a tree. Triples have the subject-predicate-object structure.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003emodifiedtripleset\u003c/code\u003e: a set of RDF triples as presented to crowdworkers (for more details on modifications, see below).\u003c/p\u003e\n\u003cp\u003eOriginal and modified triples serve different purposes: the original triples ‚Äî to link data to a knowledge base (DBpedia), whereas the modified triples ‚Äî to ensure consistency and homogeneity throughout the data. To train models, the modified triples should be used.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003elexs\u003c/code\u003e (shortened for lexicalisations): a natural language text verbalising the triples. Each lexicalisation has two attributes: a comment (\u003ccode\u003ecomment\u003c/code\u003e), and a lexicalisation ID (\u003ccode\u003elid\u003c/code\u003e). By default, comments have the value \u003ccode\u003egood\u003c/code\u003e, except rare cases when they were manually marked as \u003ccode\u003etoFix\u003c/code\u003e. That was done during the corpus creation, when it was seen that a lexicalisation did not exactly match a triple set.\u003c/p\u003e\n\u003cp\u003eRussian data has additional optional fields comparing to English:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\u0026#x3C;dbpedialinks\u003e\u003c/code\u003e: RDF triples extracted from DBpedia between English and Russian entities by means of the property \u003ccode\u003esameAs\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\u0026#x3C;links\u003e\u003c/code\u003e: RDF triples created manually for some entities to serve as pointers to translators. There are two types of them:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ewith \u003ccode\u003esameAs\u003c/code\u003e (\u003ccode\u003eSpaniards | sameAs | –∏—Å–ø–∞–Ω—Ü—ã\u003c/code\u003e)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ewith \u003ccode\u003eincludes\u003c/code\u003e (\u003ccode\u003eTomatoes, guanciale, cheese, olive oil | includes | –≥—É–∞–Ω—á–∏–∞–ª–µ\u003c/code\u003e). Those were mostly created for string literals to translate some parts of them.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLexicalisations in the Russian WebNLG have a new parameter \u003ccode\u003elang\u003c/code\u003e (values: \u003ccode\u003een\u003c/code\u003e, \u003ccode\u003eru\u003c/code\u003e) because original English texts were kept in the Russian version (see the example above).\u003c/p\u003e\n\u003ch3 id=\"data-statistics\"\u003eData Statistics\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eEnglish (v3.0)\u003c/th\u003e\n\u003cth\u003eTrain\u003c/th\u003e\n\u003cth\u003eDev\u003c/th\u003e\n\u003cth\u003eTest\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003etriple sets\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e13,211\u003c/td\u003e\n\u003ctd\u003e1,667\u003c/td\u003e\n\u003ctd\u003e1,779\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003etexts\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e35,426\u003c/td\u003e\n\u003ctd\u003e4,464\u003c/td\u003e\n\u003ctd\u003e5,150\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eproperties\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e372\u003c/td\u003e\n\u003ctd\u003e290\u003c/td\u003e\n\u003ctd\u003e220\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eRussian (v3.0)\u003c/th\u003e\n\u003cth\u003eTrain\u003c/th\u003e\n\u003cth\u003eDev\u003c/th\u003e\n\u003cth\u003eTest\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003etriple sets\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e5,573\u003c/td\u003e\n\u003ctd\u003e790\u003c/td\u003e\n\u003ctd\u003e1,102\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003etexts\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e14,239\u003c/td\u003e\n\u003ctd\u003e2,026\u003c/td\u003e\n\u003ctd\u003e2,780\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eproperties\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e226\u003c/td\u003e\n\u003ctd\u003e115\u003c/td\u003e\n\u003ctd\u003e192\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"dataset-creation\"\u003eDataset Creation\u003c/h2\u003e\n\u003ch3 id=\"curation-rationale\"\u003eCuration Rationale\u003c/h3\u003e\n\u003cp\u003eThe WebNLG dataset was created to promote the development (\u003cem\u003ei\u003c/em\u003e) of RDF verbalisers and (\u003cem\u003eii\u003c/em\u003e) of microplanners able to handle a wide range of linguistic constructions. The dataset aims at covering knowledge in different domains (\"categories\"). The same properties and entities can appear in several categories.\u003c/p\u003e\n\u003ch3 id=\"communicative-goal\"\u003eCommunicative Goal\u003c/h3\u003e\n\u003cp\u003eThe systems are required to produce one text (one or more sentences) that verbalises all and only the input triples in a grammatical and natural way.\u003c/p\u003e\n\u003ch3 id=\"source-data\"\u003eSource Data\u003c/h3\u003e\n\u003cp\u003eThe data was compiled from raw DBpedia triples. \u003ca href=\"https://www.aclweb.org/anthology/C16-1141/\"\u003eThis paper\u003c/a\u003e explains how the triples were selected.\u003c/p\u003e\n\u003ch4 id=\"initial-data-collection-and-normalization\"\u003eInitial Data Collection and Normalization\u003c/h4\u003e\n\u003cp\u003eInitial triples extracted from DBpedia were modified in several ways. See \u003ca href=\"https://webnlg-challenge.loria.fr/docs/\"\u003eofficial documentation\u003c/a\u003e for the most frequent changes that have been made. An original tripleset and a modified tripleset usually represent a one-to-one mapping. However, there are cases with many-to-one mappings when several original triplesets are mapped to one modified tripleset.\u003c/p\u003e\n\u003cp\u003eEntities that served as roots of RDF trees are listed in \u003ca href=\"https://gitlab.com/shimorina/webnlg-dataset/-/blob/master/supplementary/entities_dict.json\"\u003ethis file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe English WebNLG 2020 dataset (v3.0) for training comprises data-text pairs for 16 distinct DBpedia categories:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe 10 seen categories used in the 2017 version: Airport, Astronaut, Building, City, ComicsCharacter, Food, Monument, SportsTeam, University, and WrittenWork.\u003c/li\u003e\n\u003cli\u003eThe 5 unseen categories of 2017, which are now part of the seen data: Athlete, Artist, CelestialBody, MeanOfTransportation, Politician.\u003c/li\u003e\n\u003cli\u003e1 new category: Company.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe Russian dataset (v3.0) comprises data-text pairs for 9 distinct categories: Airport, Astronaut, Building, CelestialBody, ComicsCharacter, Food, Monument, SportsTeam, and University.\u003c/p\u003e\n\u003ch4 id=\"who-are-the-source-language-producers\"\u003eWho are the source language producers?\u003c/h4\u003e\n\u003cp\u003eThere are no source texts, all textual material was compiled during the annotation process.\u003c/p\u003e\n\u003ch3 id=\"annotations\"\u003eAnnotations\u003c/h3\u003e\n\u003ch4 id=\"annotation-process\"\u003eAnnotation process\u003c/h4\u003e\n\u003cp\u003eAnnotators were first asked to create sentences that verbalise single triples. In a second round, annotators were asked to combine single-triple sentences together into sentences that cover 2 triples. And so on until 7 triples. Quality checks were performed to ensure the quality of the annotations. See Section 3.3 in \u003ca href=\"https://www.aclweb.org/anthology/P17-1017.pdf\"\u003ethe dataset paper\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eRussian data was translated from English with an MT system and then was post-edited by crowdworkers. See Section 2.2 of \u003ca href=\"https://webnlg-challenge.loria.fr/files/2020.webnlg-papers.7.pdf\"\u003ethis paper\u003c/a\u003e.\u003c/p\u003e\n\u003ch4 id=\"who-are-the-annotators\"\u003eWho are the annotators?\u003c/h4\u003e\n\u003cp\u003eAll references were collected through crowdsourcing platforms (CrowdFlower/Figure 8 and Amazon Mechanical Turk). For Russian, post-editing was done using the Yandex.Toloka crowdsourcing platform.\u003c/p\u003e\n\u003ch3 id=\"personal-and-sensitive-information\"\u003ePersonal and Sensitive Information\u003c/h3\u003e\n\u003cp\u003eNeither the dataset as published or the annotation process involves the collection or sharing of any kind of personal / demographic information.\u003c/p\u003e\n\u003ch2 id=\"changes-to-the-original-dataset-for-gem\"\u003eChanges to the Original Dataset for GEM\u003c/h2\u003e\n\u003cp\u003eNo changes to the main content of the dataset. The \u003ca href=\"https://gitlab.com/shimorina/webnlg-dataset/-/tree/master/release_v3.0\"\u003eversion 3.0\u003c/a\u003e of the dataset is used.\u003c/p\u003e\n\u003ch3 id=\"special-test-sets\"\u003eSpecial test sets\u003c/h3\u003e\n\u003cp\u003e23 special test sets for WebNLG were added to the GEM evaluation suite, 12 for English and 11 for Russian.\u003c/p\u003e\n\u003ch4 id=\"data-shift\"\u003eData shift\u003c/h4\u003e\n\u003cp\u003eFor both languages, we created subsets of the training and development sets of ~500 randomly selected inputs each. The inputs were sampled proportionnally from each category.\u003c/p\u003e\n\u003ch4 id=\"transformations\"\u003eTransformations\u003c/h4\u003e\n\u003cp\u003eTwo types of transformations have been applied to WebNLG: (i) input scrambling (English and Russian) and (ii) numerical value replacements (English); in both cases, a subset of about 500 inputs was randomly selected. For (i), the order of the triples was randomly reassigned (each triple kept the same Subject-Property-Object internal order). For (ii), the change was performed respecting the format of the current cardinal value (e.g., alpha, integer, or floating-point) and replacing it with a new random value. The new number is lower-bounded between zero and upper bounded to be within to the highest power of 10 unit for the given value (e.g., replacing 54 would result in a random value between 0-100). Floating values maintain the degree of precision.\u003c/p\u003e\n\u003ch4 id=\"subpopulations\"\u003eSubpopulations\u003c/h4\u003e\n\u003cp\u003eFor both languages, we did identify different subsets of the test set that we could compare to each other so that we would have a better understanding of the results. There are currently 8 selections that we have made:\u003c/p\u003e\n\u003cp\u003eSelection 1 (size): input length. This selection corresponds to the number of predicates in the input. By comparing inputs of different lengths, we can see to what extent NLG systems are able to handle different input sizes.  The table below provides the relevant frequencies. Please be aware that comparing selections with fewer than 100 items may result in unreliable comparisons.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eInput length\u003c/th\u003e\n\u003cth\u003eFrequency English\u003c/th\u003e\n\u003cth\u003eFrequency Russian\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e369\u003c/td\u003e\n\u003ctd\u003e254\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e349\u003c/td\u003e\n\u003ctd\u003e200\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e3\u003c/td\u003e\n\u003ctd\u003e350\u003c/td\u003e\n\u003ctd\u003e214\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e305\u003c/td\u003e\n\u003ctd\u003e214\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e213\u003c/td\u003e\n\u003ctd\u003e159\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e6\u003c/td\u003e\n\u003ctd\u003e114\u003c/td\u003e\n\u003ctd\u003e32\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e7\u003c/td\u003e\n\u003ctd\u003e79\u003c/td\u003e\n\u003ctd\u003e29\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eSelection 2 (frequency): seen/unseen single predicates. This selection corresponds to the inputs with only one predicate. We compare which predicates are seen/unseen in the training data. The table below provides the relevant frequencies. Note that the comparison is only valid for English. Not for Russian, since there is only one example of unseen single predicates.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e_ in training\u003c/th\u003e\n\u003cth\u003eFrequency English\u003c/th\u003e\n\u003cth\u003eFrequency Russian\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eSeen\u003c/td\u003e\n\u003ctd\u003e297\u003c/td\u003e\n\u003ctd\u003e253\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eUnseen\u003c/td\u003e\n\u003ctd\u003e72\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eSelection 3 (frequency): seen/unseen combinations of predicates. This selection checks for all combinations of predicates whether that combination has been seen in the training data. For example: if the combination of predicates A and B is seen, that means that there is an input in the training data consisting of two triples, where one triple uses predicate A and the other uses predicate B. If the combination is unseen, then the converse is true. The table below provides the relevant frequencies.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e_ in training\u003c/th\u003e\n\u003cth\u003eFrequency English\u003c/th\u003e\n\u003cth\u003eFrequency Russian\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eunseen\u003c/td\u003e\n\u003ctd\u003e1295\u003c/td\u003e\n\u003ctd\u003e354\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eseen\u003c/td\u003e\n\u003ctd\u003e115\u003c/td\u003e\n\u003ctd\u003e494\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eSelection 4 (frequency): seen/unseen arguments. This selection checks for all input whether or not all arg1s and arg2s in the input have been seen during the training phase. For this selection, \u003cem\u003eSeen\u003c/em\u003e is the default. Only if all arg1 instances for a particular input are unseen, do we count the arg1s of the input as unseen. The same holds for arg2. So \"seen\" here really means that at least some of the arg1s or arg2s are seen in the input. The table below provides the relevant frequencies. Note that the comparison is only valid for English. Not for Russian, since there are very few examples of unseen combinations of predicates.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eArguments seen in training?\u003c/th\u003e\n\u003cth\u003eFrequency English\u003c/th\u003e\n\u003cth\u003eFrequency Russian\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eboth_seen\u003c/td\u003e\n\u003ctd\u003e518\u003c/td\u003e\n\u003ctd\u003e1075\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eboth_unseen\u003c/td\u003e\n\u003ctd\u003e1177\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003earg1_unseen\u003c/td\u003e\n\u003ctd\u003e56\u003c/td\u003e\n\u003ctd\u003e19\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003earg2_unseen\u003c/td\u003e\n\u003ctd\u003e28\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eSelection 5 (shape): repeated subjects. For this selection, the subsets are based on the times a subject is repeated in the input; it only takes into account the maximum number of times a subject is repeated, that is, if in one input a subject appears 3 times and a different subject 2 times, this input will be in the \"3_subjects_same' split. Unique_subjects means all subjects are different.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eMax num. of repeated subjects\u003c/th\u003e\n\u003cth\u003eFrequency English\u003c/th\u003e\n\u003cth\u003eFrequency Russian\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eunique_subjects\u003c/td\u003e\n\u003ctd\u003e453\u003c/td\u003e\n\u003ctd\u003e339\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2_subjects_same\u003c/td\u003e\n\u003ctd\u003e414\u003c/td\u003e\n\u003ctd\u003e316\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e3_subjects_same\u003c/td\u003e\n\u003ctd\u003e382\u003c/td\u003e\n\u003ctd\u003e217\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e4_subjects_same\u003c/td\u003e\n\u003ctd\u003e251\u003c/td\u003e\n\u003ctd\u003e143\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e5_subjects_same\u003c/td\u003e\n\u003ctd\u003e158\u003c/td\u003e\n\u003ctd\u003e56\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e6_subjects_same\u003c/td\u003e\n\u003ctd\u003e80\u003c/td\u003e\n\u003ctd\u003e19\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e7_subjects_same\u003c/td\u003e\n\u003ctd\u003e41\u003c/td\u003e\n\u003ctd\u003e12\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eSelection 6 (shape): repeated objects. Same as for subjects above, but for objects. There are much less cases of repeated objects, so there are only two categories for this selection, unique_objects and some_objects_repeated; for the latter, we have up to 3 coreferring objects in English, and XXX in Russian.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eMax num. of repeated objects\u003c/th\u003e\n\u003cth\u003eFrequency English\u003c/th\u003e\n\u003cth\u003eFrequency Russian\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eunique_objects\u003c/td\u003e\n\u003ctd\u003e1654\u003c/td\u003e\n\u003ctd\u003e1099\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003esome_objects_same\u003c/td\u003e\n\u003ctd\u003e125\u003c/td\u003e\n\u003ctd\u003e3\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eSelection 7 (shape): repeated properties. Same as for objects above, but for properties; up to two properties can be the same in English, up to XXX in Russian.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eMax num. of repeated properties\u003c/th\u003e\n\u003cth\u003eFrequency English\u003c/th\u003e\n\u003cth\u003eFrequency Russian\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eunique_properties\u003c/td\u003e\n\u003ctd\u003e1510\u003c/td\u003e\n\u003ctd\u003e986\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003esome_properties_same\u003c/td\u003e\n\u003ctd\u003e269\u003c/td\u003e\n\u003ctd\u003e116\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eSelection 8 (shape): entities that appear both as subject and object. For this selection, we grouped together the inputs in which no entity is found as both subject and object, and on the other side inputs in which one or more entity/ies appear both as subject and as object. We found up to two such entities per input in English, and up to XXX in Russian.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eMax num. of objects and subjects in common\u003c/th\u003e\n\u003cth\u003eFrequency English\u003c/th\u003e\n\u003cth\u003eFrequency Russian\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eunique_properties\u003c/td\u003e\n\u003ctd\u003e1322\u003c/td\u003e\n\u003ctd\u003e642\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003esome_properties_same\u003c/td\u003e\n\u003ctd\u003e457\u003c/td\u003e\n\u003ctd\u003e460\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"considerations-for-using-the-data\"\u003eConsiderations for Using the Data\u003c/h2\u003e\n\u003ch3 id=\"social-impact-of-the-dataset\"\u003eSocial Impact of the Dataset\u003c/h3\u003e\n\u003cp\u003eWe do not foresee any negative social impact in particular from this dataset or task.\u003c/p\u003e\n\u003cp\u003ePositive outlooks: Being able to generate good quality text from RDF data would permit, e.g., making this data more accessible to lay users, enriching existing text with information drawn from knowledge bases such as DBpedia or describing, comparing and relating entities present in these knowledge bases.\u003c/p\u003e\n\u003ch3 id=\"impact-on-underserved-communities\"\u003eImpact on Underserved Communities\u003c/h3\u003e\n\u003cp\u003eN/A\u003c/p\u003e\n\u003ch3 id=\"discussion-of-biases\"\u003eDiscussion of Biases\u003c/h3\u003e\n\u003cp\u003eThis dataset is created using DBpedia RDF triples which naturally exhibit biases that have been found to exist in Wikipedia such as some forms of, e.g., gender bias.\u003c/p\u003e\n\u003cp\u003eThe choice of \u003ca href=\"https://gitlab.com/shimorina/webnlg-dataset/-/blob/master/supplementary/entities_dict.json\"\u003eentities\u003c/a\u003e, described by RDF trees, was not controlled. As such, they may contain gender biases; for instance, all the astronauts described by RDF triples are male. Hence, in texts, pronouns \u003cem\u003ehe/him/his\u003c/em\u003e occur more often. Similarly, entities can be related to the Western culture more often than to other cultures.\u003c/p\u003e\n\u003ch3 id=\"other-known-limitations\"\u003eOther Known Limitations\u003c/h3\u003e\n\u003cp\u003eThe quality of the crowdsourced references is limited, in particular in terms of fluency/naturalness of the collected texts.\u003c/p\u003e\n\u003cp\u003eRussian data was machine-translated and then post-edited by crowdworkers, so some examples may still exhibit issues related to bad translations.\u003c/p\u003e\n\u003ch2 id=\"getting-started-with-in-depth-research-on-the-task\"\u003eGetting started with in-depth research on the task\u003c/h2\u003e\n\u003cp\u003eDataset construction: \u003ca href=\"https://www.aclweb.org/anthology/P17-1017/\"\u003emain dataset paper\u003c/a\u003e, \u003ca href=\"https://www.aclweb.org/anthology/C16-1141/\"\u003eRDF triple extraction\u003c/a\u003e, \u003ca href=\"https://www.aclweb.org/anthology/W19-3706/\"\u003eRussian translation\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWebNLG Challenge 2017: \u003ca href=\"https://webnlg-challenge.loria.fr/challenge_2017/\"\u003ewebpage\u003c/a\u003e, \u003ca href=\"https://www.aclweb.org/anthology/W17-3518/\"\u003epaper\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWebNLG Challenge 2020: \u003ca href=\"https://webnlg-challenge.loria.fr/challenge_2020/\"\u003ewebpage\u003c/a\u003e, \u003ca href=\"https://webnlg-challenge.loria.fr/files/2020.webnlg-papers.7.pdf\"\u003epaper\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eEnriched version of WebNLG: \u003ca href=\"https://github.com/ThiagoCF05/webnlg\"\u003erepository\u003c/a\u003e, \u003ca href=\"https://www.aclweb.org/anthology/W18-6521/\"\u003epaper\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eRelated research papers: \u003ca href=\"https://webnlg-challenge.loria.fr/research/\"\u003ewebpage\u003c/a\u003e\u003c/p\u003e\n","title":"WebNLG","type":"Structure-to-text","motivation":"The WebNLG dataset is a large bi-lingual dataset with crowdsourced reference texts and a rather large variety of knowledge in the inputs. A web-based evaluation platform is already existing."}},"__N_SSG":true},"page":"/data_cards/[id]","query":{"id":"WebNLG"},"buildId":"Nx8QlaATgB-argoYvzlf6","nextExport":false,"isFallback":false,"gsp":true,"head":[["meta",{"name":"viewport","content":"width=device-width"}],["meta",{"charSet":"utf-8"}],["link",{"rel":"icon","href":"/favicon.ico"}],["meta",{"name":"description","content":"Benchmark natural language generation systems with GEM."}],["meta",{"property":"og:image","content":"https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light\u0026md=1\u0026fontSize=100px\u0026images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"}],["meta",{"name":"og:title","content":"GEM"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["title",{"children":"GEM WebNLG"}]]}</script><script nomodule="" src="/_next/static/chunks/polyfills-e69cc13a7e89296a69e4.js"></script><script src="/_next/static/chunks/main-47bc8f80085b54a800da.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.baa41d4dbf5d52db897c.js" async=""></script><script src="/_next/static/chunks/e70fad557dfa42f32a11d0d2c99fe8f6e8d1fa86.4a36a385313236c59b19.js" async=""></script><script src="/_next/static/chunks/pages/_app-a9ae7a6d1de4e51a7ab6.js" async=""></script><script src="/_next/static/chunks/cb1608f2.c3a9f0eb95374ca4919a.js" async=""></script><script src="/_next/static/chunks/451c6be158cef50d8cc28b919cf08d1e5b9ff3fc.f0ec181e43727e8a893e.js" async=""></script><script src="/_next/static/chunks/pages/data_cards/%5Bid%5D-e1361c13d80af81deda6.js" async=""></script><script src="/_next/static/Nx8QlaATgB-argoYvzlf6/_buildManifest.js" async=""></script><script src="/_next/static/Nx8QlaATgB-argoYvzlf6/_ssgManifest.js" async=""></script></body></html>