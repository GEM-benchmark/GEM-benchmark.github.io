<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="Benchmark natural language generation systems with GEM."/><meta property="og:image" content="https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"/><meta name="og:title" content="GEM"/><meta name="twitter:card" content="summary_large_image"/><title>GEM CommonGen</title><link rel="preload" href="/_next/static/css/2786522978a02f025205.css" as="style"/><link rel="stylesheet" href="/_next/static/css/2786522978a02f025205.css" data-n-g=""/><link rel="preload" href="/_next/static/css/f2fce7b83fe6ca04479b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f2fce7b83fe6ca04479b.css" data-n-p=""/><noscript data-n-css="true"></noscript><link rel="preload" href="/_next/static/chunks/main-47bc8f80085b54a800da.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.baa41d4dbf5d52db897c.js" as="script"/><link rel="preload" href="/_next/static/chunks/e70fad557dfa42f32a11d0d2c99fe8f6e8d1fa86.4a36a385313236c59b19.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-a9ae7a6d1de4e51a7ab6.js" as="script"/><link rel="preload" href="/_next/static/chunks/cb1608f2.c3a9f0eb95374ca4919a.js" as="script"/><link rel="preload" href="/_next/static/chunks/451c6be158cef50d8cc28b919cf08d1e5b9ff3fc.f0ec181e43727e8a893e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/data_cards/%5Bid%5D-e1361c13d80af81deda6.js" as="script"/></head><body><div id="__next"><div class="layout_background__1AVEa undefined"><header class="layout_header__2rhWq"><div class="navbar_navwrapper__15zia"><div class="navbar_gradbar__1Xi5u"></div><nav class="navbar_navbar__3gnco"><span class="utils_headingLg__de7p0 navbar_navbarlogo__PLEwr"><a href="/">GEM BENCHMARK</a></span><div class="navbar_menutoggle__358pJ" id="mobile-menu"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars fa-w-14 navbar_bar__QVPSR" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"></path></svg></div><ul><li class="navbar_navitem__3ICSG navbar_pushright__3G2DM"><a href="/resources">Resources</a></li><li class="navbar_navitem__3ICSG"><a href="/data_cards">Data Cards</a></li><li class="navbar_navitem__3ICSG"><a href="/model_cards">Model Cards</a></li><li class="navbar_navitem__3ICSG"><a href="/tutorials">tutorials</a></li><li class="navbar_navitem__3ICSG"><a href="/results">Results</a></li><li class="navbar_navitem__3ICSG"><a href="/papers">Papers</a></li><li class="navbar_navitem__3ICSG"><a href="/team">Team</a></li><li class="navbar_navitem__3ICSG"><a href="/nl_augmenter">NL-Augmenter</a></li><li class="navbar_navitem__3ICSG"><a href="/workshop">Workshop</a></li></ul></nav></div></header><div class="layout_container__2t4v2"><main><article><span class="utils_headingXl__1XecN">CommonGen</span><span class="utils_smallSpace__375iy"></span><span class="utils_lightText__12Ckm">Structure-to-text</span><div><h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#dataset-and-task-description">Dataset and Task Description</a>
<ul>
<li><a href="#dataset-and-task-summary">Dataset and Task Summary</a></li>
<li><a href="#why-is-this-dataset-part-of-gem">Why is this dataset part of GEM?</a></li>
<li><a href="#languages">Languages</a></li>
</ul>
</li>
<li><a href="#meta-information">Meta Information</a>
<ul>
<li><a href="#dataset-curators">Dataset Curators</a></li>
<li><a href="#licensing-information">Licensing Information</a></li>
<li><a href="#citation-information">Citation Information</a></li>
<li><a href="#leaderboard">Leaderboard</a></li>
</ul>
</li>
<li><a href="#dataset-structure">Dataset Structure</a>
<ul>
<li><a href="#data-instances">Data Instances</a></li>
<li><a href="#data-fields">Data Fields</a></li>
<li><a href="#data-statistics">Data Statistics</a></li>
</ul>
</li>
<li><a href="#dataset-creation">Dataset Creation</a>
<ul>
<li><a href="#curation-rationale">Curation Rationale</a></li>
<li><a href="#communicative-goal">Communicative Goal</a></li>
<li><a href="#source-data">Source Data</a>
<ul>
<li><a href="#initial-data-collection-and-normalization">Initial Data Collection and Normalization</a></li>
<li><a href="#who-are-the-source-language-producers">Who are the source language producers?</a></li>
</ul>
</li>
<li><a href="#annotations">Annotations</a>
<ul>
<li><a href="#annotation-process">Annotation process</a></li>
<li><a href="#who-are-the-annotators">Who are the annotators?</a></li>
</ul>
</li>
<li><a href="#personal-and-sensitive-information">Personal and Sensitive Information</a></li>
</ul>
</li>
<li><a href="#changes-to-the-original-dataset-for-gem">Changes to the Original Dataset for GEM</a>
<ul>
<li><a href="#special-test-sets">Special test sets</a>
<ul>
<li><a href="#data-shift">Data shift</a></li>
<li><a href="#transformations">Transformations</a></li>
<li><a href="#subpopulations">Subpopulations</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#considerations-for-using-the-data">Considerations for Using the Data</a>
<ul>
<li><a href="#social-impact-of-the-dataset">Social Impact of the Dataset</a></li>
<li><a href="#impact-on-underserved-communities">Impact on Underserved Communities</a></li>
<li><a href="#discussion-of-biases">Discussion of Biases</a></li>
<li><a href="#other-known-limitations">Other Known Limitations</a></li>
</ul>
</li>
<li><a href="#getting-started-with-in-depth-research-on-the-task">Getting started with in-depth research on the task</a></li>
<li><a href="#credits">Credits</a></li>
</ul>
<h2 id="dataset-and-task-description">Dataset and Task Description</h2>
<ul>
<li><strong>Homepage:</strong> <a href="https://inklab.usc.edu/CommonGen/">CommonGen Homepage</a></li>
<li><strong>Repository:</strong> <a href="https://github.com/INK-USC/CommonGen">CommonGen repository</a></li>
<li><strong>Paper:</strong> <a href="https://arxiv.org/abs/1911.03705">CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning</a></li>
<li><strong>Point of Contact:</strong> <a href="yuchen.lin@usc.edu">yuchen.lin@usc.edu</a></li>
</ul>
<h3 id="dataset-and-task-summary">Dataset and Task Summary</h3>
<p>CommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts; the task is to generate a coherent sentence describing an everyday scenario using these concepts.</p>
<p>The dataset was introduced in the paper <a href="https://arxiv.org/abs/1911.03705">CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning</a> published in the Findings of EMNLP 2020. It was built by selecting concept sets and corresponding training sentences from a set of image and video captioning datasets, and having Amazon Mechanical Turk crowd workers write additional sentences to provide references for the development and test set.</p>
<h3 id="why-is-this-dataset-part-of-gem">Why is this dataset part of GEM?</h3>
<p>CommonGen is a medium sized corpus with a unique reasoning challenge and interesting evaluation possibilities.</p>
<h3 id="languages">Languages</h3>
<p>CommonGen contains English text only (BCP-47: <code>en</code>).</p>
<h2 id="meta-information">Meta Information</h2>
<h3 id="dataset-curators">Dataset Curators</h3>
<p>The dataset was curated by a joint team of researchers from the University of Southern California and Allen Institute for Artificial Intelligence. The research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), the DARPA MCS program, and NSF SMA 18-29268.</p>
<h3 id="licensing-information">Licensing Information</h3>
<p>The dataset can be obtained via filing a form with the maintainers, without explicit licensing information. The repository code is under an <a href="https://github.com/INK-USC/CommonGen/blob/master/LICENSE">MIT license</a>.</p>
<h3 id="citation-information">Citation Information</h3>
<pre><code>@inproceedings{lin-etal-2020-commongen,
    title = "{C}ommon{G}en: A Constrained Text Generation Challenge for Generative Commonsense Reasoning",
    author = "Lin, Bill Yuchen  and
      Zhou, Wangchunshu  and
      Shen, Ming  and
      Zhou, Pei  and
      Bhagavatula, Chandra  and
      Choi, Yejin  and
      Ren, Xiang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.165",
    pages = "1823--1840",
}
</code></pre>
<h3 id="leaderboard">Leaderboard</h3>
<p>The dataset supports an active leaderboard, the best resukts are tracked <a href="https://inklab.usc.edu/CommonGen/leaderboard.html">here</a>. The model outputs are evaluated against the crowdsourced references, and ranked by SPICE score. The leaderboard also reports BLEU-4 and CIDEr scores.</p>
<h2 id="dataset-structure">Dataset Structure</h2>
<h3 id="data-instances">Data Instances</h3>
<p>The following are two examples sharing the same concept set:</p>
<pre><code>[
  {
    "concepts": ['ski', 'mountain', 'skier'],
    "target": 'Skier skis down the mountain',
  },
  {
    "concepts": ['ski', 'mountain', 'skier'],
    "target": 'Three skiers are skiing on a snowy mountain.',
  },
]
</code></pre>
<p>It should be noted that each concept-set/target pair constitutes an example, but the concept sets are repeated across various numbers of examples.</p>
<h3 id="data-fields">Data Fields</h3>
<p>A data instance has the following fields</p>
<ul>
<li><code>concepts</code>: a <code>list</code> of  <code>string</code> values denoting the concept the system should write about. Has 3 to 5 items, constitutes the <code>input</code> of the task.</li>
<li><code>target</code>: a sentence <code>string</code> mentioning all of the above mentioned <code>concepts</code>. Constitutes the desired <code>output</code> of the task.</li>
</ul>
<h3 id="data-statistics">Data Statistics</h3>
<p>Each example in the dataset consists of a set of 3 to 5 concepts denoted by a single noun, verb, or adjective (the input), and a sentence using these concepts (the output). The dataset provides several such sentences for each such concept.</p>
<table>
<thead>
<tr>
<th></th>
<th>Train</th>
<th>Dev</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Total concept-sets</strong></td>
<td>32,651</td>
<td>993</td>
<td>1,497</td>
</tr>
<tr>
<td><strong>Total sentences</strong></td>
<td>67,389</td>
<td>4,018</td>
<td>6,042</td>
</tr>
<tr>
<td><strong>Average sentence length</strong></td>
<td>10.54</td>
<td>11.55</td>
<td>13.34</td>
</tr>
</tbody>
</table>
<p>The dev and test set were created by sampling sets of concepts of size 4 or 5 (and as many of size 3 for the dev set) present in the source captioning datasets and having crowd-workers write reference sentences using these concepts.</p>
<p>Conversely, the training set has more concept sets of size 3 than of size 4 and 5, and uses the original captions from the source datasets as references.</p>
<p>The authors also ensured that the training, dev and test set have different combinations of unique concepts to ensure compositionality (details in <a href="https://arxiv.org/pdf/1911.03705v3.pdf">Table 1</a>).</p>
<h2 id="dataset-creation">Dataset Creation</h2>
<h3 id="curation-rationale">Curation Rationale</h3>
<p>The dataset creators selected sets of concepts that appeared in image and video captions (as identified by a POS tagger) to ensure that a likely real-world scenario including the set could be imagined and constructed. Section 3.1 of the <a href="https://arxiv.org/pdf/1911.03705v3.pdf">paper</a> describes a sampling scheme which encourages diversity of sets while selecting common concepts.</p>
<h3 id="communicative-goal">Communicative Goal</h3>
<p>The speaker is required to produce a <em>coherent</em> sentence which mentions all of the source concepts, and which describes a <em>likely</em> situation that could be captured in a picture or video.</p>
<h3 id="source-data">Source Data</h3>
<p>The dataset re-uses data from the following pre-existing resources:</p>
<ul>
<li>Image captioning datasets:
<ul>
<li><a href="https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00166">Flickr30k</a></li>
<li><a href="https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48">MSCOCO</a></li>
<li><a href="https://www.aclweb.org/anthology/P18-1238/">Conceptual Captions</a></li>
</ul>
</li>
<li>Video captioning datasets:
<ul>
<li><a href="https://link.springer.com/article/10.1007/s11263-016-0987-1">LSMDC</a></li>
<li><a href="https://openaccess.thecvf.com/content_iccv_2017/html/Krishna_Dense-Captioning_Events_in_ICCV_2017_paper.html">ActivityNet</a></li>
<li><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_VaTeX_A_Large-Scale_High-Quality_Multilingual_Dataset_for_Video-and-Language_Research_ICCV_2019_paper.html">VaTeX</a></li>
</ul>
</li>
</ul>
<p>We refer the reader to the papers describing these sources for further information.</p>
<h4 id="initial-data-collection-and-normalization">Initial Data Collection and Normalization</h4>
<p>The training data consists of concept sets and captions for the source datasets listed <a href="#source-data">above</a>. The concept sets are the sets of labels of the images or videos, selected with a heuristic to maximize diversity while ensuring that they represent likely scenarios.</p>
<p>The dev and test set sentences were created by Amazon Mechanical Turk crowd workers. The workers were shown an example generation and a set of 4 or 5 concept names along with their part-of-speech and asked to write:</p>
<ol>
<li>One sentence mentioning all of the concepts</li>
<li>A rationale explaining how the sentence connects the concept</li>
</ol>
<p>A screenshot of the interface is provided in Figure 7 of the <a href="https://arxiv.org/pdf/1911.03705v3.pdf">Appendix</a>.</p>
<h4 id="who-are-the-source-language-producers">Who are the source language producers?</h4>
<p>The original language of this dataset was created by Amazon Mechanical Turk workers.</p>
<p>During the data collection, workers who provided rationales that were too short, failed to have good coverage of the input in their sentences, or workers whose output had a high perplexity under a GPT-2 model were disqualified from the pool and replaced with newcomers.</p>
<p>No demographic information is provided.</p>
<h3 id="annotations">Annotations</h3>
<p>The dataset does not contain any additional annotations.</p>
<h4 id="annotation-process">Annotation process</h4>
<p>[N/A]</p>
<h4 id="who-are-the-annotators">Who are the annotators?</h4>
<p>[N/A]</p>
<h3 id="personal-and-sensitive-information">Personal and Sensitive Information</h3>
<p>The concepts are restricted to verbs, adjectives, and common nouns, and no personal information is given in the captions.</p>
<h2 id="changes-to-the-original-dataset-for-gem">Changes to the Original Dataset for GEM</h2>
<p>If the originally published dataset was modified in any way for GEM, please record the changes here. These could include data cleaning, exclusion of certain languages, changes to the data splits, additional challenge examples, among others.</p>
<h3 id="special-test-sets">Special test sets</h3>
<p>4 challenge sets for CommenGen were added to the GEM evaluation suite.</p>
<h4 id="data-shift">Data shift</h4>
<p>We created subsets of the training and development sets of ~500 randomly selected inputs each.</p>
<h4 id="transformations">Transformations</h4>
<p>We applied input scrambling on a subset of 500 randomly selected test instances; the order of the concepts was randomly reassigned.</p>
<h4 id="subpopulations">Subpopulations</h4>
<p>We created a subpopulation based on input length, taking into account the number of concepts the input test structures. By comparing inputs of different lengths, we can see to what extent systems are able to handle different input sizes</p>
<table>
<thead>
<tr>
<th>Concept number</th>
<th>Frequency English</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>747</td>
</tr>
<tr>
<td>5</td>
<td>750</td>
</tr>
</tbody>
</table>
<h2 id="considerations-for-using-the-data">Considerations for Using the Data</h2>
<h3 id="social-impact-of-the-dataset">Social Impact of the Dataset</h3>
<p>The task is presented as a stepping stone towards building models that achieve more human-like text generation. Progress in this research direction opens the way for strong quality of life improvements through applications such as virtual assistants or automatic image captioning, while also raising concern of misuse especially by agents who want to hide that a text is machine generated.</p>
<h3 id="impact-on-underserved-communities">Impact on Underserved Communities</h3>
<p>The dataset is in English, a language with an abundance of existing resources.</p>
<h3 id="discussion-of-biases">Discussion of Biases</h3>
<p>The dataset is created using data from image captioning systems and might inherit some of the social biases represented therein (see e.g. <a href="https://arxiv.org/abs/2006.08315">Tang et al. 2020</a>).</p>
<p>Another related concern is the exposure bias introduced by the initial selection of pictures and video, which are likely to over-represent situations that are common in the US at the expense of other parts of the world (Flickr, for example, is a US-based company founded in Canada). For more discussion of the potential impacts of exposure bias, see e.g. <a href="https://www.aclweb.org/anthology/P16-2096.pdf">The Social Impact of Natural Language Processing</a>.</p>
<p>The use of GPT-2 to validate development ant test sentences <a href="https://www.aclweb.org/anthology/D19-1339.pdf">might be cause for similar concern</a>, but we do note that the authors only use the model to discount very high perplexity sequences which is less likely to surface those biases.</p>
<h3 id="other-known-limitations">Other Known Limitations</h3>
<p>The language in the development and test set is crowdsourced, which means that it was written by workers whose main goal was speed. This is likely to impact the quality and variety of the targets. The population of crowdsource workers is also not identically distributed as the the base population of the locations the workers come from, which may lead to different representation of situations or underlying expectations of what these situations are.</p>
<h2 id="getting-started-with-in-depth-research-on-the-task">Getting started with in-depth research on the task</h2>
<ul>
<li>Two variants of <a href="https://arxiv.org/abs/1910.13461">BART</a>, <a href="https://arxiv.org/abs/2009.12677">Knowledge Graph augemnted-BART</a> and <a href="https://arxiv.org/abs/2012.00366">Enhanced Knowledge Injection Model for Commonsense Generation</a>, hold the top two spots on the leaderboard, followed by a fine-tuned <a href="https://arxiv.org/abs/1910.10683">T5 model</a>.</li>
<li>The following script shows how to download and load the data, fine-tune, and evaluate a model using the ROUGE, BLEU, and METEOR metrics: <a href="https://github.com/GEM-benchmark/GEM-baseline-models/blob/main/examples/GEM-common_gen.ipynb">GEM sample script</a>.</li>
</ul>
<h2 id="credits">Credits</h2>
<p>Data sheets were introduced by the following two publications:</p>
<ul>
<li><a href="https://www.aclweb.org/anthology/Q18-1041/">Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science</a>, Bender and Friedman</li>
<li><a href="https://arxiv.org/abs/1803.09010">Datasheets for Datasets</a>, Gebru et al.</li>
</ul>
<p>This guide and template is an NLG-specific variant of the one produced by <a href="https://github.com/huggingface/datasets/blob/master/templates/README_guide.md">HuggingFace</a>.</p>
</div></article></main><div class="layout_push__1J9g0"></div></div><footer class="layout_footer__127N0 utils_eggshell__Njxsh"><span class="layout_backToHome__1vZsp"><a href="/">‚Üê Home</a></span><span>If you have any questions, please join our <a href="https://groups.google.com/g/gem-benchmark" target="_blank" class="utils_accentUnderline__k083p">google group</a> for support.</span></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"taskData":{"id":"CommonGen","contentHtml":"\u003ch2 id=\"table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#dataset-and-task-description\"\u003eDataset and Task Description\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#dataset-and-task-summary\"\u003eDataset and Task Summary\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#why-is-this-dataset-part-of-gem\"\u003eWhy is this dataset part of GEM?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#languages\"\u003eLanguages\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#meta-information\"\u003eMeta Information\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#dataset-curators\"\u003eDataset Curators\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#licensing-information\"\u003eLicensing Information\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#citation-information\"\u003eCitation Information\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#leaderboard\"\u003eLeaderboard\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dataset-structure\"\u003eDataset Structure\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#data-instances\"\u003eData Instances\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#data-fields\"\u003eData Fields\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#data-statistics\"\u003eData Statistics\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dataset-creation\"\u003eDataset Creation\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#curation-rationale\"\u003eCuration Rationale\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#communicative-goal\"\u003eCommunicative Goal\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#source-data\"\u003eSource Data\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#initial-data-collection-and-normalization\"\u003eInitial Data Collection and Normalization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#who-are-the-source-language-producers\"\u003eWho are the source language producers?\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#annotations\"\u003eAnnotations\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#annotation-process\"\u003eAnnotation process\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#who-are-the-annotators\"\u003eWho are the annotators?\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#personal-and-sensitive-information\"\u003ePersonal and Sensitive Information\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#changes-to-the-original-dataset-for-gem\"\u003eChanges to the Original Dataset for GEM\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#special-test-sets\"\u003eSpecial test sets\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#data-shift\"\u003eData shift\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#transformations\"\u003eTransformations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#subpopulations\"\u003eSubpopulations\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#considerations-for-using-the-data\"\u003eConsiderations for Using the Data\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#social-impact-of-the-dataset\"\u003eSocial Impact of the Dataset\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#impact-on-underserved-communities\"\u003eImpact on Underserved Communities\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#discussion-of-biases\"\u003eDiscussion of Biases\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#other-known-limitations\"\u003eOther Known Limitations\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#getting-started-with-in-depth-research-on-the-task\"\u003eGetting started with in-depth research on the task\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#credits\"\u003eCredits\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"dataset-and-task-description\"\u003eDataset and Task Description\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHomepage:\u003c/strong\u003e \u003ca href=\"https://inklab.usc.edu/CommonGen/\"\u003eCommonGen Homepage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRepository:\u003c/strong\u003e \u003ca href=\"https://github.com/INK-USC/CommonGen\"\u003eCommonGen repository\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePaper:\u003c/strong\u003e \u003ca href=\"https://arxiv.org/abs/1911.03705\"\u003eCommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoint of Contact:\u003c/strong\u003e \u003ca href=\"yuchen.lin@usc.edu\"\u003eyuchen.lin@usc.edu\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"dataset-and-task-summary\"\u003eDataset and Task Summary\u003c/h3\u003e\n\u003cp\u003eCommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts; the task is to generate a coherent sentence describing an everyday scenario using these concepts.\u003c/p\u003e\n\u003cp\u003eThe dataset was introduced in the paper \u003ca href=\"https://arxiv.org/abs/1911.03705\"\u003eCommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning\u003c/a\u003e published in the Findings of EMNLP 2020. It was built by selecting concept sets and corresponding training sentences from a set of image and video captioning datasets, and having Amazon Mechanical Turk crowd workers write additional sentences to provide references for the development and test set.\u003c/p\u003e\n\u003ch3 id=\"why-is-this-dataset-part-of-gem\"\u003eWhy is this dataset part of GEM?\u003c/h3\u003e\n\u003cp\u003eCommonGen is a medium sized corpus with a unique reasoning challenge and interesting evaluation possibilities.\u003c/p\u003e\n\u003ch3 id=\"languages\"\u003eLanguages\u003c/h3\u003e\n\u003cp\u003eCommonGen contains English text only (BCP-47: \u003ccode\u003een\u003c/code\u003e).\u003c/p\u003e\n\u003ch2 id=\"meta-information\"\u003eMeta Information\u003c/h2\u003e\n\u003ch3 id=\"dataset-curators\"\u003eDataset Curators\u003c/h3\u003e\n\u003cp\u003eThe dataset was curated by a joint team of researchers from the University of Southern California and Allen Institute for Artificial Intelligence. The research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), the DARPA MCS program, and NSF SMA 18-29268.\u003c/p\u003e\n\u003ch3 id=\"licensing-information\"\u003eLicensing Information\u003c/h3\u003e\n\u003cp\u003eThe dataset can be obtained via filing a form with the maintainers, without explicit licensing information. The repository code is under an \u003ca href=\"https://github.com/INK-USC/CommonGen/blob/master/LICENSE\"\u003eMIT license\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"citation-information\"\u003eCitation Information\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{lin-etal-2020-commongen,\n    title = \"{C}ommon{G}en: A Constrained Text Generation Challenge for Generative Commonsense Reasoning\",\n    author = \"Lin, Bill Yuchen  and\n      Zhou, Wangchunshu  and\n      Shen, Ming  and\n      Zhou, Pei  and\n      Bhagavatula, Chandra  and\n      Choi, Yejin  and\n      Ren, Xiang\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.findings-emnlp.165\",\n    pages = \"1823--1840\",\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"leaderboard\"\u003eLeaderboard\u003c/h3\u003e\n\u003cp\u003eThe dataset supports an active leaderboard, the best resukts are tracked \u003ca href=\"https://inklab.usc.edu/CommonGen/leaderboard.html\"\u003ehere\u003c/a\u003e. The model outputs are evaluated against the crowdsourced references, and ranked by SPICE score. The leaderboard also reports BLEU-4 and CIDEr scores.\u003c/p\u003e\n\u003ch2 id=\"dataset-structure\"\u003eDataset Structure\u003c/h2\u003e\n\u003ch3 id=\"data-instances\"\u003eData Instances\u003c/h3\u003e\n\u003cp\u003eThe following are two examples sharing the same concept set:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[\n  {\n    \"concepts\": ['ski', 'mountain', 'skier'],\n    \"target\": 'Skier skis down the mountain',\n  },\n  {\n    \"concepts\": ['ski', 'mountain', 'skier'],\n    \"target\": 'Three skiers are skiing on a snowy mountain.',\n  },\n]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIt should be noted that each concept-set/target pair constitutes an example, but the concept sets are repeated across various numbers of examples.\u003c/p\u003e\n\u003ch3 id=\"data-fields\"\u003eData Fields\u003c/h3\u003e\n\u003cp\u003eA data instance has the following fields\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003econcepts\u003c/code\u003e: a \u003ccode\u003elist\u003c/code\u003e of  \u003ccode\u003estring\u003c/code\u003e values denoting the concept the system should write about. Has 3 to 5 items, constitutes the \u003ccode\u003einput\u003c/code\u003e of the task.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003etarget\u003c/code\u003e: a sentence \u003ccode\u003estring\u003c/code\u003e mentioning all of the above mentioned \u003ccode\u003econcepts\u003c/code\u003e. Constitutes the desired \u003ccode\u003eoutput\u003c/code\u003e of the task.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"data-statistics\"\u003eData Statistics\u003c/h3\u003e\n\u003cp\u003eEach example in the dataset consists of a set of 3 to 5 concepts denoted by a single noun, verb, or adjective (the input), and a sentence using these concepts (the output). The dataset provides several such sentences for each such concept.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eTrain\u003c/th\u003e\n\u003cth\u003eDev\u003c/th\u003e\n\u003cth\u003eTest\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eTotal concept-sets\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e32,651\u003c/td\u003e\n\u003ctd\u003e993\u003c/td\u003e\n\u003ctd\u003e1,497\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eTotal sentences\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e67,389\u003c/td\u003e\n\u003ctd\u003e4,018\u003c/td\u003e\n\u003ctd\u003e6,042\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eAverage sentence length\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e10.54\u003c/td\u003e\n\u003ctd\u003e11.55\u003c/td\u003e\n\u003ctd\u003e13.34\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eThe dev and test set were created by sampling sets of concepts of size 4 or 5 (and as many of size 3 for the dev set) present in the source captioning datasets and having crowd-workers write reference sentences using these concepts.\u003c/p\u003e\n\u003cp\u003eConversely, the training set has more concept sets of size 3 than of size 4 and 5, and uses the original captions from the source datasets as references.\u003c/p\u003e\n\u003cp\u003eThe authors also ensured that the training, dev and test set have different combinations of unique concepts to ensure compositionality (details in \u003ca href=\"https://arxiv.org/pdf/1911.03705v3.pdf\"\u003eTable 1\u003c/a\u003e).\u003c/p\u003e\n\u003ch2 id=\"dataset-creation\"\u003eDataset Creation\u003c/h2\u003e\n\u003ch3 id=\"curation-rationale\"\u003eCuration Rationale\u003c/h3\u003e\n\u003cp\u003eThe dataset creators selected sets of concepts that appeared in image and video captions (as identified by a POS tagger) to ensure that a likely real-world scenario including the set could be imagined and constructed. Section 3.1 of the \u003ca href=\"https://arxiv.org/pdf/1911.03705v3.pdf\"\u003epaper\u003c/a\u003e describes a sampling scheme which encourages diversity of sets while selecting common concepts.\u003c/p\u003e\n\u003ch3 id=\"communicative-goal\"\u003eCommunicative Goal\u003c/h3\u003e\n\u003cp\u003eThe speaker is required to produce a \u003cem\u003ecoherent\u003c/em\u003e sentence which mentions all of the source concepts, and which describes a \u003cem\u003elikely\u003c/em\u003e situation that could be captured in a picture or video.\u003c/p\u003e\n\u003ch3 id=\"source-data\"\u003eSource Data\u003c/h3\u003e\n\u003cp\u003eThe dataset re-uses data from the following pre-existing resources:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImage captioning datasets:\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00166\"\u003eFlickr30k\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48\"\u003eMSCOCO\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.aclweb.org/anthology/P18-1238/\"\u003eConceptual Captions\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eVideo captioning datasets:\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://link.springer.com/article/10.1007/s11263-016-0987-1\"\u003eLSMDC\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://openaccess.thecvf.com/content_iccv_2017/html/Krishna_Dense-Captioning_Events_in_ICCV_2017_paper.html\"\u003eActivityNet\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_VaTeX_A_Large-Scale_High-Quality_Multilingual_Dataset_for_Video-and-Language_Research_ICCV_2019_paper.html\"\u003eVaTeX\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe refer the reader to the papers describing these sources for further information.\u003c/p\u003e\n\u003ch4 id=\"initial-data-collection-and-normalization\"\u003eInitial Data Collection and Normalization\u003c/h4\u003e\n\u003cp\u003eThe training data consists of concept sets and captions for the source datasets listed \u003ca href=\"#source-data\"\u003eabove\u003c/a\u003e. The concept sets are the sets of labels of the images or videos, selected with a heuristic to maximize diversity while ensuring that they represent likely scenarios.\u003c/p\u003e\n\u003cp\u003eThe dev and test set sentences were created by Amazon Mechanical Turk crowd workers. The workers were shown an example generation and a set of 4 or 5 concept names along with their part-of-speech and asked to write:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eOne sentence mentioning all of the concepts\u003c/li\u003e\n\u003cli\u003eA rationale explaining how the sentence connects the concept\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eA screenshot of the interface is provided in Figure 7 of the \u003ca href=\"https://arxiv.org/pdf/1911.03705v3.pdf\"\u003eAppendix\u003c/a\u003e.\u003c/p\u003e\n\u003ch4 id=\"who-are-the-source-language-producers\"\u003eWho are the source language producers?\u003c/h4\u003e\n\u003cp\u003eThe original language of this dataset was created by Amazon Mechanical Turk workers.\u003c/p\u003e\n\u003cp\u003eDuring the data collection, workers who provided rationales that were too short, failed to have good coverage of the input in their sentences, or workers whose output had a high perplexity under a GPT-2 model were disqualified from the pool and replaced with newcomers.\u003c/p\u003e\n\u003cp\u003eNo demographic information is provided.\u003c/p\u003e\n\u003ch3 id=\"annotations\"\u003eAnnotations\u003c/h3\u003e\n\u003cp\u003eThe dataset does not contain any additional annotations.\u003c/p\u003e\n\u003ch4 id=\"annotation-process\"\u003eAnnotation process\u003c/h4\u003e\n\u003cp\u003e[N/A]\u003c/p\u003e\n\u003ch4 id=\"who-are-the-annotators\"\u003eWho are the annotators?\u003c/h4\u003e\n\u003cp\u003e[N/A]\u003c/p\u003e\n\u003ch3 id=\"personal-and-sensitive-information\"\u003ePersonal and Sensitive Information\u003c/h3\u003e\n\u003cp\u003eThe concepts are restricted to verbs, adjectives, and common nouns, and no personal information is given in the captions.\u003c/p\u003e\n\u003ch2 id=\"changes-to-the-original-dataset-for-gem\"\u003eChanges to the Original Dataset for GEM\u003c/h2\u003e\n\u003cp\u003eIf the originally published dataset was modified in any way for GEM, please record the changes here. These could include data cleaning, exclusion of certain languages, changes to the data splits, additional challenge examples, among others.\u003c/p\u003e\n\u003ch3 id=\"special-test-sets\"\u003eSpecial test sets\u003c/h3\u003e\n\u003cp\u003e4 challenge sets for CommenGen were added to the GEM evaluation suite.\u003c/p\u003e\n\u003ch4 id=\"data-shift\"\u003eData shift\u003c/h4\u003e\n\u003cp\u003eWe created subsets of the training and development sets of ~500 randomly selected inputs each.\u003c/p\u003e\n\u003ch4 id=\"transformations\"\u003eTransformations\u003c/h4\u003e\n\u003cp\u003eWe applied input scrambling on a subset of 500 randomly selected test instances; the order of the concepts was randomly reassigned.\u003c/p\u003e\n\u003ch4 id=\"subpopulations\"\u003eSubpopulations\u003c/h4\u003e\n\u003cp\u003eWe created a subpopulation based on input length, taking into account the number of concepts the input test structures. By comparing inputs of different lengths, we can see to what extent systems are able to handle different input sizes\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eConcept number\u003c/th\u003e\n\u003cth\u003eFrequency English\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e747\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e750\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"considerations-for-using-the-data\"\u003eConsiderations for Using the Data\u003c/h2\u003e\n\u003ch3 id=\"social-impact-of-the-dataset\"\u003eSocial Impact of the Dataset\u003c/h3\u003e\n\u003cp\u003eThe task is presented as a stepping stone towards building models that achieve more human-like text generation. Progress in this research direction opens the way for strong quality of life improvements through applications such as virtual assistants or automatic image captioning, while also raising concern of misuse especially by agents who want to hide that a text is machine generated.\u003c/p\u003e\n\u003ch3 id=\"impact-on-underserved-communities\"\u003eImpact on Underserved Communities\u003c/h3\u003e\n\u003cp\u003eThe dataset is in English, a language with an abundance of existing resources.\u003c/p\u003e\n\u003ch3 id=\"discussion-of-biases\"\u003eDiscussion of Biases\u003c/h3\u003e\n\u003cp\u003eThe dataset is created using data from image captioning systems and might inherit some of the social biases represented therein (see e.g. \u003ca href=\"https://arxiv.org/abs/2006.08315\"\u003eTang et al. 2020\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eAnother related concern is the exposure bias introduced by the initial selection of pictures and video, which are likely to over-represent situations that are common in the US at the expense of other parts of the world (Flickr, for example, is a US-based company founded in Canada). For more discussion of the potential impacts of exposure bias, see e.g. \u003ca href=\"https://www.aclweb.org/anthology/P16-2096.pdf\"\u003eThe Social Impact of Natural Language Processing\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe use of GPT-2 to validate development ant test sentences \u003ca href=\"https://www.aclweb.org/anthology/D19-1339.pdf\"\u003emight be cause for similar concern\u003c/a\u003e, but we do note that the authors only use the model to discount very high perplexity sequences which is less likely to surface those biases.\u003c/p\u003e\n\u003ch3 id=\"other-known-limitations\"\u003eOther Known Limitations\u003c/h3\u003e\n\u003cp\u003eThe language in the development and test set is crowdsourced, which means that it was written by workers whose main goal was speed. This is likely to impact the quality and variety of the targets. The population of crowdsource workers is also not identically distributed as the the base population of the locations the workers come from, which may lead to different representation of situations or underlying expectations of what these situations are.\u003c/p\u003e\n\u003ch2 id=\"getting-started-with-in-depth-research-on-the-task\"\u003eGetting started with in-depth research on the task\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eTwo variants of \u003ca href=\"https://arxiv.org/abs/1910.13461\"\u003eBART\u003c/a\u003e, \u003ca href=\"https://arxiv.org/abs/2009.12677\"\u003eKnowledge Graph augemnted-BART\u003c/a\u003e and \u003ca href=\"https://arxiv.org/abs/2012.00366\"\u003eEnhanced Knowledge Injection Model for Commonsense Generation\u003c/a\u003e, hold the top two spots on the leaderboard, followed by a fine-tuned \u003ca href=\"https://arxiv.org/abs/1910.10683\"\u003eT5 model\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eThe following script shows how to download and load the data, fine-tune, and evaluate a model using the ROUGE, BLEU, and METEOR metrics: \u003ca href=\"https://github.com/GEM-benchmark/GEM-baseline-models/blob/main/examples/GEM-common_gen.ipynb\"\u003eGEM sample script\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"credits\"\u003eCredits\u003c/h2\u003e\n\u003cp\u003eData sheets were introduced by the following two publications:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.aclweb.org/anthology/Q18-1041/\"\u003eData Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science\u003c/a\u003e, Bender and Friedman\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1803.09010\"\u003eDatasheets for Datasets\u003c/a\u003e, Gebru et al.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis guide and template is an NLG-specific variant of the one produced by \u003ca href=\"https://github.com/huggingface/datasets/blob/master/templates/README_guide.md\"\u003eHuggingFace\u003c/a\u003e.\u003c/p\u003e\n","title":"CommonGen","type":"Structure-to-text","motivation":"A medium sized corpus with a unique reasoning challenge and interesting evaluation possibilities."}},"__N_SSG":true},"page":"/data_cards/[id]","query":{"id":"CommonGen"},"buildId":"Nx8QlaATgB-argoYvzlf6","nextExport":false,"isFallback":false,"gsp":true,"head":[["meta",{"name":"viewport","content":"width=device-width"}],["meta",{"charSet":"utf-8"}],["link",{"rel":"icon","href":"/favicon.ico"}],["meta",{"name":"description","content":"Benchmark natural language generation systems with GEM."}],["meta",{"property":"og:image","content":"https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light\u0026md=1\u0026fontSize=100px\u0026images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"}],["meta",{"name":"og:title","content":"GEM"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["title",{"children":"GEM CommonGen"}]]}</script><script nomodule="" src="/_next/static/chunks/polyfills-e69cc13a7e89296a69e4.js"></script><script src="/_next/static/chunks/main-47bc8f80085b54a800da.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.baa41d4dbf5d52db897c.js" async=""></script><script src="/_next/static/chunks/e70fad557dfa42f32a11d0d2c99fe8f6e8d1fa86.4a36a385313236c59b19.js" async=""></script><script src="/_next/static/chunks/pages/_app-a9ae7a6d1de4e51a7ab6.js" async=""></script><script src="/_next/static/chunks/cb1608f2.c3a9f0eb95374ca4919a.js" async=""></script><script src="/_next/static/chunks/451c6be158cef50d8cc28b919cf08d1e5b9ff3fc.f0ec181e43727e8a893e.js" async=""></script><script src="/_next/static/chunks/pages/data_cards/%5Bid%5D-e1361c13d80af81deda6.js" async=""></script><script src="/_next/static/Nx8QlaATgB-argoYvzlf6/_buildManifest.js" async=""></script><script src="/_next/static/Nx8QlaATgB-argoYvzlf6/_ssgManifest.js" async=""></script></body></html>