<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="Benchmark natural language generation systems with GEM."/><meta property="og:image" content="https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"/><meta name="og:title" content="GEM"/><meta name="twitter:card" content="summary_large_image"/><title>GEM Wiki-Auto</title><link rel="preload" href="/_next/static/css/2786522978a02f025205.css" as="style"/><link rel="stylesheet" href="/_next/static/css/2786522978a02f025205.css" data-n-g=""/><link rel="preload" href="/_next/static/css/f2fce7b83fe6ca04479b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f2fce7b83fe6ca04479b.css" data-n-p=""/><noscript data-n-css="true"></noscript><link rel="preload" href="/_next/static/chunks/main-47bc8f80085b54a800da.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.baa41d4dbf5d52db897c.js" as="script"/><link rel="preload" href="/_next/static/chunks/e70fad557dfa42f32a11d0d2c99fe8f6e8d1fa86.4a36a385313236c59b19.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-a9ae7a6d1de4e51a7ab6.js" as="script"/><link rel="preload" href="/_next/static/chunks/cb1608f2.c3a9f0eb95374ca4919a.js" as="script"/><link rel="preload" href="/_next/static/chunks/451c6be158cef50d8cc28b919cf08d1e5b9ff3fc.f0ec181e43727e8a893e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/data_cards/%5Bid%5D-e1361c13d80af81deda6.js" as="script"/></head><body><div id="__next"><div class="layout_background__1AVEa undefined"><header class="layout_header__2rhWq"><div class="navbar_navwrapper__15zia"><div class="navbar_gradbar__1Xi5u"></div><nav class="navbar_navbar__3gnco"><span class="utils_headingLg__de7p0 navbar_navbarlogo__PLEwr"><a href="/">GEM BENCHMARK</a></span><div class="navbar_menutoggle__358pJ" id="mobile-menu"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars fa-w-14 navbar_bar__QVPSR" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"></path></svg></div><ul><li class="navbar_navitem__3ICSG navbar_pushright__3G2DM"><a href="/resources">Resources</a></li><li class="navbar_navitem__3ICSG"><a href="/data_cards">Data Cards</a></li><li class="navbar_navitem__3ICSG"><a href="/model_cards">Model Cards</a></li><li class="navbar_navitem__3ICSG"><a href="/tutorials">tutorials</a></li><li class="navbar_navitem__3ICSG"><a href="/results">Results</a></li><li class="navbar_navitem__3ICSG"><a href="/papers">Papers</a></li><li class="navbar_navitem__3ICSG"><a href="/team">Team</a></li><li class="navbar_navitem__3ICSG"><a href="/nl_augmenter">NL-Augmenter</a></li><li class="navbar_navitem__3ICSG"><a href="/workshop">Workshop</a></li></ul></nav></div></header><div class="layout_container__2t4v2"><main><article><span class="utils_headingXl__1XecN">Wiki-Auto</span><span class="utils_smallSpace__375iy"></span><span class="utils_lightText__12Ckm">Simplification</span><div><h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#dataset-description">Dataset Description</a>
<ul>
<li><a href="#dataset-and-task-summary">Dataset and Task Summary</a></li>
<li><a href="#why-is-this-dataset-part-of-gem">Why is this dataset part of GEM?</a></li>
<li><a href="#languages">Languages</a></li>
</ul>
</li>
<li><a href="#meta-information">Meta Information</a>
<ul>
<li><a href="#dataset-curators">Dataset Curators</a></li>
<li><a href="#licensing-information">Licensing Information</a></li>
<li><a href="#citation-information">Citation Information</a></li>
<li><a href="#leaderboard">Leaderboard</a></li>
</ul>
</li>
<li><a href="#dataset-structure">Dataset Structure</a>
<ul>
<li><a href="#data-instances">Data Instances</a></li>
<li><a href="#data-fields">Data Fields</a></li>
<li><a href="#data-statistics">Data Statistics</a></li>
</ul>
</li>
<li><a href="#dataset-creation">Dataset Creation</a>
<ul>
<li><a href="#curation-rationale">Curation Rationale</a></li>
<li><a href="#communicative-goal">Communicative Goal</a></li>
<li><a href="#source-data">Source Data</a>
<ul>
<li><a href="#initial-data-collection-and-normalization">Initial Data Collection and Normalization</a></li>
<li><a href="#who-are-the-source-language-producers">Who are the source language producers?</a></li>
</ul>
</li>
<li><a href="#annotations">Annotations</a>
<ul>
<li><a href="#annotation-process">Annotation process</a></li>
<li><a href="#who-are-the-annotators">Who are the annotators?</a></li>
</ul>
</li>
<li><a href="#personal-and-sensitive-information">Personal and Sensitive Information</a></li>
</ul>
</li>
<li><a href="#changes-to-the-original-dataset-for-gem">Changes to the Original Dataset for GEM</a></li>
<li><a href="#considerations-for-using-the-data">Considerations for Using the Data</a>
<ul>
<li><a href="#social-impact-of-the-dataset">Social Impact of the Dataset</a></li>
<li><a href="#impact-on-underserved-communities">Impact on Underserved Communities</a></li>
<li><a href="#discussion-of-biases">Discussion of Biases</a></li>
<li><a href="#other-known-limitations">Other Known Limitations</a></li>
</ul>
</li>
<li><a href="#getting-started-with-in-depth-research-on-the-task">Getting started with in-depth research on the task</a></li>
</ul>
<h2 id="dataset-description">Dataset Description</h2>
<ul>
<li><strong>Homepage:</strong> None (See <strong>Repository</strong>)</li>
<li><strong>Repository:</strong> <a href="https://github.com/chaojiang06/wiki-auto">Wiki-Auto repository</a></li>
<li><strong>Paper:</strong> <a href="https://www.aclweb.org/anthology/2020.acl-main.709.pdf">Neural CRF Model for Sentence Alignment in Text Simplification</a></li>
<li><strong>Point of Contact:</strong> <a href="jiang.1530@osu.edu">Chao Jiang</a></li>
</ul>
<h3 id="dataset-and-task-summary">Dataset and Task Summary</h3>
<p>WikiAuto provides a set of aligned sentences from English Wikipedia and Simple English Wikipedia as a resource to train sentence simplification systems.</p>
<p>The authors first crowd-sourced a set of manual alignments between sentences in a subset of the Simple English Wikipedia and their corresponding versions in English Wikipedia (this corresponds to the <code>manual</code> config in this version of the dataset), then trained a neural CRF system to predict these alignments.</p>
<p>The trained alignment prediction model was then applied to the other articles in Simple English Wikipedia with an English counterpart to create a larger corpus of aligned sentences (corresponding to the <code>auto</code> and <code>auto_acl</code> configs here).</p>
<h3 id="why-is-this-dataset-part-of-gem">Why is this dataset part of GEM?</h3>
<p>Wiki-Auto is the largest open text simplification dataset currently available. It is the training dataset for the text simplification task in GEM.</p>
<h3 id="languages">Languages</h3>
<p>Wiki-Auto contains English text only (BCP-47: <code>en</code>). It is presented as a translation task where Wikipedia Simple English is treated as its own idiom. For a statement of what is intended (but not always observed) to constitute Simple English on this platform, see <a href="https://simple.wikipedia.org/wiki/Wikipedia:About#Simple_English">Simple English in Wikipedia</a>.</p>
<h2 id="meta-information">Meta Information</h2>
<h3 id="dataset-curators">Dataset Curators</h3>
<p>The dataset was created by Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu from Ohio State University. The research is based upon work supported in part by the NSF awards IIS-1755898 and IIS-1822754, ODNI and  IARPA  via the  BETTER  program contract 19051600004, ARO and DARPA via the Social-Sim program contract W911NF-17-C-0095, Figure Eight AI for Everyone Award, and Criteo Faculty Research Award to Wei Xu.</p>
<h3 id="licensing-information">Licensing Information</h3>
<p>The dataset is not licensed by itself, but the source Wikipedia data is under a <code>cc-by-sa-3.0</code> license.</p>
<h3 id="citation-information">Citation Information</h3>
<pre><code>@inproceedings{jiang-etal-2020-neural,
    title = "Neural {CRF} Model for Sentence Alignment in Text Simplification",
    author = "Jiang, Chao  and
      Maddela, Mounica  and
      Lan, Wuwei  and
      Zhong, Yang  and
      Xu, Wei",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.709",
    doi = "10.18653/v1/2020.acl-main.709",
    pages = "7943--7960",
}

</code></pre>
<h3 id="leaderboard">Leaderboard</h3>
<p>There is no official leaderboard associated with Wiki-Auto.</p>
<h2 id="dataset-structure">Dataset Structure</h2>
<h3 id="data-instances">Data Instances</h3>
<p>The data in all of the configurations look a little different.</p>
<p>A <code>manual</code> config instance consists of a sentence from the Simple English Wikipedia article, one from the linked English Wikipedia article, IDs for each of them, and a label indicating whether they are aligned. Sentences on either side can be repeated so that the aligned sentences are in the same instances. For example:</p>
<pre><code>{'alignment_label': 1,
 'normal_sentence': 'The Local Government Act 1985 is an Act of Parliament in the United Kingdom.',
 'normal_sentence_id': '0_66252-1-0-0',
 'simple_sentence': 'The Local Government Act 1985 was an Act of Parliament in the United Kingdom.',
 'simple_sentence_id': '0_66252-0-0-0'}
</code></pre>
<p>Is followed by</p>
<pre><code>{'alignment_label': 0,
 'normal_sentence': 'Its main effect was to abolish the six county councils of the metropolitan counties that had been set up in 1974, 11 years earlier, by the Local Government Act 1972, along with the Greater London Council that had been established in 1965.',
 'normal_sentence_id': '0_66252-1-0-1',
 'simple_sentence': 'The Local Government Act 1985 was an Act of Parliament in the United Kingdom.',
 'simple_sentence_id': '0_66252-0-0-0'}
</code></pre>
<p>The <code>auto</code> config shows a pair of an English and corresponding Simple English Wikipedia as an instance, with an alignment at the paragraph and sentence level:</p>
<pre><code>{'example_id': '0',
 'normal': {'normal_article_content': {'normal_sentence': ["Lata Mondal ( ; born: 16 January 1993, Dhaka) is a Bangladeshi cricketer who plays for the Bangladesh national women's cricket team.",
    'She is a right handed batter.',
    'Mondal was born on January 16, 1993 in Dhaka, Bangladesh.',
    "Mondal made her ODI career against the Ireland women's cricket team on November 26, 2011.",
    "Mondal made her T20I career against the Ireland women's cricket team on August 28, 2012.",
    "In October 2018, she was named in Bangladesh's squad for the 2018 ICC Women's World Twenty20 tournament in the West Indies.",
    "Mondal was a member of the team that won a silver medal in cricket against the China national women's cricket team at the 2010 Asian Games in Guangzhou, China."],
   'normal_sentence_id': ['normal-41918715-0-0',
    'normal-41918715-0-1',
    'normal-41918715-1-0',
    'normal-41918715-2-0',
    'normal-41918715-3-0',
    'normal-41918715-3-1',
    'normal-41918715-4-0']},
  'normal_article_id': 41918715,
  'normal_article_title': 'Lata Mondal',
  'normal_article_url': 'https://en.wikipedia.org/wiki?curid=41918715'},
 'paragraph_alignment': {'normal_paragraph_id': ['normal-41918715-0'],
  'simple_paragraph_id': ['simple-702227-0']},
 'sentence_alignment': {'normal_sentence_id': ['normal-41918715-0-0',
   'normal-41918715-0-1'],
  'simple_sentence_id': ['simple-702227-0-0', 'simple-702227-0-1']},
 'simple': {'simple_article_content': {'simple_sentence': ["Lata Mondal (born: 16 January 1993) is a Bangladeshi cricketer who plays for the Bangladesh national women's cricket team.",
    'She is a right handed bat.'],
   'simple_sentence_id': ['simple-702227-0-0', 'simple-702227-0-1']},
  'simple_article_id': 702227,
  'simple_article_title': 'Lata Mondal',
  'simple_article_url': 'https://simple.wikipedia.org/wiki?curid=702227'}}
</code></pre>
<p>Finally, the <code>auto_acl</code>, the <code>auto_full_no_split</code>, and the <code>auto_full_with_split</code> configs were obtained by selecting the aligned pairs of sentences from <code>auto</code> to provide a ready-to-go aligned dataset to train a sequence-to-sequence system. While <code>auto_acl</code> corresponds to the filtered version of the data used to train the systems in the paper, <code>auto_full_no_split</code> and <code>auto_full_with_split</code> correspond to the unfiltered versions with and without sentence splits respectively. In the <code>auto_full_with_split</code> config, we join the sentences in the simple article mapped to the same sentence in the complex article to capture sentence splitting. Split sentences are seperated by a <code>&#x3C;SEP></code> token. In the <code>auto_full_no_split config</code>, we do not join the splits and treat them as seperate pairs. An instance is a single pair of sentences:</p>
<pre><code>{'normal_sentence': 'In early work, Rutherford discovered the concept of radioactive half-life , the radioactive element radon, and differentiated and named alpha and beta radiation .\n',
 'simple_sentence': 'Rutherford discovered the radioactive half-life, and the three parts of radiation which he named Alpha, Beta, and Gamma.\n'}
</code></pre>
<p>Thus, for training a text simplification model for GEM, the data with the <code>auto_acl</code> config can be directly used.</p>
<h3 id="data-fields">Data Fields</h3>
<p>The data has the following field:</p>
<ul>
<li><code>normal_sentence</code>: a sentence from English Wikipedia.</li>
<li><code>normal_sentence_id</code>: a unique ID for each English Wikipedia sentence. The last two dash-separated numbers correspond to the paragraph number in the article and the sentence number in the paragraph.</li>
<li><code>simple_sentence</code>: a sentence from Simple English Wikipedia.</li>
<li><code>simple_sentence_id</code>: a unique ID for each Simple English Wikipedia sentence. The last two dash-separated numbers correspond to the paragraph number in the article and the sentence number in the paragraph.</li>
<li><code>alignment_label</code>: signifies whether a pair of sentences is aligned: labels are <code>>=1:aligned</code> and <code>0:notAligned</code></li>
<li><code>paragraph_alignment</code>: a first step of alignment mapping English and Simple English paragraphs from linked articles</li>
<li><code>sentence_alignment</code>: the full alignment mapping English and Simple English sentences from linked articles</li>
</ul>
<h3 id="data-statistics">Data Statistics</h3>
<p>In <code>auto</code>, the <code>part_2</code> split corresponds to the articles used in <code>manual</code>, and <code>part_1</code> has the rest of Wikipedia.</p>
<p>The <code>manual</code> config is provided with a <code>train</code>/<code>dev</code>/<code>test</code> split with the following amounts of data:</p>
<table>
<thead>
<tr>
<th></th>
<th>Tain</th>
<th>Dev</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Total sentence pairs</td>
<td>373801</td>
<td>73249</td>
<td>118074</td>
</tr>
<tr>
<td>Aligned sentence pairs</td>
<td>1889</td>
<td>346</td>
<td>677</td>
</tr>
</tbody>
</table>
<p>The <code>auto_acl</code> has 488,332 complex-sentence pairs that are to be used for training the model. The average sentence length for complex and simple sentences are 26.6 and 18.7 respectively.</p>
<h2 id="dataset-creation">Dataset Creation</h2>
<h3 id="curation-rationale">Curation Rationale</h3>
<p>Wiki-Auto provides a new version of the Wikipedia corpus that is larger, contains 75% less defective pairs and has more complex rewrites than the previous WIKILARGE dataset.</p>
<h3 id="communicative-goal">Communicative Goal</h3>
<p>The goal is to communicate the same information as the source sentence using simpler words and grammar.</p>
<h3 id="source-data">Source Data</h3>
<h4 id="initial-data-collection-and-normalization">Initial Data Collection and Normalization</h4>
<p>The authors mention that they "extracted 138,095 article pairs from the 2019/09 Wikipedia dump using an improved version of the <a href="https://github.com/attardi/wikiextractor">WikiExtractor</a> library". The <a href="https://spacy.io/">SpaCy</a> library is used for sentence splitting.</p>
<h4 id="who-are-the-source-language-producers">Who are the source language producers?</h4>
<p>The dataset uses language from Wikipedia: some demographic information is provided <a href="https://en.wikipedia.org/wiki/Wikipedia:Who_writes_Wikipedia%3F">here</a>.</p>
<h3 id="annotations">Annotations</h3>
<h4 id="annotation-process">Annotation process</h4>
<p>Sentence alignment labels were crowdsourced for 500 randomly sampled document pairs (10,123 sentence pairs total). The authors pre-selected several alignment candidates from English Wikipedia for each Simple Wikipedia sentence based on various similarity metrics, then asked the crowd-workers to annotate these pairs. Finally, they trained their alignment model on this manually annotated dataset to obtain automatically aligned sentences (138,095 document pairs, 488,332 sentence pairs).</p>
<h4 id="who-are-the-annotators">Who are the annotators?</h4>
<p>No demographic annotation is provided for the crowd workers. The <a href="https://www.figure-eight.com/">Figure Eight</a> platform was used for the annotation process.</p>
<h3 id="personal-and-sensitive-information">Personal and Sensitive Information</h3>
<p>Since the dataset is created from Wikipedia/Simple Wikipedia, all the information contained in the dataset is already in the public domain.</p>
<h2 id="changes-to-the-original-dataset-for-gem">Changes to the Original Dataset for GEM</h2>
<p>No change is made to the original dataset.</p>
<h2 id="considerations-for-using-the-data">Considerations for Using the Data</h2>
<h3 id="social-impact-of-the-dataset">Social Impact of the Dataset</h3>
<p>The dataset helps move forward the research towards text simplification by creating a larger and more accurate dataset. Progress in text simplification in turn has the potential to increase the accessibility of written documents to wider audiences.</p>
<h3 id="impact-on-underserved-communities">Impact on Underserved Communities</h3>
<p>The dataset is in English, a language with an abundance of existing resources.</p>
<h3 id="discussion-of-biases">Discussion of Biases</h3>
<p>The dataset may contain some social biases, as the input sentences are based on Wikipedia. Studies have shown that the English Wikipedia contains both gender biases <a href="https://research.tudelft.nl/en/publications/is-wikipedia-succeeding-in-reducing-gender-bias-assessing-changes">(Schmahl et al., 2020)</a> and racial biases <a href="https://journals.sagepub.com/doi/pdf/10.1177/2378023118823946">(Adams et al., 2019)</a>.</p>
<h3 id="other-known-limitations">Other Known Limitations</h3>
<p>Since the data is created using an automatic alignment model (which is not perfect) there could be still some alignment issues in the data.</p>
<h2 id="getting-started-with-in-depth-research-on-the-task">Getting started with in-depth research on the task</h2>
<p>The dataset can be downloaded from the original repository <a href="https://github.com/chaojiang06/wiki-auto">(here)</a> by the authors or can also be used via <a href="https://huggingface.co/datasets/wiki_auto">HuggingFace</a> and <a href="https://www.tensorflow.org/datasets/overview">TFDS</a>.</p>
<p>The dataset repository provided by the authors also contains instructions to load a transformer-based sequence-to-sequence model trained on the dataset. There are also other recent supervised (<a href="https://arxiv.org/abs/1910.02677">Martin et al., 2019</a>, <a href="https://www.aclweb.org/anthology/N19-1317/">Kriz et al., 2019</a>, <a href="https://www.aclweb.org/anthology/P19-1331/">Dong et al., 2019</a>, <a href="https://www.aclweb.org/anthology/D17-1062/">Zhang and Lapata, 2017</a>) and unsupervised (<a href="https://arxiv.org/abs/2005.00352v1">Martin et al., 2020</a>, <a href="https://www.aclweb.org/anthology/2020.acl-main.707/">Kumar et al., 2020</a>, <a href="https://www.aclweb.org/anthology/P19-1198/">Surya et al., 2019</a>) text simplification models that can be used as baselines.</p>
<p>The common metric used for automatic evaluation is SARI <a href="https://www.aclweb.org/anthology/Q16-1029/">(Xu et al., 2016)</a>.</p>
</div></article></main><div class="layout_push__1J9g0"></div></div><footer class="layout_footer__127N0 utils_eggshell__Njxsh"><span class="layout_backToHome__1vZsp"><a href="/">← Home</a></span><span>If you have any questions, please join our <a href="https://groups.google.com/g/gem-benchmark" target="_blank" class="utils_accentUnderline__k083p">google group</a> for support.</span></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"taskData":{"id":"Wiki-Auto","contentHtml":"\u003ch2 id=\"table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#dataset-description\"\u003eDataset Description\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#dataset-and-task-summary\"\u003eDataset and Task Summary\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#why-is-this-dataset-part-of-gem\"\u003eWhy is this dataset part of GEM?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#languages\"\u003eLanguages\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#meta-information\"\u003eMeta Information\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#dataset-curators\"\u003eDataset Curators\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#licensing-information\"\u003eLicensing Information\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#citation-information\"\u003eCitation Information\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#leaderboard\"\u003eLeaderboard\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dataset-structure\"\u003eDataset Structure\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#data-instances\"\u003eData Instances\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#data-fields\"\u003eData Fields\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#data-statistics\"\u003eData Statistics\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dataset-creation\"\u003eDataset Creation\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#curation-rationale\"\u003eCuration Rationale\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#communicative-goal\"\u003eCommunicative Goal\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#source-data\"\u003eSource Data\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#initial-data-collection-and-normalization\"\u003eInitial Data Collection and Normalization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#who-are-the-source-language-producers\"\u003eWho are the source language producers?\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#annotations\"\u003eAnnotations\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#annotation-process\"\u003eAnnotation process\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#who-are-the-annotators\"\u003eWho are the annotators?\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#personal-and-sensitive-information\"\u003ePersonal and Sensitive Information\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#changes-to-the-original-dataset-for-gem\"\u003eChanges to the Original Dataset for GEM\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#considerations-for-using-the-data\"\u003eConsiderations for Using the Data\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#social-impact-of-the-dataset\"\u003eSocial Impact of the Dataset\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#impact-on-underserved-communities\"\u003eImpact on Underserved Communities\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#discussion-of-biases\"\u003eDiscussion of Biases\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#other-known-limitations\"\u003eOther Known Limitations\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#getting-started-with-in-depth-research-on-the-task\"\u003eGetting started with in-depth research on the task\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"dataset-description\"\u003eDataset Description\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHomepage:\u003c/strong\u003e None (See \u003cstrong\u003eRepository\u003c/strong\u003e)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRepository:\u003c/strong\u003e \u003ca href=\"https://github.com/chaojiang06/wiki-auto\"\u003eWiki-Auto repository\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePaper:\u003c/strong\u003e \u003ca href=\"https://www.aclweb.org/anthology/2020.acl-main.709.pdf\"\u003eNeural CRF Model for Sentence Alignment in Text Simplification\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoint of Contact:\u003c/strong\u003e \u003ca href=\"jiang.1530@osu.edu\"\u003eChao Jiang\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"dataset-and-task-summary\"\u003eDataset and Task Summary\u003c/h3\u003e\n\u003cp\u003eWikiAuto provides a set of aligned sentences from English Wikipedia and Simple English Wikipedia as a resource to train sentence simplification systems.\u003c/p\u003e\n\u003cp\u003eThe authors first crowd-sourced a set of manual alignments between sentences in a subset of the Simple English Wikipedia and their corresponding versions in English Wikipedia (this corresponds to the \u003ccode\u003emanual\u003c/code\u003e config in this version of the dataset), then trained a neural CRF system to predict these alignments.\u003c/p\u003e\n\u003cp\u003eThe trained alignment prediction model was then applied to the other articles in Simple English Wikipedia with an English counterpart to create a larger corpus of aligned sentences (corresponding to the \u003ccode\u003eauto\u003c/code\u003e and \u003ccode\u003eauto_acl\u003c/code\u003e configs here).\u003c/p\u003e\n\u003ch3 id=\"why-is-this-dataset-part-of-gem\"\u003eWhy is this dataset part of GEM?\u003c/h3\u003e\n\u003cp\u003eWiki-Auto is the largest open text simplification dataset currently available. It is the training dataset for the text simplification task in GEM.\u003c/p\u003e\n\u003ch3 id=\"languages\"\u003eLanguages\u003c/h3\u003e\n\u003cp\u003eWiki-Auto contains English text only (BCP-47: \u003ccode\u003een\u003c/code\u003e). It is presented as a translation task where Wikipedia Simple English is treated as its own idiom. For a statement of what is intended (but not always observed) to constitute Simple English on this platform, see \u003ca href=\"https://simple.wikipedia.org/wiki/Wikipedia:About#Simple_English\"\u003eSimple English in Wikipedia\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"meta-information\"\u003eMeta Information\u003c/h2\u003e\n\u003ch3 id=\"dataset-curators\"\u003eDataset Curators\u003c/h3\u003e\n\u003cp\u003eThe dataset was created by Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu from Ohio State University. The research is based upon work supported in part by the NSF awards IIS-1755898 and IIS-1822754, ODNI and  IARPA  via the  BETTER  program contract 19051600004, ARO and DARPA via the Social-Sim program contract W911NF-17-C-0095, Figure Eight AI for Everyone Award, and Criteo Faculty Research Award to Wei Xu.\u003c/p\u003e\n\u003ch3 id=\"licensing-information\"\u003eLicensing Information\u003c/h3\u003e\n\u003cp\u003eThe dataset is not licensed by itself, but the source Wikipedia data is under a \u003ccode\u003ecc-by-sa-3.0\u003c/code\u003e license.\u003c/p\u003e\n\u003ch3 id=\"citation-information\"\u003eCitation Information\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{jiang-etal-2020-neural,\n    title = \"Neural {CRF} Model for Sentence Alignment in Text Simplification\",\n    author = \"Jiang, Chao  and\n      Maddela, Mounica  and\n      Lan, Wuwei  and\n      Zhong, Yang  and\n      Xu, Wei\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-main.709\",\n    doi = \"10.18653/v1/2020.acl-main.709\",\n    pages = \"7943--7960\",\n}\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"leaderboard\"\u003eLeaderboard\u003c/h3\u003e\n\u003cp\u003eThere is no official leaderboard associated with Wiki-Auto.\u003c/p\u003e\n\u003ch2 id=\"dataset-structure\"\u003eDataset Structure\u003c/h2\u003e\n\u003ch3 id=\"data-instances\"\u003eData Instances\u003c/h3\u003e\n\u003cp\u003eThe data in all of the configurations look a little different.\u003c/p\u003e\n\u003cp\u003eA \u003ccode\u003emanual\u003c/code\u003e config instance consists of a sentence from the Simple English Wikipedia article, one from the linked English Wikipedia article, IDs for each of them, and a label indicating whether they are aligned. Sentences on either side can be repeated so that the aligned sentences are in the same instances. For example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e{'alignment_label': 1,\n 'normal_sentence': 'The Local Government Act 1985 is an Act of Parliament in the United Kingdom.',\n 'normal_sentence_id': '0_66252-1-0-0',\n 'simple_sentence': 'The Local Government Act 1985 was an Act of Parliament in the United Kingdom.',\n 'simple_sentence_id': '0_66252-0-0-0'}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIs followed by\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e{'alignment_label': 0,\n 'normal_sentence': 'Its main effect was to abolish the six county councils of the metropolitan counties that had been set up in 1974, 11 years earlier, by the Local Government Act 1972, along with the Greater London Council that had been established in 1965.',\n 'normal_sentence_id': '0_66252-1-0-1',\n 'simple_sentence': 'The Local Government Act 1985 was an Act of Parliament in the United Kingdom.',\n 'simple_sentence_id': '0_66252-0-0-0'}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003eauto\u003c/code\u003e config shows a pair of an English and corresponding Simple English Wikipedia as an instance, with an alignment at the paragraph and sentence level:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e{'example_id': '0',\n 'normal': {'normal_article_content': {'normal_sentence': [\"Lata Mondal ( ; born: 16 January 1993, Dhaka) is a Bangladeshi cricketer who plays for the Bangladesh national women's cricket team.\",\n    'She is a right handed batter.',\n    'Mondal was born on January 16, 1993 in Dhaka, Bangladesh.',\n    \"Mondal made her ODI career against the Ireland women's cricket team on November 26, 2011.\",\n    \"Mondal made her T20I career against the Ireland women's cricket team on August 28, 2012.\",\n    \"In October 2018, she was named in Bangladesh's squad for the 2018 ICC Women's World Twenty20 tournament in the West Indies.\",\n    \"Mondal was a member of the team that won a silver medal in cricket against the China national women's cricket team at the 2010 Asian Games in Guangzhou, China.\"],\n   'normal_sentence_id': ['normal-41918715-0-0',\n    'normal-41918715-0-1',\n    'normal-41918715-1-0',\n    'normal-41918715-2-0',\n    'normal-41918715-3-0',\n    'normal-41918715-3-1',\n    'normal-41918715-4-0']},\n  'normal_article_id': 41918715,\n  'normal_article_title': 'Lata Mondal',\n  'normal_article_url': 'https://en.wikipedia.org/wiki?curid=41918715'},\n 'paragraph_alignment': {'normal_paragraph_id': ['normal-41918715-0'],\n  'simple_paragraph_id': ['simple-702227-0']},\n 'sentence_alignment': {'normal_sentence_id': ['normal-41918715-0-0',\n   'normal-41918715-0-1'],\n  'simple_sentence_id': ['simple-702227-0-0', 'simple-702227-0-1']},\n 'simple': {'simple_article_content': {'simple_sentence': [\"Lata Mondal (born: 16 January 1993) is a Bangladeshi cricketer who plays for the Bangladesh national women's cricket team.\",\n    'She is a right handed bat.'],\n   'simple_sentence_id': ['simple-702227-0-0', 'simple-702227-0-1']},\n  'simple_article_id': 702227,\n  'simple_article_title': 'Lata Mondal',\n  'simple_article_url': 'https://simple.wikipedia.org/wiki?curid=702227'}}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFinally, the \u003ccode\u003eauto_acl\u003c/code\u003e, the \u003ccode\u003eauto_full_no_split\u003c/code\u003e, and the \u003ccode\u003eauto_full_with_split\u003c/code\u003e configs were obtained by selecting the aligned pairs of sentences from \u003ccode\u003eauto\u003c/code\u003e to provide a ready-to-go aligned dataset to train a sequence-to-sequence system. While \u003ccode\u003eauto_acl\u003c/code\u003e corresponds to the filtered version of the data used to train the systems in the paper, \u003ccode\u003eauto_full_no_split\u003c/code\u003e and \u003ccode\u003eauto_full_with_split\u003c/code\u003e correspond to the unfiltered versions with and without sentence splits respectively. In the \u003ccode\u003eauto_full_with_split\u003c/code\u003e config, we join the sentences in the simple article mapped to the same sentence in the complex article to capture sentence splitting. Split sentences are seperated by a \u003ccode\u003e\u0026#x3C;SEP\u003e\u003c/code\u003e token. In the \u003ccode\u003eauto_full_no_split config\u003c/code\u003e, we do not join the splits and treat them as seperate pairs. An instance is a single pair of sentences:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e{'normal_sentence': 'In early work, Rutherford discovered the concept of radioactive half-life , the radioactive element radon, and differentiated and named alpha and beta radiation .\\n',\n 'simple_sentence': 'Rutherford discovered the radioactive half-life, and the three parts of radiation which he named Alpha, Beta, and Gamma.\\n'}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThus, for training a text simplification model for GEM, the data with the \u003ccode\u003eauto_acl\u003c/code\u003e config can be directly used.\u003c/p\u003e\n\u003ch3 id=\"data-fields\"\u003eData Fields\u003c/h3\u003e\n\u003cp\u003eThe data has the following field:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003enormal_sentence\u003c/code\u003e: a sentence from English Wikipedia.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enormal_sentence_id\u003c/code\u003e: a unique ID for each English Wikipedia sentence. The last two dash-separated numbers correspond to the paragraph number in the article and the sentence number in the paragraph.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esimple_sentence\u003c/code\u003e: a sentence from Simple English Wikipedia.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esimple_sentence_id\u003c/code\u003e: a unique ID for each Simple English Wikipedia sentence. The last two dash-separated numbers correspond to the paragraph number in the article and the sentence number in the paragraph.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ealignment_label\u003c/code\u003e: signifies whether a pair of sentences is aligned: labels are \u003ccode\u003e\u003e=1:aligned\u003c/code\u003e and \u003ccode\u003e0:notAligned\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eparagraph_alignment\u003c/code\u003e: a first step of alignment mapping English and Simple English paragraphs from linked articles\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esentence_alignment\u003c/code\u003e: the full alignment mapping English and Simple English sentences from linked articles\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"data-statistics\"\u003eData Statistics\u003c/h3\u003e\n\u003cp\u003eIn \u003ccode\u003eauto\u003c/code\u003e, the \u003ccode\u003epart_2\u003c/code\u003e split corresponds to the articles used in \u003ccode\u003emanual\u003c/code\u003e, and \u003ccode\u003epart_1\u003c/code\u003e has the rest of Wikipedia.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003emanual\u003c/code\u003e config is provided with a \u003ccode\u003etrain\u003c/code\u003e/\u003ccode\u003edev\u003c/code\u003e/\u003ccode\u003etest\u003c/code\u003e split with the following amounts of data:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eTain\u003c/th\u003e\n\u003cth\u003eDev\u003c/th\u003e\n\u003cth\u003eTest\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eTotal sentence pairs\u003c/td\u003e\n\u003ctd\u003e373801\u003c/td\u003e\n\u003ctd\u003e73249\u003c/td\u003e\n\u003ctd\u003e118074\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eAligned sentence pairs\u003c/td\u003e\n\u003ctd\u003e1889\u003c/td\u003e\n\u003ctd\u003e346\u003c/td\u003e\n\u003ctd\u003e677\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eThe \u003ccode\u003eauto_acl\u003c/code\u003e has 488,332 complex-sentence pairs that are to be used for training the model. The average sentence length for complex and simple sentences are 26.6 and 18.7 respectively.\u003c/p\u003e\n\u003ch2 id=\"dataset-creation\"\u003eDataset Creation\u003c/h2\u003e\n\u003ch3 id=\"curation-rationale\"\u003eCuration Rationale\u003c/h3\u003e\n\u003cp\u003eWiki-Auto provides a new version of the Wikipedia corpus that is larger, contains 75% less defective pairs and has more complex rewrites than the previous WIKILARGE dataset.\u003c/p\u003e\n\u003ch3 id=\"communicative-goal\"\u003eCommunicative Goal\u003c/h3\u003e\n\u003cp\u003eThe goal is to communicate the same information as the source sentence using simpler words and grammar.\u003c/p\u003e\n\u003ch3 id=\"source-data\"\u003eSource Data\u003c/h3\u003e\n\u003ch4 id=\"initial-data-collection-and-normalization\"\u003eInitial Data Collection and Normalization\u003c/h4\u003e\n\u003cp\u003eThe authors mention that they \"extracted 138,095 article pairs from the 2019/09 Wikipedia dump using an improved version of the \u003ca href=\"https://github.com/attardi/wikiextractor\"\u003eWikiExtractor\u003c/a\u003e library\". The \u003ca href=\"https://spacy.io/\"\u003eSpaCy\u003c/a\u003e library is used for sentence splitting.\u003c/p\u003e\n\u003ch4 id=\"who-are-the-source-language-producers\"\u003eWho are the source language producers?\u003c/h4\u003e\n\u003cp\u003eThe dataset uses language from Wikipedia: some demographic information is provided \u003ca href=\"https://en.wikipedia.org/wiki/Wikipedia:Who_writes_Wikipedia%3F\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"annotations\"\u003eAnnotations\u003c/h3\u003e\n\u003ch4 id=\"annotation-process\"\u003eAnnotation process\u003c/h4\u003e\n\u003cp\u003eSentence alignment labels were crowdsourced for 500 randomly sampled document pairs (10,123 sentence pairs total). The authors pre-selected several alignment candidates from English Wikipedia for each Simple Wikipedia sentence based on various similarity metrics, then asked the crowd-workers to annotate these pairs. Finally, they trained their alignment model on this manually annotated dataset to obtain automatically aligned sentences (138,095 document pairs, 488,332 sentence pairs).\u003c/p\u003e\n\u003ch4 id=\"who-are-the-annotators\"\u003eWho are the annotators?\u003c/h4\u003e\n\u003cp\u003eNo demographic annotation is provided for the crowd workers. The \u003ca href=\"https://www.figure-eight.com/\"\u003eFigure Eight\u003c/a\u003e platform was used for the annotation process.\u003c/p\u003e\n\u003ch3 id=\"personal-and-sensitive-information\"\u003ePersonal and Sensitive Information\u003c/h3\u003e\n\u003cp\u003eSince the dataset is created from Wikipedia/Simple Wikipedia, all the information contained in the dataset is already in the public domain.\u003c/p\u003e\n\u003ch2 id=\"changes-to-the-original-dataset-for-gem\"\u003eChanges to the Original Dataset for GEM\u003c/h2\u003e\n\u003cp\u003eNo change is made to the original dataset.\u003c/p\u003e\n\u003ch2 id=\"considerations-for-using-the-data\"\u003eConsiderations for Using the Data\u003c/h2\u003e\n\u003ch3 id=\"social-impact-of-the-dataset\"\u003eSocial Impact of the Dataset\u003c/h3\u003e\n\u003cp\u003eThe dataset helps move forward the research towards text simplification by creating a larger and more accurate dataset. Progress in text simplification in turn has the potential to increase the accessibility of written documents to wider audiences.\u003c/p\u003e\n\u003ch3 id=\"impact-on-underserved-communities\"\u003eImpact on Underserved Communities\u003c/h3\u003e\n\u003cp\u003eThe dataset is in English, a language with an abundance of existing resources.\u003c/p\u003e\n\u003ch3 id=\"discussion-of-biases\"\u003eDiscussion of Biases\u003c/h3\u003e\n\u003cp\u003eThe dataset may contain some social biases, as the input sentences are based on Wikipedia. Studies have shown that the English Wikipedia contains both gender biases \u003ca href=\"https://research.tudelft.nl/en/publications/is-wikipedia-succeeding-in-reducing-gender-bias-assessing-changes\"\u003e(Schmahl et al., 2020)\u003c/a\u003e and racial biases \u003ca href=\"https://journals.sagepub.com/doi/pdf/10.1177/2378023118823946\"\u003e(Adams et al., 2019)\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"other-known-limitations\"\u003eOther Known Limitations\u003c/h3\u003e\n\u003cp\u003eSince the data is created using an automatic alignment model (which is not perfect) there could be still some alignment issues in the data.\u003c/p\u003e\n\u003ch2 id=\"getting-started-with-in-depth-research-on-the-task\"\u003eGetting started with in-depth research on the task\u003c/h2\u003e\n\u003cp\u003eThe dataset can be downloaded from the original repository \u003ca href=\"https://github.com/chaojiang06/wiki-auto\"\u003e(here)\u003c/a\u003e by the authors or can also be used via \u003ca href=\"https://huggingface.co/datasets/wiki_auto\"\u003eHuggingFace\u003c/a\u003e and \u003ca href=\"https://www.tensorflow.org/datasets/overview\"\u003eTFDS\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe dataset repository provided by the authors also contains instructions to load a transformer-based sequence-to-sequence model trained on the dataset. There are also other recent supervised (\u003ca href=\"https://arxiv.org/abs/1910.02677\"\u003eMartin et al., 2019\u003c/a\u003e, \u003ca href=\"https://www.aclweb.org/anthology/N19-1317/\"\u003eKriz et al., 2019\u003c/a\u003e, \u003ca href=\"https://www.aclweb.org/anthology/P19-1331/\"\u003eDong et al., 2019\u003c/a\u003e, \u003ca href=\"https://www.aclweb.org/anthology/D17-1062/\"\u003eZhang and Lapata, 2017\u003c/a\u003e) and unsupervised (\u003ca href=\"https://arxiv.org/abs/2005.00352v1\"\u003eMartin et al., 2020\u003c/a\u003e, \u003ca href=\"https://www.aclweb.org/anthology/2020.acl-main.707/\"\u003eKumar et al., 2020\u003c/a\u003e, \u003ca href=\"https://www.aclweb.org/anthology/P19-1198/\"\u003eSurya et al., 2019\u003c/a\u003e) text simplification models that can be used as baselines.\u003c/p\u003e\n\u003cp\u003eThe common metric used for automatic evaluation is SARI \u003ca href=\"https://www.aclweb.org/anthology/Q16-1029/\"\u003e(Xu et al., 2016)\u003c/a\u003e.\u003c/p\u003e\n","title":"Wiki-Auto","type":"Simplification","motivation":"Wiki-Auto is the largest open text simplification dataset currently available. For GEM, Wiki-Auto acts as the training set."}},"__N_SSG":true},"page":"/data_cards/[id]","query":{"id":"Wiki-Auto"},"buildId":"-4BzhAC0_kuCZeeA-vHE_","nextExport":false,"isFallback":false,"gsp":true,"head":[["meta",{"name":"viewport","content":"width=device-width"}],["meta",{"charSet":"utf-8"}],["link",{"rel":"icon","href":"/favicon.ico"}],["meta",{"name":"description","content":"Benchmark natural language generation systems with GEM."}],["meta",{"property":"og:image","content":"https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light\u0026md=1\u0026fontSize=100px\u0026images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"}],["meta",{"name":"og:title","content":"GEM"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["title",{"children":"GEM Wiki-Auto"}]]}</script><script nomodule="" src="/_next/static/chunks/polyfills-e69cc13a7e89296a69e4.js"></script><script src="/_next/static/chunks/main-47bc8f80085b54a800da.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.baa41d4dbf5d52db897c.js" async=""></script><script src="/_next/static/chunks/e70fad557dfa42f32a11d0d2c99fe8f6e8d1fa86.4a36a385313236c59b19.js" async=""></script><script src="/_next/static/chunks/pages/_app-a9ae7a6d1de4e51a7ab6.js" async=""></script><script src="/_next/static/chunks/cb1608f2.c3a9f0eb95374ca4919a.js" async=""></script><script src="/_next/static/chunks/451c6be158cef50d8cc28b919cf08d1e5b9ff3fc.f0ec181e43727e8a893e.js" async=""></script><script src="/_next/static/chunks/pages/data_cards/%5Bid%5D-e1361c13d80af81deda6.js" async=""></script><script src="/_next/static/-4BzhAC0_kuCZeeA-vHE_/_buildManifest.js" async=""></script><script src="/_next/static/-4BzhAC0_kuCZeeA-vHE_/_ssgManifest.js" async=""></script></body></html>