<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="Benchmark natural language generation systems with GEM."/><meta property="og:image" content="https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"/><meta name="og:title" content="GEM"/><meta name="twitter:card" content="summary_large_image"/><title>GEM POINTER</title><meta name="next-head-count" content="8"/><link rel="preload" href="/_next/static/css/8f219039bec51df6.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8f219039bec51df6.css" data-n-g=""/><link rel="preload" href="/_next/static/css/9c6cbdf6de0d7f7f.css" as="style"/><link rel="stylesheet" href="/_next/static/css/9c6cbdf6de0d7f7f.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-a7ce780f37f41655.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-9d34f203832e9748.js" defer=""></script><script src="/_next/static/chunks/pages/_app-dcd786a11fa40981.js" defer=""></script><script src="/_next/static/chunks/c16184b3-ddb1b99b5e568a2a.js" defer=""></script><script src="/_next/static/chunks/50-215a96335ed97100.js" defer=""></script><script src="/_next/static/chunks/pages/model_cards/%5Bid%5D-3c3fdd4b2f3cf577.js" defer=""></script><script src="/_next/static/EoAnBz4AQY14m3O08pVBK/_buildManifest.js" defer=""></script><script src="/_next/static/EoAnBz4AQY14m3O08pVBK/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="layout_background__qqb_S undefined"><header class="layout_header__kY0Lt"><div class="navbar_navwrapper__PQ35R"><div class="navbar_gradbar__2_FPI"></div><nav class="navbar_navbar__9q1fQ"><span class="utils_headingLg__5535D navbar_navbarlogo__R_s98"><a href="/">GEM BENCHMARK</a></span><div class="navbar_menutoggle__XS8Qx" id="mobile-menu"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars navbar_bar___PSVO" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg></div><ul><li class="navbar_navitem__wFPZL navbar_pushright__cG1uq"><a href="/resources">Resources</a></li><li class="navbar_navitem__wFPZL"><a href="/data_cards">Data Cards</a></li><li class="navbar_navitem__wFPZL"><a href="/model_cards">Model Cards</a></li><li class="navbar_navitem__wFPZL"><a href="/tutorials">tutorials</a></li><li class="navbar_navitem__wFPZL"><a href="/results">Results</a></li><li class="navbar_navitem__wFPZL"><a href="/papers">Papers</a></li><li class="navbar_navitem__wFPZL"><a href="/team">Team</a></li><li class="navbar_navitem__wFPZL"><a href="/workshop">Workshop</a></li></ul></nav></div></header><div class="layout_container__fbLkO undefined"><main><article><span class="utils_headingXl__u25Y2">POINTER</span><span class="utils_smallSpace__n1Oyc"></span><span class="utils_lightText__eUzGY">Shared Task 2021</span><div><h2 id="user-content-table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#model-description">Model Description</a></li>
<li><a href="#social-impact">Social Impact</a>
<ul>
<li><a href="#additional-data">Additional Data</a></li>
<li><a href="#training-process">Training Process</a></li>
<li><a href="#real-world-use">Real-World Use</a></li>
<li><a href="#measuring-impact">Measuring Impact</a></li>
</ul>
</li>
<li><a href="#reproducibility">Reproducibility</a>
<ul>
<li><a href="#model-description-1">Model Description</a></li>
<li><a href="#model-hyperparameters">Model Hyperparameters</a></li>
<li><a href="#the-hyperparameter-specifications-for-best-performing-models">The Hyperparameter specifications for best performing models</a></li>
<li><a href="#dataset-details">Dataset Details</a></li>
<li><a href="#dependencies-and-external-libraries">Dependencies and External Libraries</a></li>
<li><a href="#link-to-downloadable-source-code">Link to downloadable source code</a></li>
<li><a href="#computing-infrastructure-used">Computing Infrastructure Used</a></li>
<li><a href="#evaluation-details">Evaluation Details</a></li>
</ul>
</li>
</ul>
<h2 id="user-content-model-description">Model Description</h2>
<ul>
<li><strong>Paper:</strong> <a href="https://aclanthology.org/2021.gem-1.15/">System Description for the CommonGen task with the POINTER model</a></li>
<li><strong>Creator:</strong> Anna Shvets</li>
<li><strong>Point of Contact:</strong> Anna Shvets (<a href="mailto:anna.shvets@inetum.com">anna.shvets@inetum.com</a>)</li>
</ul>
<h2 id="user-content-social-impact">Social Impact</h2>
<p>In this section, we ask you to provide information on all of the steps that went into obtaining your models, especially in regards to how they would affect a user’s interactions with the technology if it was deployed in a live system.</p>
<p>Then, please choose one of these steps to analyze in terms of possible negative impacts on potential direct and indirect users, and propose a test to evaluate the existence and magnitude of that impact. We provide examples of such analyses in the following two paragraphs.</p>
<p>Consider for example a model pre-trained on English Wikipedia and fine-tuned on a summarization dataset, and imagine that such a model is deployed in a news website to provide automatic summaries. Given the gender gap on Wikipedia, we can imagine two possible effects:</p>
<ol>
<li>the model could systematically produce summaries with lower ROUGE scores when compared to a reference for articles describing women than it would for articles describing men.</li>
<li>the model may be less likely to name the subject of an article if the subject is a woman than if the subject is a man.
We can measure either of these effects by running an entity linking system on the articles, for example, and comparing subsets of the test set where the gender of the actors is known.</li>
</ol>
<p>Alternatively, imagine that a system that is fine-tuned on WikiAuto, which aligns English Wikipedia text to its Simple English Wikipedia version, is used in an attempt to make a blog where the writer talks about their personal experience more accessible. How well does the model handle the shift from a third person to a first person point of view? One way we can check whether the behavior is roughly the same is by comparing the copy rate (number of words from the input that are re-used in the output) between the Wikipedia and blog setting.</p>
<p>We ask you to take a similar approach to analyzing your model:</p>
<h3 id="user-content-additional-data">Additional Data</h3>
<p><em>If you used a model that was pre-trained on additional data or used additional data, please describe it. Provide a link to a datasheet or data statement if there is one available, otherwise, provide as much relevant information as possible on the source of the data, the people represented in it, its languages, licensing, pre-processing, etc.</em></p>
<p>The model used for CommonGEN task is based on pre-trained POINTER model available at <a href="https://github.com/dreasysnail/POINTER">https://github.com/dreasysnail/POINTER</a>. According to authors of the article, describing the pre-training process (<a href="https://arxiv.org/abs/2005.00558">https://arxiv.org/abs/2005.00558</a>), 12Ga of English Wikipedia corpora has been used to produce above-metioned pre-trained wiki model. The pretrained model is distributed under MIT licence: <a href="https://github.com/dreasysnail/POINTER/blob/master/LICENSE">https://github.com/dreasysnail/POINTER/blob/master/LICENSE</a>.</p>
<h3 id="user-content-training-process">Training Process</h3>
<p><em>Describe the training or fine tuning setup, including whether the final model was trained on a single task or in a multi-task setting. If a data augmentation technique was used, describe the technique</em></p>
<p>The fune tuning was done using training data for CommonGEN task. The data pre-processing included the ml masks formation with a special NOI tag on 3 data epochs, which resulted in data augmentation from  67.389 source entries to 160.680 processed entries.</p>
<h3 id="user-content-real-world-use">Real-World Use</h3>
<p><em>Describe a possible real-use application of your model, then choose one of the steps (e.g. choice of the pre-trained model, data source, data augmentation, training loss, etc.) above and describe a negative impact it may have on the user experience</em></p>
<p>Since the model was pre-trained on English Wikipedia corpora, the bias inherent to the dataset will potentially influence the generated texts, leading to potential discrimination of minority groups or marginalized communities.</p>
<h3 id="user-content-measuring-impact">Measuring Impact</h3>
<p><em>Propose a method to test the magnitude of the impact identified in your previous answer</em></p>
<p>There are several methodologies to measure bias in a dataset, reflected in a devoted study: <a href="https://arxiv.org/abs/1908.09635">https://arxiv.org/abs/1908.09635</a>. In order to measure the fairness of the generated text, one of the fairness metrics might be applied, including equalized odds, predictive parity, counterfactual fairness or demographic parity.</p>
<h2 id="user-content-reproducibility">Reproducibility</h2>
<p><em>In this section, we ask you to provide any information that would be required for someone to reproduce your model and experimental results. These questions are derived from the suggested model card in [1] and the reproducibility checklist in [2].</em></p>
<p><em>[1] Dodge, Jesse, et al. "Show Your Work: Improved Reporting of Experimental Results." EMNLP. 2019.</em></p>
<p><em>[1] Mitchell et al. "Model cards for model reporting." Proceedings of the conference on fairness, accountability, and transparency. 2019.</em></p>
<h3 id="user-content-model-description-1">Model Description</h3>
<p><em>Provide basic information about your model, including (1) the model type (e.g., BART, Pointer Network), (2) model version/date if multiple versions are available, (3) training algorithms used. Please cite papers or other resources where further information about the model can be found. Also include the model license and citation details.</em></p>
<p>POINTER is a hybrid architecture, combining transformers with insertion-based networks. The original paper was published in 2020 (<a href="https://arxiv.org/abs/2005.00558">https://arxiv.org/abs/2005.00558</a>), along with the pretrained wiki model available for download, along with the source code at <a href="https://github.com/dreasysnail/POINTER">https://github.com/dreasysnail/POINTER</a></p>
<h3 id="user-content-model-hyperparameters">Model Hyperparameters</h3>
<p><em>Provide the range of hyperparameters that would be required to reproduce your final model (e.g., optimizer used, number of epochs, learning rate, etc.). If hyperparameter search was used, please describe (1) the bounds for each hyperparameter, (2) the number of hyperparameter search trials, (3) the method for choosing hyperparameter values (e.g., uniform sampling, manual tuning, evolutionary optimization, etc.).</em></p>
<p>Optimizer: AdamW</p>
<p>Learningrate: 1e-5</p>
<p>Adam epsilon: 1e-8</p>
<p>Warmup steps: 10</p>
<p>Seed: 1</p>
<p>Epochs: 10</p>
<p>Batch size: 64</p>
<h3 id="user-content-the-hyperparameter-specifications-for-best-performing-models">The Hyperparameter specifications for best performing models</h3>
<p>There are two sampling methods used while the inference phase - greedy and sampling. Greedy is based on a greedy search algorithm, while sampling uses top-k, top-p and temperature parameters to render model predictions. Here is a set for both of sampling strategies:</p>
<ol>
<li>
<p>Greedy:
noi_decay: 1,
reduce_decay: 1,
prevent: True,
reduce_stop: True,
lessrepeat: True.</p>
</li>
<li>
<p>Sampling:
top_k: 10,
top_p: 0.9,
temperature: 1.</p>
</li>
</ol>
<h3 id="user-content-dataset-details">Dataset Details</h3>
<p><em>Include relevant training data statistics (e.g., number of samples used, whether some subsets of the dataset were discarded), the training/validation/test splits for the number of samples and any pre-processing steps if used.</em></p>
<p>67.389 source entries from training set of CommonGEM dataset were converted to 160.680 processed entries. The main purpose of pre-processing consists of creation of lm labels with a special NOI tag, masking the word that should be further inserted at the inference phase.</p>
<h3 id="user-content-dependencies-and-external-libraries">Dependencies and External Libraries</h3>
<p><em>Include a specification of library dependencies</em></p>
<p>Since the TPU was used for training, XLA support is necessary.</p>
<h3 id="user-content-link-to-downloadable-source-code">Link to downloadable source code</h3>
<p><a href="https://github.com/dreasysnail/POINTER">https://github.com/dreasysnail/POINTER</a></p>
<h3 id="user-content-computing-infrastructure-used">Computing Infrastructure Used</h3>
<p><em>Describe the computing infrastructure used to train your model (e.g., number of GPUs, GPU type and vRAM) and the time taken to train your final model.</em></p>
<p>The training was done using TPU-v3-8, following the multiprocessing paradigm. The total training time was 3 hours.</p>
<h3 id="user-content-evaluation-details">Evaluation Details</h3>
<p><em>How were your models evaluated? Please include evaluation metric details (including links to code), train/validation/test splits, and model performance on both test and validation sets. If more than one model was trained and evaluated, what was the number of training and evaluation runs, and the variance in scores? If human evaluation was used, please describe the experimental setup.</em></p>
<p>The model performance was evaluated using different metrics for lexical similarity (ROUGE 1/2/L, BLEU, Meteor), semantic similarity (BERTscore, BLEURT) and diversity (MSTTR, Distinct 1/2/3, Unique 1/2/3, Entropy 1/2/3) measures. The file with all described metrics and their results is available at <a href="https://github.com/asnota/metrics">https://github.com/asnota/metrics</a></p>
</div></article></main><div class="layout_push__2FFZa"></div></div><footer class="layout_footer__dka_2 utils_eggshell__LF2UE"><span class="layout_backToHome__9sjx_"><a href="/">← Home</a></span><span>If you have any questions, please join our <a href="https://groups.google.com/g/gem-benchmark" target="_blank" class="utils_accentUnderline__FmNQy">google group</a> for support.</span></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"taskData":{"contentHtml":"\u003ch2 id=\"user-content-table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#model-description\"\u003eModel Description\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#social-impact\"\u003eSocial Impact\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#additional-data\"\u003eAdditional Data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#training-process\"\u003eTraining Process\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#real-world-use\"\u003eReal-World Use\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#measuring-impact\"\u003eMeasuring Impact\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#reproducibility\"\u003eReproducibility\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#model-description-1\"\u003eModel Description\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#model-hyperparameters\"\u003eModel Hyperparameters\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#the-hyperparameter-specifications-for-best-performing-models\"\u003eThe Hyperparameter specifications for best performing models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dataset-details\"\u003eDataset Details\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dependencies-and-external-libraries\"\u003eDependencies and External Libraries\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#link-to-downloadable-source-code\"\u003eLink to downloadable source code\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#computing-infrastructure-used\"\u003eComputing Infrastructure Used\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#evaluation-details\"\u003eEvaluation Details\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"user-content-model-description\"\u003eModel Description\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePaper:\u003c/strong\u003e \u003ca href=\"https://aclanthology.org/2021.gem-1.15/\"\u003eSystem Description for the CommonGen task with the POINTER model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCreator:\u003c/strong\u003e Anna Shvets\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoint of Contact:\u003c/strong\u003e Anna Shvets (\u003ca href=\"mailto:anna.shvets@inetum.com\"\u003eanna.shvets@inetum.com\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"user-content-social-impact\"\u003eSocial Impact\u003c/h2\u003e\n\u003cp\u003eIn this section, we ask you to provide information on all of the steps that went into obtaining your models, especially in regards to how they would affect a user’s interactions with the technology if it was deployed in a live system.\u003c/p\u003e\n\u003cp\u003eThen, please choose one of these steps to analyze in terms of possible negative impacts on potential direct and indirect users, and propose a test to evaluate the existence and magnitude of that impact. We provide examples of such analyses in the following two paragraphs.\u003c/p\u003e\n\u003cp\u003eConsider for example a model pre-trained on English Wikipedia and fine-tuned on a summarization dataset, and imagine that such a model is deployed in a news website to provide automatic summaries. Given the gender gap on Wikipedia, we can imagine two possible effects:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ethe model could systematically produce summaries with lower ROUGE scores when compared to a reference for articles describing women than it would for articles describing men.\u003c/li\u003e\n\u003cli\u003ethe model may be less likely to name the subject of an article if the subject is a woman than if the subject is a man.\nWe can measure either of these effects by running an entity linking system on the articles, for example, and comparing subsets of the test set where the gender of the actors is known.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAlternatively, imagine that a system that is fine-tuned on WikiAuto, which aligns English Wikipedia text to its Simple English Wikipedia version, is used in an attempt to make a blog where the writer talks about their personal experience more accessible. How well does the model handle the shift from a third person to a first person point of view? One way we can check whether the behavior is roughly the same is by comparing the copy rate (number of words from the input that are re-used in the output) between the Wikipedia and blog setting.\u003c/p\u003e\n\u003cp\u003eWe ask you to take a similar approach to analyzing your model:\u003c/p\u003e\n\u003ch3 id=\"user-content-additional-data\"\u003eAdditional Data\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eIf you used a model that was pre-trained on additional data or used additional data, please describe it. Provide a link to a datasheet or data statement if there is one available, otherwise, provide as much relevant information as possible on the source of the data, the people represented in it, its languages, licensing, pre-processing, etc.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThe model used for CommonGEN task is based on pre-trained POINTER model available at \u003ca href=\"https://github.com/dreasysnail/POINTER\"\u003ehttps://github.com/dreasysnail/POINTER\u003c/a\u003e. According to authors of the article, describing the pre-training process (\u003ca href=\"https://arxiv.org/abs/2005.00558\"\u003ehttps://arxiv.org/abs/2005.00558\u003c/a\u003e), 12Ga of English Wikipedia corpora has been used to produce above-metioned pre-trained wiki model. The pretrained model is distributed under MIT licence: \u003ca href=\"https://github.com/dreasysnail/POINTER/blob/master/LICENSE\"\u003ehttps://github.com/dreasysnail/POINTER/blob/master/LICENSE\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"user-content-training-process\"\u003eTraining Process\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eDescribe the training or fine tuning setup, including whether the final model was trained on a single task or in a multi-task setting. If a data augmentation technique was used, describe the technique\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThe fune tuning was done using training data for CommonGEN task. The data pre-processing included the ml masks formation with a special NOI tag on 3 data epochs, which resulted in data augmentation from  67.389 source entries to 160.680 processed entries.\u003c/p\u003e\n\u003ch3 id=\"user-content-real-world-use\"\u003eReal-World Use\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eDescribe a possible real-use application of your model, then choose one of the steps (e.g. choice of the pre-trained model, data source, data augmentation, training loss, etc.) above and describe a negative impact it may have on the user experience\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eSince the model was pre-trained on English Wikipedia corpora, the bias inherent to the dataset will potentially influence the generated texts, leading to potential discrimination of minority groups or marginalized communities.\u003c/p\u003e\n\u003ch3 id=\"user-content-measuring-impact\"\u003eMeasuring Impact\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003ePropose a method to test the magnitude of the impact identified in your previous answer\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThere are several methodologies to measure bias in a dataset, reflected in a devoted study: \u003ca href=\"https://arxiv.org/abs/1908.09635\"\u003ehttps://arxiv.org/abs/1908.09635\u003c/a\u003e. In order to measure the fairness of the generated text, one of the fairness metrics might be applied, including equalized odds, predictive parity, counterfactual fairness or demographic parity.\u003c/p\u003e\n\u003ch2 id=\"user-content-reproducibility\"\u003eReproducibility\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eIn this section, we ask you to provide any information that would be required for someone to reproduce your model and experimental results. These questions are derived from the suggested model card in [1] and the reproducibility checklist in [2].\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e[1] Dodge, Jesse, et al. \"Show Your Work: Improved Reporting of Experimental Results.\" EMNLP. 2019.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e[1] Mitchell et al. \"Model cards for model reporting.\" Proceedings of the conference on fairness, accountability, and transparency. 2019.\u003c/em\u003e\u003c/p\u003e\n\u003ch3 id=\"user-content-model-description-1\"\u003eModel Description\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eProvide basic information about your model, including (1) the model type (e.g., BART, Pointer Network), (2) model version/date if multiple versions are available, (3) training algorithms used. Please cite papers or other resources where further information about the model can be found. Also include the model license and citation details.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003ePOINTER is a hybrid architecture, combining transformers with insertion-based networks. The original paper was published in 2020 (\u003ca href=\"https://arxiv.org/abs/2005.00558\"\u003ehttps://arxiv.org/abs/2005.00558\u003c/a\u003e), along with the pretrained wiki model available for download, along with the source code at \u003ca href=\"https://github.com/dreasysnail/POINTER\"\u003ehttps://github.com/dreasysnail/POINTER\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"user-content-model-hyperparameters\"\u003eModel Hyperparameters\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eProvide the range of hyperparameters that would be required to reproduce your final model (e.g., optimizer used, number of epochs, learning rate, etc.). If hyperparameter search was used, please describe (1) the bounds for each hyperparameter, (2) the number of hyperparameter search trials, (3) the method for choosing hyperparameter values (e.g., uniform sampling, manual tuning, evolutionary optimization, etc.).\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eOptimizer: AdamW\u003c/p\u003e\n\u003cp\u003eLearningrate: 1e-5\u003c/p\u003e\n\u003cp\u003eAdam epsilon: 1e-8\u003c/p\u003e\n\u003cp\u003eWarmup steps: 10\u003c/p\u003e\n\u003cp\u003eSeed: 1\u003c/p\u003e\n\u003cp\u003eEpochs: 10\u003c/p\u003e\n\u003cp\u003eBatch size: 64\u003c/p\u003e\n\u003ch3 id=\"user-content-the-hyperparameter-specifications-for-best-performing-models\"\u003eThe Hyperparameter specifications for best performing models\u003c/h3\u003e\n\u003cp\u003eThere are two sampling methods used while the inference phase - greedy and sampling. Greedy is based on a greedy search algorithm, while sampling uses top-k, top-p and temperature parameters to render model predictions. Here is a set for both of sampling strategies:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eGreedy:\nnoi_decay: 1,\nreduce_decay: 1,\nprevent: True,\nreduce_stop: True,\nlessrepeat: True.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSampling:\ntop_k: 10,\ntop_p: 0.9,\ntemperature: 1.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"user-content-dataset-details\"\u003eDataset Details\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eInclude relevant training data statistics (e.g., number of samples used, whether some subsets of the dataset were discarded), the training/validation/test splits for the number of samples and any pre-processing steps if used.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e67.389 source entries from training set of CommonGEM dataset were converted to 160.680 processed entries. The main purpose of pre-processing consists of creation of lm labels with a special NOI tag, masking the word that should be further inserted at the inference phase.\u003c/p\u003e\n\u003ch3 id=\"user-content-dependencies-and-external-libraries\"\u003eDependencies and External Libraries\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eInclude a specification of library dependencies\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eSince the TPU was used for training, XLA support is necessary.\u003c/p\u003e\n\u003ch3 id=\"user-content-link-to-downloadable-source-code\"\u003eLink to downloadable source code\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/dreasysnail/POINTER\"\u003ehttps://github.com/dreasysnail/POINTER\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"user-content-computing-infrastructure-used\"\u003eComputing Infrastructure Used\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eDescribe the computing infrastructure used to train your model (e.g., number of GPUs, GPU type and vRAM) and the time taken to train your final model.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThe training was done using TPU-v3-8, following the multiprocessing paradigm. The total training time was 3 hours.\u003c/p\u003e\n\u003ch3 id=\"user-content-evaluation-details\"\u003eEvaluation Details\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eHow were your models evaluated? Please include evaluation metric details (including links to code), train/validation/test splits, and model performance on both test and validation sets. If more than one model was trained and evaluated, what was the number of training and evaluation runs, and the variance in scores? If human evaluation was used, please describe the experimental setup.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThe model performance was evaluated using different metrics for lexical similarity (ROUGE 1/2/L, BLEU, Meteor), semantic similarity (BERTscore, BLEURT) and diversity (MSTTR, Distinct 1/2/3, Unique 1/2/3, Entropy 1/2/3) measures. The file with all described metrics and their results is available at \u003ca href=\"https://github.com/asnota/metrics\"\u003ehttps://github.com/asnota/metrics\u003c/a\u003e\u003c/p\u003e\n","title":"POINTER","type":"Shared Task 2021","background":"POINTER is a hybrid architecture, combining transformers with insertion-based networks."}},"__N_SSG":true},"page":"/model_cards/[id]","query":{"id":"POINTER"},"buildId":"EoAnBz4AQY14m3O08pVBK","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>