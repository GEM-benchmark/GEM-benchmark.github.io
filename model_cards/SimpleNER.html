<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="Benchmark natural language generation systems with GEM."/><meta property="og:image" content="https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"/><meta name="og:title" content="GEM"/><meta name="twitter:card" content="summary_large_image"/><title>GEM SimpleNER</title><link rel="preload" href="/_next/static/css/2786522978a02f025205.css" as="style"/><link rel="stylesheet" href="/_next/static/css/2786522978a02f025205.css" data-n-g=""/><link rel="preload" href="/_next/static/css/f2fce7b83fe6ca04479b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f2fce7b83fe6ca04479b.css" data-n-p=""/><noscript data-n-css="true"></noscript><link rel="preload" href="/_next/static/chunks/main-47bc8f80085b54a800da.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.baa41d4dbf5d52db897c.js" as="script"/><link rel="preload" href="/_next/static/chunks/26846a7fd70214140cb220fed85dd1cca1639cdd.4a36a385313236c59b19.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-a9ae7a6d1de4e51a7ab6.js" as="script"/><link rel="preload" href="/_next/static/chunks/cb1608f2.c3a9f0eb95374ca4919a.js" as="script"/><link rel="preload" href="/_next/static/chunks/c32b6ed53981e30c999312738987180eb6bb48f7.50e5606a166dbfda69c2.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/model_cards/%5Bid%5D-fc0c817a6908d1f88fd5.js" as="script"/></head><body><div id="__next"><div class="layout_background__1AVEa undefined"><header class="layout_header__2rhWq"><div class="navbar_navwrapper__15zia"><div class="navbar_gradbar__1Xi5u"></div><nav class="navbar_navbar__3gnco"><span class="utils_headingLg__de7p0 navbar_navbarlogo__PLEwr"><a href="/">GEM BENCHMARK</a></span><div class="navbar_menutoggle__358pJ" id="mobile-menu"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars fa-w-14 navbar_bar__QVPSR" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"></path></svg></div><ul><li class="navbar_navitem__3ICSG navbar_pushright__3G2DM"><a href="/resources">Resources</a></li><li class="navbar_navitem__3ICSG"><a href="/data_cards">Data Cards</a></li><li class="navbar_navitem__3ICSG"><a href="/model_cards">Model Cards</a></li><li class="navbar_navitem__3ICSG"><a href="/get_started">How To</a></li><li class="navbar_navitem__3ICSG"><a href="/results">Results</a></li><li class="navbar_navitem__3ICSG"><a href="/papers">Papers</a></li><li class="navbar_navitem__3ICSG"><a href="/team">Team</a></li><li class="navbar_navitem__3ICSG"><a href="/nl_augmenter">NL-Augmenter</a></li><li class="navbar_navitem__3ICSG"><a href="/workshop">Workshop</a></li></ul></nav></div></header><div class="layout_container__2t4v2"><main><article><span class="utils_headingXl__1XecN">SimpleNER</span><span class="utils_smallSpace__375iy"></span><span class="utils_lightText__12Ckm">Shared Task 2021</span><div><h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#model-description">Model Description</a></li>
<li><a href="#social-impact">Social Impact</a>
<ul>
<li><a href="#additional-data">Additional Data</a></li>
<li><a href="#training-process">Training Process</a></li>
<li><a href="#real-world-use">Real-World Use</a></li>
<li><a href="#measuring-impact">Measuring Impact</a></li>
</ul>
</li>
<li><a href="#reproducibility">Reproducibility</a>
<ul>
<li><a href="#model-description-1">Model Description</a></li>
<li><a href="#model-hyperparameters">Model Hyperparameters</a></li>
<li><a href="#the-hyperparameter-specifications-for-best-performing-models">The Hyperparameter specifications for best performing models</a></li>
<li><a href="#dataset-details">Dataset Details</a></li>
<li><a href="#dependencies-and-external-libraries">Dependencies and External Libraries</a></li>
<li><a href="#link-to-downloadable-source-code">Link to downloadable source code</a></li>
<li><a href="#computing-infrastructure-used">Computing Infrastructure Used</a></li>
<li><a href="#evaluation-details">Evaluation Details</a></li>
</ul>
</li>
</ul>
<h2 id="model-description">Model Description</h2>
<ul>
<li><strong>Paper:</strong> <a href="https://aclanthology.org/2021.gem-1.14/">SimpleNER Sentence Simplification System for GEM 2021</a></li>
<li><strong>Creators:</strong> K V Aditya Srivatsa, Monil Gokani, Manish Shrivastava</li>
<li><strong>Point of Contact:</strong> K V Aditya Srivatsa (<a href="mailto:k.v.aditya@research.iiit.ac.in">k.v.aditya@research.iiit.ac.in</a>)</li>
</ul>
<h2 id="social-impact">Social Impact</h2>
<p>In this section, we ask you to provide information on all of the steps that went into obtaining your models, especially in regards to how they would affect a user’s interactions with the technology if it was deployed in a live system.</p>
<p>Then, please choose one of these steps to analyze in terms of possible negative impacts on potential direct and indirect users, and propose a test to evaluate the existence and magnitude of that impact. We provide examples of such analyses in the following two paragraphs.</p>
<p>Consider for example a model pre-trained on English Wikipedia and fine-tuned on a summarization dataset, and imagine that such a model is deployed in a news website to provide automatic summaries. Given the gender gap on Wikipedia, we can imagine two possible effects:</p>
<ol>
<li>the model could systematically produce summaries with lower ROUGE scores when compared to a reference for articles describing women than it would for articles describing men.</li>
<li>the model may be less likely to name the subject of an article if the subject is a woman than if the subject is a man.</li>
</ol>
<p>We can measure either of these effects by running an entity linking system on the articles, for example, and comparing subsets of the test set where the gender of the actors is known.</p>
<p>Alternatively, imagine that a system that is fine-tuned on WikiAuto, which aligns English Wikipedia text to its Simple English Wikipedia version, is used in an attempt to make a blog where the writer talks about their personal experience more accessible. How well does the model handle the shift from a third person to a first person point of view? One way we can check whether the behavior is roughly the same is by comparing the copy rate (number of words from the input that are re-used in the output) between the Wikipedia and blog setting.</p>
<p>We ask you to take a similar approach to analyzing your model:</p>
<h3 id="additional-data">Additional Data</h3>
<p><em>If you used a model that was pre-trained on additional data or used additional data, please describe it. Provide a link to a datasheet or data statement if there is one available, otherwise, provide as much relevant information as possible on the source of the data, the people represented in it, its languages, licensing, pre-processing, etc.</em></p>
<ol>
<li>
<p>WikiLarge Training data (Zhang and Lapata, 2017) (<a href="https://github.com/XingxingZhang/dress)">https://github.com/XingxingZhang/dress)</a>:</p>
<ul>
<li>Aligned complex-simple sentence pairs: 296,402</li>
<li>Created by aggregating several simplification corpora: Kauchak (2013), Woodsend and Lapata (2011), and the WikiSmall dataset (Zhu, 2010)</li>
<li>Languages: English</li>
<li>Licensing: The MIT License (MIT) Copyright © 2018 Zalando SE</li>
</ul>
</li>
<li>
<p>FastText pretrained word-vectors (Mikolov et. al., 2018) (<a href="https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip)">https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip)</a>:</p>
<ul>
<li>1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).</li>
<li>Language: English</li>
<li>License: Creative Commons Attribution-Share-Alike License 3.0.</li>
</ul>
</li>
<li>
<p>Flair Ontonotes NER Tagger (Schweter and Akbik, 2020) (<a href="https://github.com/flairNLP/flair)">https://github.com/flairNLP/flair)</a>:</p>
<ul>
<li>Uses train, development and test sets of CoNLL 2012 shared task for coreference resolution (Pradhan et al., 2012)</li>
<li>Language: English</li>
<li>License: The MIT License (MIT) Copyright © 2018 Zalando SE</li>
</ul>
</li>
</ol>
<h3 id="training-process">Training Process</h3>
<p><em>Describe the training or fine tuning setup, including whether the final model was trained on a single task or in a multi-task setting. If a data augmentation technique was used, describe the technique</em></p>
<p>The transformer model is trained (single-task) from scratch on the ~290k sentence pairs from the WikiLarge corpus (already having the named-entities replaced by special tokens), with pre-trained FastText embeddings loaded.</p>
<ol>
<li>Named-Entity tagging:</li>
</ol>
<p>Instances of named entities identified by Flair’s Ontonotes model, occurring on the complex half of the data, are replaced by respective special tokens:</p>
<p>Eg. New York --> LOC (tag predicted by NER model) --> LOC@1 (‘New York’ replaced by unique tag, while the respective token-tag mapping is saved sentence-wise for de-tagging after output generation)</p>
<ol start="2">
<li>Adding Control Tokens:</li>
</ol>
<p>Once the sentence pairs have been pruned and NER-tagged, four control attributes (Martin et. al., 2020) (<a href="https://arxiv.org/pdf/1910.02677">https://arxiv.org/pdf/1910.02677</a>) are calculated between each of the complex and simple sentence pairs:
- Compression Ratio: Simple ratio between token lengths of the simple and complex sentences. (recorded as: &#x3C;NbChars_x.xx>)
- Levenshtein Distance: Normalised character level Levenshtein distance between the complex and simple sentence. (recorded as: &#x3C;LevSim_x.xx> )
- Word Rank: Ratio of the average of the third-quartile log ranks of the words (excluding stopwords and special tokens) in the simple and complex sentence (extracted from the order of FastText word embeddings being used for training). (recorded as: &#x3C;WordRank_x.xx>)
- Dependency Tree Depth: Ratio of the maximum dependency tree depth of the simple and complex sentence. (recorded as: &#x3C;DepTreeDepth_x.xx>)</p>
<p>Eg.
Complex: it is particularly famous for the cultivation of kiwifruit .
Simple: it is mostly famous for the growing of kiwifruit .</p>
<p>Rewriting complex:
&#x3C;NbChars_0.8> &#x3C;LevSim_0.76> &#x3C;WordRank_0.79> it is particularly famous for the cultivation of kiwifruit .</p>
<h3 id="real-world-use">Real-World Use</h3>
<p><em>Describe a possible real-use application of your model, then choose one of the steps (e.g. choice of the pre-trained model, data source, data augmentation, training loss, etc.) above and describe a negative impact it may have on the user experience</em></p>
<p>Text Simplification has always found crucial importance in making it easier for people with cognitive disabilities such as aphasia, dyslexia and autism, to read and understand text. Additionally, it proves helpful for second language learners, especially in public service centres such as airports or health clinics. A model such as ours can be utilized to produce point-to-point simplifications for such people, and therefore must be made more accessible, but more importantly, must be made more reliable and faithful in terms of the original information/ message it must convey.  Even with nearly 300k aligned sentence pairs, train sets such as WikiLarge fail to help large transformer models being trained from scratch to generalize well,  which causes them to suffer from data sparsity, which can be observed with words in the vocabulary having very high word ranks (ordered by frequency), such as named entities. We attempted to mitigate this issue by replacing the named entities in our data with (18) special tokens before training, which are promptly replaced back with the original tokens. This helps to reduce the model vocabulary, and allows for greater generalization.  Although this step is beneficial to the overall performance of the model, it also brings with it a negative effect. The idea of replacing NEs is based on the assumption that such tokens or phrases need not be modified or deleted during the task of simplification. Although this is true for the most part when it comes to evaluation sets like the TurkCorpus and ASSET, potentially complex named entities need to be simplified or elaborated upon in real life uses, especially if the use involves second language learners or people with reading disabilities.  Eg. Consider an input statement as “Counters 3-6 are reserved for Commissioned Military Personnel.” Ideally, a simplified sentence should read something along the lines of “Counters 3-6 may be used only by army officers.” However, since Commissioned Military Personnel would be excluded from the simplification process by virtue of being a named entity, this simplification would not take place.</p>
<h3 id="measuring-impact">Measuring Impact</h3>
<p><em>Propose a method to test the magnitude of the impact identified in your previous answer</em></p>
<p>Gauging the need for elaboration or replacement of a named entity in a sentence requires a strong proficiency in the parent language and at least some knowledge of the named entity itself. Therefore this task is best suited for manual evaluation where annotators must be asked to rate the simplification upon how well were the constituent named entities simplified or elaborated. This can be done using a simple Likert-scale.</p>
<p>However, as manual evaluation is slow and resource intensive, there is a need for automated evaluation methods to approximate human judgment. A possible way to quantify the need for replacement or elaboration of named entities is the average similarity the NE holds to the other words in the sentence. If a NE with a high word rank shows very little similarity to the words in the sentence, it is likely that the sentence does not attempt to elaborate/ explain the NE. On the other hand, a high average similarity indicates that there are other words in the sentence that are related to the NE, and are most likely being used to elaborate upon the NE. A simple formulation of this measure can be the product of the word rank of the NE and its average similarity with other words in the sentence. Still, there is clearly a need for more nuanced methods of evaluation for text simplification, which capture the performance of a model on such attributes and expected operations.</p>
<h2 id="reproducibility">Reproducibility</h2>
<p><em>In this section, we ask you to provide any information that would be required for someone to reproduce your model and experimental results. These questions are derived from the suggested model card in [1] and the reproducibility checklist in [2].</em></p>
<p><em>[1] Dodge, Jesse, et al. "Show Your Work: Improved Reporting of Experimental Results." EMNLP. 2019.</em></p>
<p><em>[1] Mitchell et al. "Model cards for model reporting." Proceedings of the conference on fairness, accountability, and transparency. 2019.</em></p>
<h3 id="model-description-1">Model Description</h3>
<p><em>Provide basic information about your model, including (1) the model type (e.g., BART, Pointer Network), (2) model version/date if multiple versions are available, (3) training algorithms used. Please cite papers or other resources where further information about the model can be found. Also include the model license and citation details.</em></p>
<ul>
<li>
<p>Model architecture: Transformer (Vaswani et. al., 2017) (Facebook’s FairSequence Implementation)</p>
</li>
<li>
<p>Encoder and decoder layers: 6</p>
</li>
<li>
<p>Encoder and decoder attentions heads: 6</p>
</li>
<li>
<p>Encoder and decoder embedding dimensionality: 300</p>
</li>
<li>
<p>Encoder and decoder fully-connected layer dimensionality: 2048</p>
</li>
<li>
<p>Training Algorithm: Fairseq task set to ‘translation’. Complex and simple sentence pairs set as source and target language pairs for translation.</p>
</li>
<li>
<p>Version: <a href="mailto:fairseq@v0.10.0">fairseq@v0.10.0</a> (<a href="https://github.com/pytorch/fairseq@v0.10.0">https://github.com/pytorch/fairseq@v0.10.0</a>)</p>
</li>
<li>
<p>Model architecture citation:
@article{DBLP:journals/corr/VaswaniSPUJGKP17,
author    = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
title     = {Attention Is All You Need},
journal   = {CoRR},
volume    = {abs/1706.03762},
year      = {2017},
url       = {http://arxiv.org/abs/1706.03762},
archivePrefix = {arXiv},
eprint    = {1706.03762},
timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
bibsource = {dblp computer science bibliography, <a href="https://dblp.org%7D">https://dblp.org}</a>
}</p>
</li>
<li>
<p>Model implementation citation:
@inproceedings{ott2019fairseq,
title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
year = {2019},
}</p>
</li>
<li>
<p>Model implementation license: The MIT License (MIT) Copyright © 2018 Zalando SE (<a href="https://github.com/pytorch/fairseq/blob/master/LICENSE">https://github.com/pytorch/fairseq/blob/master/LICENSE</a>)</p>
</li>
</ul>
<h3 id="model-hyperparameters">Model Hyperparameters</h3>
<p><em>Provide the range of hyperparameters that would be required to reproduce your final model (e.g., optimizer used, number of epochs, learning rate, etc.). If hyperparameter search was used, please describe (1) the bounds for each hyperparameter, (2) the number of hyperparameter search trials, (3) the method for choosing hyperparameter values (e.g., uniform sampling, manual tuning, evolutionary optimization, etc.).</em></p>
<p>Number of epochs, learning rate, optimizer, dropout, number of warmup updates. (No hyperparameter search was used)</p>
<h3 id="the-hyperparameter-specifications-for-best-performing-models">The Hyperparameter specifications for best performing models</h3>
<ul>
<li>Number of epochs: 20 (best checkpoint: epoch_13)</li>
<li>Learning rate: 0.00011</li>
<li>Optimizer: adam (betas: (0.9, 0.999) ; eps: (1e-8))</li>
<li>Dropout: 0.2</li>
<li>Warmup updates: 4000</li>
</ul>
<h3 id="dataset-details">Dataset Details</h3>
<p><em>Include relevant training data statistics (e.g., number of samples used, whether some subsets of the dataset were discarded), the training/validation/test splits for the number of samples and any pre-processing steps if used.</em></p>
<ol>
<li>Training Dataset: WikiLarge (Zhang and Lapata, 2017) train set  - Aligned complex-simple sentence pairs: 296,402  - Link: <a href="https://github.com/XingxingZhang/dress">https://github.com/XingxingZhang/dress</a>  2. Validation and Testing Datasets:     a. TurkCorpus (Xu et. al., 2016) (<a href="https://gem-benchmark.com/data_cards/TURK)">https://gem-benchmark.com/data_cards/TURK)</a>:         i. Test split: 359 complex sentences each with 8 different simplifications.         ii. Validation split: 2000 complex sentences each with 8 different simplifications.     b. ASSET (Alva-Manchego et. al., 2020) (<a href="https://gem-benchmark.com/data_cards/ASSET)">https://gem-benchmark.com/data_cards/ASSET)</a>:          i. Test split: 359 complex sentences each with 10 different simplifications.         ii. Validation split: 2000 complex sentences each with 10 different simplifications.  3. Preprocessing:  - All three (train, validation and test) data splits were converted to lowercase.  - For train set (WikiLarge):      - Sentence pairs with either instance having token length lower than 3 were removed.      - Sentence pairs with compression ratio (len(target)/len(source)) out of the closed bounds [0.2, 1.5] were removed.  - For all three data splits, flair’s ontonotes NER tagger (<a href="https://github.com/flairNLP/flair">https://github.com/flairNLP/flair</a>) was used to replace all detectable named entities with respective special tokens (see system description) for training.</li>
</ol>
<h3 id="dependencies-and-external-libraries">Dependencies and External Libraries</h3>
<p><em>Include a specification of library dependencies</em></p>
<ul>
<li>torch==1.7.0</li>
<li>fairseq (git+https://github.com/pytorch/fairseq@v0.10.0)</li>
<li>easse (git+https://github.com/feralvam/easse.git)</li>
<li>datasets (datasets@git+https://github.com/huggingface/datasets.git@a5e45816a72fbb20dc9122f88238ef8acba43ee3)</li>
<li>flair</li>
<li>numpy==1.20.3</li>
<li>spacy ; en_core_web_sm</li>
<li>nltk ; stopwords</li>
<li>Levenshtein</li>
<li>wget</li>
</ul>
<p>(Included in the install script of the code repository.)</p>
<h3 id="link-to-downloadable-source-code">Link to downloadable source code</h3>
<p><a href="https://github.com/kvadityasrivatsa/gem_2021_simplification_task">https://github.com/kvadityasrivatsa/gem_2021_simplification_task</a></p>
<h3 id="computing-infrastructure-used">Computing Infrastructure Used</h3>
<p><em>Describe the computing infrastructure used to train your model (e.g., number of GPUs, GPU type and vRAM) and the time taken to train your final model.</em></p>
<ul>
<li>Number of GPUs: 4</li>
<li>Type of GPUs: Nvidia GeForce GTX 1080 Ti</li>
<li>vRAM: 64GB</li>
<li>Time taken for training (for 20 epochs): ~27 hours</li>
</ul>
<h3 id="evaluation-details">Evaluation Details</h3>
<p><em>How were your models evaluated? Please include evaluation metric details (including links to code), train/validation/test splits, and model performance on both test and validation sets. If more than one model was trained and evaluated, what was the number of training and evaluation runs, and the variance in scores? If human evaluation was used, please describe the experimental setup.</em></p>
<p>The model was trained on the WikiLarge corpus and evaluated on the validation and test sets of TurkCorups (Xu et. al., 2016) (<a href="https://gem-benchmark.com/data_cards/TURK">https://gem-benchmark.com/data_cards/TURK</a>) &#x26; (<a href="https://huggingface.co/datasets/turk">https://huggingface.co/datasets/turk</a>) as well as the ASSET (Alva-Manchego et. al., 2020) (<a href="https://gem-benchmark.com/data_cards/ASSET">https://gem-benchmark.com/data_cards/ASSET</a>) &#x26; (<a href="https://huggingface.co/datasets/asset">https://huggingface.co/datasets/asset</a>) dataset. The evaluation metrics used were BLEU (Papineni et. al., 2002) and SARI (Xu et. al., 2016), with the best model checkpoint chosen according to the respective SARI score on the validation sets. The evaluation for both metrics were carried out using the EASSE (Alva-Manchego et. al., 2019) package (link: <a href="https://github.com/feralvam/easse)">https://github.com/feralvam/easse)</a>.</p>
<p>Model Performance (on automatic metrics BLEU and SARI):</p>
<ul>
<li>on ASSET-test | BLEU: 66.722 | SARI: 38.922</li>
<li>on ASSET-validation | BLEU: 73.935 | SARI: 37.588</li>
<li>on TurkCorpus-test | BLEU: 67.667 | SARI: 39.695</li>
<li>on TurkCorpus-validation | BLEU: 75.672 | SARI: 39.407</li>
</ul>
</div></article></main><div class="layout_push__1J9g0"></div></div><footer class="layout_footer__127N0 utils_eggshell__Njxsh"><span class="layout_backToHome__1vZsp"><a href="/">← Home</a></span><span>If you have any questions, please join our <a href="https://groups.google.com/g/gem-benchmark" target="_blank" class="utils_accentUnderline__k083p">google group</a> for support.</span></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"taskData":{"id":"SimpleNER","contentHtml":"\u003ch2 id=\"table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#model-description\"\u003eModel Description\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#social-impact\"\u003eSocial Impact\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#additional-data\"\u003eAdditional Data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#training-process\"\u003eTraining Process\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#real-world-use\"\u003eReal-World Use\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#measuring-impact\"\u003eMeasuring Impact\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#reproducibility\"\u003eReproducibility\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#model-description-1\"\u003eModel Description\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#model-hyperparameters\"\u003eModel Hyperparameters\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#the-hyperparameter-specifications-for-best-performing-models\"\u003eThe Hyperparameter specifications for best performing models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dataset-details\"\u003eDataset Details\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dependencies-and-external-libraries\"\u003eDependencies and External Libraries\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#link-to-downloadable-source-code\"\u003eLink to downloadable source code\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#computing-infrastructure-used\"\u003eComputing Infrastructure Used\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#evaluation-details\"\u003eEvaluation Details\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"model-description\"\u003eModel Description\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePaper:\u003c/strong\u003e \u003ca href=\"https://aclanthology.org/2021.gem-1.14/\"\u003eSimpleNER Sentence Simplification System for GEM 2021\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCreators:\u003c/strong\u003e K V Aditya Srivatsa, Monil Gokani, Manish Shrivastava\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoint of Contact:\u003c/strong\u003e K V Aditya Srivatsa (\u003ca href=\"mailto:k.v.aditya@research.iiit.ac.in\"\u003ek.v.aditya@research.iiit.ac.in\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"social-impact\"\u003eSocial Impact\u003c/h2\u003e\n\u003cp\u003eIn this section, we ask you to provide information on all of the steps that went into obtaining your models, especially in regards to how they would affect a user’s interactions with the technology if it was deployed in a live system.\u003c/p\u003e\n\u003cp\u003eThen, please choose one of these steps to analyze in terms of possible negative impacts on potential direct and indirect users, and propose a test to evaluate the existence and magnitude of that impact. We provide examples of such analyses in the following two paragraphs.\u003c/p\u003e\n\u003cp\u003eConsider for example a model pre-trained on English Wikipedia and fine-tuned on a summarization dataset, and imagine that such a model is deployed in a news website to provide automatic summaries. Given the gender gap on Wikipedia, we can imagine two possible effects:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ethe model could systematically produce summaries with lower ROUGE scores when compared to a reference for articles describing women than it would for articles describing men.\u003c/li\u003e\n\u003cli\u003ethe model may be less likely to name the subject of an article if the subject is a woman than if the subject is a man.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWe can measure either of these effects by running an entity linking system on the articles, for example, and comparing subsets of the test set where the gender of the actors is known.\u003c/p\u003e\n\u003cp\u003eAlternatively, imagine that a system that is fine-tuned on WikiAuto, which aligns English Wikipedia text to its Simple English Wikipedia version, is used in an attempt to make a blog where the writer talks about their personal experience more accessible. How well does the model handle the shift from a third person to a first person point of view? One way we can check whether the behavior is roughly the same is by comparing the copy rate (number of words from the input that are re-used in the output) between the Wikipedia and blog setting.\u003c/p\u003e\n\u003cp\u003eWe ask you to take a similar approach to analyzing your model:\u003c/p\u003e\n\u003ch3 id=\"additional-data\"\u003eAdditional Data\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eIf you used a model that was pre-trained on additional data or used additional data, please describe it. Provide a link to a datasheet or data statement if there is one available, otherwise, provide as much relevant information as possible on the source of the data, the people represented in it, its languages, licensing, pre-processing, etc.\u003c/em\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eWikiLarge Training data (Zhang and Lapata, 2017) (\u003ca href=\"https://github.com/XingxingZhang/dress)\"\u003ehttps://github.com/XingxingZhang/dress)\u003c/a\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAligned complex-simple sentence pairs: 296,402\u003c/li\u003e\n\u003cli\u003eCreated by aggregating several simplification corpora: Kauchak (2013), Woodsend and Lapata (2011), and the WikiSmall dataset (Zhu, 2010)\u003c/li\u003e\n\u003cli\u003eLanguages: English\u003c/li\u003e\n\u003cli\u003eLicensing: The MIT License (MIT) Copyright © 2018 Zalando SE\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFastText pretrained word-vectors (Mikolov et. al., 2018) (\u003ca href=\"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip)\"\u003ehttps://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip)\u003c/a\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\u003c/li\u003e\n\u003cli\u003eLanguage: English\u003c/li\u003e\n\u003cli\u003eLicense: Creative Commons Attribution-Share-Alike License 3.0.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFlair Ontonotes NER Tagger (Schweter and Akbik, 2020) (\u003ca href=\"https://github.com/flairNLP/flair)\"\u003ehttps://github.com/flairNLP/flair)\u003c/a\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUses train, development and test sets of CoNLL 2012 shared task for coreference resolution (Pradhan et al., 2012)\u003c/li\u003e\n\u003cli\u003eLanguage: English\u003c/li\u003e\n\u003cli\u003eLicense: The MIT License (MIT) Copyright © 2018 Zalando SE\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"training-process\"\u003eTraining Process\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eDescribe the training or fine tuning setup, including whether the final model was trained on a single task or in a multi-task setting. If a data augmentation technique was used, describe the technique\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThe transformer model is trained (single-task) from scratch on the ~290k sentence pairs from the WikiLarge corpus (already having the named-entities replaced by special tokens), with pre-trained FastText embeddings loaded.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eNamed-Entity tagging:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eInstances of named entities identified by Flair’s Ontonotes model, occurring on the complex half of the data, are replaced by respective special tokens:\u003c/p\u003e\n\u003cp\u003eEg. New York --\u003e LOC (tag predicted by NER model) --\u003e LOC@1 (‘New York’ replaced by unique tag, while the respective token-tag mapping is saved sentence-wise for de-tagging after output generation)\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eAdding Control Tokens:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eOnce the sentence pairs have been pruned and NER-tagged, four control attributes (Martin et. al., 2020) (\u003ca href=\"https://arxiv.org/pdf/1910.02677\"\u003ehttps://arxiv.org/pdf/1910.02677\u003c/a\u003e) are calculated between each of the complex and simple sentence pairs:\n- Compression Ratio: Simple ratio between token lengths of the simple and complex sentences. (recorded as: \u0026#x3C;NbChars_x.xx\u003e)\n- Levenshtein Distance: Normalised character level Levenshtein distance between the complex and simple sentence. (recorded as: \u0026#x3C;LevSim_x.xx\u003e )\n- Word Rank: Ratio of the average of the third-quartile log ranks of the words (excluding stopwords and special tokens) in the simple and complex sentence (extracted from the order of FastText word embeddings being used for training). (recorded as: \u0026#x3C;WordRank_x.xx\u003e)\n- Dependency Tree Depth: Ratio of the maximum dependency tree depth of the simple and complex sentence. (recorded as: \u0026#x3C;DepTreeDepth_x.xx\u003e)\u003c/p\u003e\n\u003cp\u003eEg.\nComplex: it is particularly famous for the cultivation of kiwifruit .\nSimple: it is mostly famous for the growing of kiwifruit .\u003c/p\u003e\n\u003cp\u003eRewriting complex:\n\u0026#x3C;NbChars_0.8\u003e \u0026#x3C;LevSim_0.76\u003e \u0026#x3C;WordRank_0.79\u003e it is particularly famous for the cultivation of kiwifruit .\u003c/p\u003e\n\u003ch3 id=\"real-world-use\"\u003eReal-World Use\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eDescribe a possible real-use application of your model, then choose one of the steps (e.g. choice of the pre-trained model, data source, data augmentation, training loss, etc.) above and describe a negative impact it may have on the user experience\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eText Simplification has always found crucial importance in making it easier for people with cognitive disabilities such as aphasia, dyslexia and autism, to read and understand text. Additionally, it proves helpful for second language learners, especially in public service centres such as airports or health clinics. A model such as ours can be utilized to produce point-to-point simplifications for such people, and therefore must be made more accessible, but more importantly, must be made more reliable and faithful in terms of the original information/ message it must convey.  Even with nearly 300k aligned sentence pairs, train sets such as WikiLarge fail to help large transformer models being trained from scratch to generalize well,  which causes them to suffer from data sparsity, which can be observed with words in the vocabulary having very high word ranks (ordered by frequency), such as named entities. We attempted to mitigate this issue by replacing the named entities in our data with (18) special tokens before training, which are promptly replaced back with the original tokens. This helps to reduce the model vocabulary, and allows for greater generalization.  Although this step is beneficial to the overall performance of the model, it also brings with it a negative effect. The idea of replacing NEs is based on the assumption that such tokens or phrases need not be modified or deleted during the task of simplification. Although this is true for the most part when it comes to evaluation sets like the TurkCorpus and ASSET, potentially complex named entities need to be simplified or elaborated upon in real life uses, especially if the use involves second language learners or people with reading disabilities.  Eg. Consider an input statement as “Counters 3-6 are reserved for Commissioned Military Personnel.” Ideally, a simplified sentence should read something along the lines of “Counters 3-6 may be used only by army officers.” However, since Commissioned Military Personnel would be excluded from the simplification process by virtue of being a named entity, this simplification would not take place.\u003c/p\u003e\n\u003ch3 id=\"measuring-impact\"\u003eMeasuring Impact\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003ePropose a method to test the magnitude of the impact identified in your previous answer\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eGauging the need for elaboration or replacement of a named entity in a sentence requires a strong proficiency in the parent language and at least some knowledge of the named entity itself. Therefore this task is best suited for manual evaluation where annotators must be asked to rate the simplification upon how well were the constituent named entities simplified or elaborated. This can be done using a simple Likert-scale.\u003c/p\u003e\n\u003cp\u003eHowever, as manual evaluation is slow and resource intensive, there is a need for automated evaluation methods to approximate human judgment. A possible way to quantify the need for replacement or elaboration of named entities is the average similarity the NE holds to the other words in the sentence. If a NE with a high word rank shows very little similarity to the words in the sentence, it is likely that the sentence does not attempt to elaborate/ explain the NE. On the other hand, a high average similarity indicates that there are other words in the sentence that are related to the NE, and are most likely being used to elaborate upon the NE. A simple formulation of this measure can be the product of the word rank of the NE and its average similarity with other words in the sentence. Still, there is clearly a need for more nuanced methods of evaluation for text simplification, which capture the performance of a model on such attributes and expected operations.\u003c/p\u003e\n\u003ch2 id=\"reproducibility\"\u003eReproducibility\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eIn this section, we ask you to provide any information that would be required for someone to reproduce your model and experimental results. These questions are derived from the suggested model card in [1] and the reproducibility checklist in [2].\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e[1] Dodge, Jesse, et al. \"Show Your Work: Improved Reporting of Experimental Results.\" EMNLP. 2019.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e[1] Mitchell et al. \"Model cards for model reporting.\" Proceedings of the conference on fairness, accountability, and transparency. 2019.\u003c/em\u003e\u003c/p\u003e\n\u003ch3 id=\"model-description-1\"\u003eModel Description\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eProvide basic information about your model, including (1) the model type (e.g., BART, Pointer Network), (2) model version/date if multiple versions are available, (3) training algorithms used. Please cite papers or other resources where further information about the model can be found. Also include the model license and citation details.\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eModel architecture: Transformer (Vaswani et. al., 2017) (Facebook’s FairSequence Implementation)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEncoder and decoder layers: 6\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEncoder and decoder attentions heads: 6\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEncoder and decoder embedding dimensionality: 300\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEncoder and decoder fully-connected layer dimensionality: 2048\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTraining Algorithm: Fairseq task set to ‘translation’. Complex and simple sentence pairs set as source and target language pairs for translation.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eVersion: \u003ca href=\"mailto:fairseq@v0.10.0\"\u003efairseq@v0.10.0\u003c/a\u003e (\u003ca href=\"https://github.com/pytorch/fairseq@v0.10.0\"\u003ehttps://github.com/pytorch/fairseq@v0.10.0\u003c/a\u003e)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eModel architecture citation:\n@article{DBLP:journals/corr/VaswaniSPUJGKP17,\nauthor    = {Ashish Vaswani and\nNoam Shazeer and\nNiki Parmar and\nJakob Uszkoreit and\nLlion Jones and\nAidan N. Gomez and\nLukasz Kaiser and\nIllia Polosukhin},\ntitle     = {Attention Is All You Need},\njournal   = {CoRR},\nvolume    = {abs/1706.03762},\nyear      = {2017},\nurl       = {http://arxiv.org/abs/1706.03762},\narchivePrefix = {arXiv},\neprint    = {1706.03762},\ntimestamp = {Sat, 23 Jan 2021 01:20:40 +0100},\nbiburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},\nbibsource = {dblp computer science bibliography, \u003ca href=\"https://dblp.org%7D\"\u003ehttps://dblp.org}\u003c/a\u003e\n}\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eModel implementation citation:\n@inproceedings{ott2019fairseq,\ntitle = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},\nauthor = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},\nbooktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},\nyear = {2019},\n}\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eModel implementation license: The MIT License (MIT) Copyright © 2018 Zalando SE (\u003ca href=\"https://github.com/pytorch/fairseq/blob/master/LICENSE\"\u003ehttps://github.com/pytorch/fairseq/blob/master/LICENSE\u003c/a\u003e)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"model-hyperparameters\"\u003eModel Hyperparameters\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eProvide the range of hyperparameters that would be required to reproduce your final model (e.g., optimizer used, number of epochs, learning rate, etc.). If hyperparameter search was used, please describe (1) the bounds for each hyperparameter, (2) the number of hyperparameter search trials, (3) the method for choosing hyperparameter values (e.g., uniform sampling, manual tuning, evolutionary optimization, etc.).\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eNumber of epochs, learning rate, optimizer, dropout, number of warmup updates. (No hyperparameter search was used)\u003c/p\u003e\n\u003ch3 id=\"the-hyperparameter-specifications-for-best-performing-models\"\u003eThe Hyperparameter specifications for best performing models\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eNumber of epochs: 20 (best checkpoint: epoch_13)\u003c/li\u003e\n\u003cli\u003eLearning rate: 0.00011\u003c/li\u003e\n\u003cli\u003eOptimizer: adam (betas: (0.9, 0.999) ; eps: (1e-8))\u003c/li\u003e\n\u003cli\u003eDropout: 0.2\u003c/li\u003e\n\u003cli\u003eWarmup updates: 4000\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"dataset-details\"\u003eDataset Details\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eInclude relevant training data statistics (e.g., number of samples used, whether some subsets of the dataset were discarded), the training/validation/test splits for the number of samples and any pre-processing steps if used.\u003c/em\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTraining Dataset: WikiLarge (Zhang and Lapata, 2017) train set  - Aligned complex-simple sentence pairs: 296,402  - Link: \u003ca href=\"https://github.com/XingxingZhang/dress\"\u003ehttps://github.com/XingxingZhang/dress\u003c/a\u003e  2. Validation and Testing Datasets:     a. TurkCorpus (Xu et. al., 2016) (\u003ca href=\"https://gem-benchmark.com/data_cards/TURK)\"\u003ehttps://gem-benchmark.com/data_cards/TURK)\u003c/a\u003e:         i. Test split: 359 complex sentences each with 8 different simplifications.         ii. Validation split: 2000 complex sentences each with 8 different simplifications.     b. ASSET (Alva-Manchego et. al., 2020) (\u003ca href=\"https://gem-benchmark.com/data_cards/ASSET)\"\u003ehttps://gem-benchmark.com/data_cards/ASSET)\u003c/a\u003e:          i. Test split: 359 complex sentences each with 10 different simplifications.         ii. Validation split: 2000 complex sentences each with 10 different simplifications.  3. Preprocessing:  - All three (train, validation and test) data splits were converted to lowercase.  - For train set (WikiLarge):      - Sentence pairs with either instance having token length lower than 3 were removed.      - Sentence pairs with compression ratio (len(target)/len(source)) out of the closed bounds [0.2, 1.5] were removed.  - For all three data splits, flair’s ontonotes NER tagger (\u003ca href=\"https://github.com/flairNLP/flair\"\u003ehttps://github.com/flairNLP/flair\u003c/a\u003e) was used to replace all detectable named entities with respective special tokens (see system description) for training.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"dependencies-and-external-libraries\"\u003eDependencies and External Libraries\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eInclude a specification of library dependencies\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etorch==1.7.0\u003c/li\u003e\n\u003cli\u003efairseq (git+https://github.com/pytorch/fairseq@v0.10.0)\u003c/li\u003e\n\u003cli\u003eeasse (git+https://github.com/feralvam/easse.git)\u003c/li\u003e\n\u003cli\u003edatasets (datasets@git+https://github.com/huggingface/datasets.git@a5e45816a72fbb20dc9122f88238ef8acba43ee3)\u003c/li\u003e\n\u003cli\u003eflair\u003c/li\u003e\n\u003cli\u003enumpy==1.20.3\u003c/li\u003e\n\u003cli\u003espacy ; en_core_web_sm\u003c/li\u003e\n\u003cli\u003enltk ; stopwords\u003c/li\u003e\n\u003cli\u003eLevenshtein\u003c/li\u003e\n\u003cli\u003ewget\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e(Included in the install script of the code repository.)\u003c/p\u003e\n\u003ch3 id=\"link-to-downloadable-source-code\"\u003eLink to downloadable source code\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/kvadityasrivatsa/gem_2021_simplification_task\"\u003ehttps://github.com/kvadityasrivatsa/gem_2021_simplification_task\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"computing-infrastructure-used\"\u003eComputing Infrastructure Used\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eDescribe the computing infrastructure used to train your model (e.g., number of GPUs, GPU type and vRAM) and the time taken to train your final model.\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNumber of GPUs: 4\u003c/li\u003e\n\u003cli\u003eType of GPUs: Nvidia GeForce GTX 1080 Ti\u003c/li\u003e\n\u003cli\u003evRAM: 64GB\u003c/li\u003e\n\u003cli\u003eTime taken for training (for 20 epochs): ~27 hours\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"evaluation-details\"\u003eEvaluation Details\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eHow were your models evaluated? Please include evaluation metric details (including links to code), train/validation/test splits, and model performance on both test and validation sets. If more than one model was trained and evaluated, what was the number of training and evaluation runs, and the variance in scores? If human evaluation was used, please describe the experimental setup.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThe model was trained on the WikiLarge corpus and evaluated on the validation and test sets of TurkCorups (Xu et. al., 2016) (\u003ca href=\"https://gem-benchmark.com/data_cards/TURK\"\u003ehttps://gem-benchmark.com/data_cards/TURK\u003c/a\u003e) \u0026#x26; (\u003ca href=\"https://huggingface.co/datasets/turk\"\u003ehttps://huggingface.co/datasets/turk\u003c/a\u003e) as well as the ASSET (Alva-Manchego et. al., 2020) (\u003ca href=\"https://gem-benchmark.com/data_cards/ASSET\"\u003ehttps://gem-benchmark.com/data_cards/ASSET\u003c/a\u003e) \u0026#x26; (\u003ca href=\"https://huggingface.co/datasets/asset\"\u003ehttps://huggingface.co/datasets/asset\u003c/a\u003e) dataset. The evaluation metrics used were BLEU (Papineni et. al., 2002) and SARI (Xu et. al., 2016), with the best model checkpoint chosen according to the respective SARI score on the validation sets. The evaluation for both metrics were carried out using the EASSE (Alva-Manchego et. al., 2019) package (link: \u003ca href=\"https://github.com/feralvam/easse)\"\u003ehttps://github.com/feralvam/easse)\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eModel Performance (on automatic metrics BLEU and SARI):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eon ASSET-test | BLEU: 66.722 | SARI: 38.922\u003c/li\u003e\n\u003cli\u003eon ASSET-validation | BLEU: 73.935 | SARI: 37.588\u003c/li\u003e\n\u003cli\u003eon TurkCorpus-test | BLEU: 67.667 | SARI: 39.695\u003c/li\u003e\n\u003cli\u003eon TurkCorpus-validation | BLEU: 75.672 | SARI: 39.407\u003c/li\u003e\n\u003c/ul\u003e\n","title":"SimpleNER","type":"Shared Task 2021","background":"Finetuned Transformer architecture for simplification."}},"__N_SSG":true},"page":"/model_cards/[id]","query":{"id":"SimpleNER"},"buildId":"OFsWCK0TI6Vd_cFn0NHzz","nextExport":false,"isFallback":false,"gsp":true,"head":[["meta",{"name":"viewport","content":"width=device-width"}],["meta",{"charSet":"utf-8"}],["link",{"rel":"icon","href":"/favicon.ico"}],["meta",{"name":"description","content":"Benchmark natural language generation systems with GEM."}],["meta",{"property":"og:image","content":"https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light\u0026md=1\u0026fontSize=100px\u0026images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"}],["meta",{"name":"og:title","content":"GEM"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["title",{"children":"GEM SimpleNER"}]]}</script><script nomodule="" src="/_next/static/chunks/polyfills-e69cc13a7e89296a69e4.js"></script><script src="/_next/static/chunks/main-47bc8f80085b54a800da.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.baa41d4dbf5d52db897c.js" async=""></script><script src="/_next/static/chunks/26846a7fd70214140cb220fed85dd1cca1639cdd.4a36a385313236c59b19.js" async=""></script><script src="/_next/static/chunks/pages/_app-a9ae7a6d1de4e51a7ab6.js" async=""></script><script src="/_next/static/chunks/cb1608f2.c3a9f0eb95374ca4919a.js" async=""></script><script src="/_next/static/chunks/c32b6ed53981e30c999312738987180eb6bb48f7.50e5606a166dbfda69c2.js" async=""></script><script src="/_next/static/chunks/pages/model_cards/%5Bid%5D-fc0c817a6908d1f88fd5.js" async=""></script><script src="/_next/static/OFsWCK0TI6Vd_cFn0NHzz/_buildManifest.js" async=""></script><script src="/_next/static/OFsWCK0TI6Vd_cFn0NHzz/_ssgManifest.js" async=""></script></body></html>