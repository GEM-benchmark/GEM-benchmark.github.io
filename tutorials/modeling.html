<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="Benchmark natural language generation systems with GEM."/><meta property="og:image" content="https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"/><meta name="og:title" content="GEM"/><meta name="twitter:card" content="summary_large_image"/><title>GEM From pretrained model to submission</title><link rel="preload" href="/_next/static/css/2786522978a02f025205.css" as="style"/><link rel="stylesheet" href="/_next/static/css/2786522978a02f025205.css" data-n-g=""/><link rel="preload" href="/_next/static/css/f2fce7b83fe6ca04479b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f2fce7b83fe6ca04479b.css" data-n-p=""/><noscript data-n-css="true"></noscript><link rel="preload" href="/_next/static/chunks/main-47bc8f80085b54a800da.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.baa41d4dbf5d52db897c.js" as="script"/><link rel="preload" href="/_next/static/chunks/e70fad557dfa42f32a11d0d2c99fe8f6e8d1fa86.4a36a385313236c59b19.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-a9ae7a6d1de4e51a7ab6.js" as="script"/><link rel="preload" href="/_next/static/chunks/cb1608f2.c3a9f0eb95374ca4919a.js" as="script"/><link rel="preload" href="/_next/static/chunks/451c6be158cef50d8cc28b919cf08d1e5b9ff3fc.f0ec181e43727e8a893e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/tutorials/%5Bid%5D-c1ef01944be3b5e528b1.js" as="script"/></head><body><div id="__next"><div class="layout_background__1AVEa undefined"><header class="layout_header__2rhWq"><div class="navbar_navwrapper__15zia"><div class="navbar_gradbar__1Xi5u"></div><nav class="navbar_navbar__3gnco"><span class="utils_headingLg__de7p0 navbar_navbarlogo__PLEwr"><a href="/">GEM BENCHMARK</a></span><div class="navbar_menutoggle__358pJ" id="mobile-menu"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars fa-w-14 navbar_bar__QVPSR" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"></path></svg></div><ul><li class="navbar_navitem__3ICSG navbar_pushright__3G2DM"><a href="/resources">Resources</a></li><li class="navbar_navitem__3ICSG"><a href="/data_cards">Data Cards</a></li><li class="navbar_navitem__3ICSG"><a href="/model_cards">Model Cards</a></li><li class="navbar_navitem__3ICSG"><a href="/tutorials">tutorials</a></li><li class="navbar_navitem__3ICSG"><a href="/results">Results</a></li><li class="navbar_navitem__3ICSG"><a href="/papers">Papers</a></li><li class="navbar_navitem__3ICSG"><a href="/team">Team</a></li><li class="navbar_navitem__3ICSG"><a href="/nl_augmenter">NL-Augmenter</a></li><li class="navbar_navitem__3ICSG"><a href="/workshop">Workshop</a></li></ul></nav></div></header><div class="layout_container__2t4v2"><main><article><span class="utils_headingXl__1XecN">From pretrained model to submission</span><span class="utils_smallSpace__375iy"></span><span class="utils_lightText__12Ckm">Modeling</span><div><p>This tutorial presents a full walk-through on how to get started with GEM, how to load and inspect data, how to finetune a baseline model, and how to generate predictions.
Throughout this tutorial, we will focus on the CommonGen task, but we will note
what changes to make to use another of the <a href="/data_cards">GEM datasets</a>.</p>
<p>You can also run this tutorial as a notebook <a href="https://colab.research.google.com/drive/1iREkGABObpdluTBNAnLhvyABEtdbLokT?usp=sharing">here</a>.</p>
<p><strong>SUBMITTING</strong> Our <a href="https://forms.gle/vbTZDMCuqzok8tTA9">submission form</a> is permanently open! Please account for some extra time to write your model card.</p>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#preliminaries">Preliminaries</a></li>
<li><a href="#loading-the-data">Loading the data</a>
<ul>
<li><a href="#loading-a-single-example">Loading a single example</a></li>
</ul>
</li>
<li><a href="#finetuning-a-pretrained-model">Finetuning a pretrained model</a></li>
<li><a href="#generating-and-evaluating-predictions">Generating and evaluating predictions</a></li>
<li><a href="#generating-and-submitting-test-predictions">Generating and submitting test predictions</a>
<ul>
<li><a href="#format-description">Format description</a></li>
<li><a href="#formatting-your-predictions">Formatting Your Predictions</a></li>
</ul>
</li>
<li><a href="#evaluating-your-submission-file-with-the-gem-evaluation-framework">Evaluating your submission file with the GEM evaluation framework</a></li>
</ul>
<h2 id="preliminaries">Preliminaries</h2>
<p>This tutorial uses PyTorch and the HuggingFace infrastructure to finetune models.
You need to install the following dependencies:</p>
<pre><code class="hljs language-bash">pip install git+https://github.com/huggingface/datasets.git
pip install rouge_score
pip install sentencepiece
pip install transformers</code></pre>
<p>We recommend you use a GPU machine. You should be able to run all the code inside of a <a href="https://colab.research.google.com/">colab notebook for free GPU access</a>.</p>
<h2 id="loading-the-data">Loading the data</h2>
<p>We will be using <a href="https://huggingface.co/datasets/gem">HuggingFace datasets</a>, but the GEM datasets are available in <a href="https://www.tensorflow.org/datasets">TFDS</a> as well.</p>
<p>You can load and inspect datasets like this:</p>
<pre><code class="hljs language-python">>> <span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
>> DATASET_NAME = <span class="hljs-string">"common_gen"</span>
>> data = load_dataset(<span class="hljs-string">"gem"</span>, DATASET_NAME)
>> data

DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">'gem_id'</span>, <span class="hljs-string">'gem_parent_id'</span>, <span class="hljs-string">'concept_set_id'</span>, <span class="hljs-string">'concepts'</span>, <span class="hljs-string">'target'</span>, <span class="hljs-string">'references'</span>],
        num_rows: <span class="hljs-number">67389</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">'gem_id'</span>, <span class="hljs-string">'gem_parent_id'</span>, <span class="hljs-string">'concept_set_id'</span>, <span class="hljs-string">'concepts'</span>, <span class="hljs-string">'target'</span>, <span class="hljs-string">'references'</span>],
        num_rows: <span class="hljs-number">993</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">'gem_id'</span>, <span class="hljs-string">'gem_parent_id'</span>, <span class="hljs-string">'concept_set_id'</span>, <span class="hljs-string">'concepts'</span>, <span class="hljs-string">'target'</span>, <span class="hljs-string">'references'</span>],
        num_rows: <span class="hljs-number">1497</span>
    })
    challenge_train_sample: Dataset({
        features: [<span class="hljs-string">'gem_id'</span>, <span class="hljs-string">'gem_parent_id'</span>, <span class="hljs-string">'concept_set_id'</span>, <span class="hljs-string">'concepts'</span>, <span class="hljs-string">'target'</span>, <span class="hljs-string">'references'</span>],
        num_rows: <span class="hljs-number">500</span>
    })
    challenge_validation_sample: Dataset({
        features: [<span class="hljs-string">'gem_id'</span>, <span class="hljs-string">'gem_parent_id'</span>, <span class="hljs-string">'concept_set_id'</span>, <span class="hljs-string">'concepts'</span>, <span class="hljs-string">'target'</span>, <span class="hljs-string">'references'</span>],
        num_rows: <span class="hljs-number">500</span>
    })
    challenge_test_scramble: Dataset({
        features: [<span class="hljs-string">'gem_id'</span>, <span class="hljs-string">'gem_parent_id'</span>, <span class="hljs-string">'concept_set_id'</span>, <span class="hljs-string">'concepts'</span>, <span class="hljs-string">'target'</span>, <span class="hljs-string">'references'</span>],
        num_rows: <span class="hljs-number">500</span>
    })
})</code></pre>
<p>You can notice that challenge sets created as part of GEM act just like any other data split, which means that you can use them with exactly the same code!</p>
<p>GEM supports many other datasets, simply pick one from this list and check out the corresponding <a href="/data_cards">data cards</a>.</p>
<h3 id="loading-a-single-example">Loading a single example</h3>
<p>Now let's look at a single example:</p>
<pre><code class="hljs language-python">>> data[<span class="hljs-string">'train'</span>][<span class="hljs-number">0</span>]

{
    <span class="hljs-string">'concept_set_id'</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">'concepts'</span>: [<span class="hljs-string">'mountain'</span>, <span class="hljs-string">'ski'</span>, <span class="hljs-string">'skier'</span>],
    <span class="hljs-string">'gem_id'</span>: <span class="hljs-string">'common_gen-train-0'</span>,
    <span class="hljs-string">'references'</span>: [],
    <span class="hljs-string">'target'</span>: <span class="hljs-string">'Skier skis down the mountain'</span>
}</code></pre>
<p>CommonGen is a task that asks for the production of a sentence (<code>target</code>) from a set of concepts (<code>concepts</code>). Since one concept set can generate multiple meaningful sentences, the example also includes a unique identifier (<code>concept_set_id</code>) so that multiple references can be linked to an input.</p>
<p>Next, let's define utility functions that can generate batches of (tokenized) examples which we can use during training.</p>
<p>We create a function that takes a batch from a dataset and constructs the corresponding input string. In our CommonGen example, we concatenate concepts into a single string for each instance.</p>
<pre><code class="hljs language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">construct_input_for_batch</span>(<span class="hljs-params">batch</span>):</span>
    <span class="hljs-string">"""Construct input strings from a batch."""</span>
    source = [<span class="hljs-string">' '</span>.join(concepts) <span class="hljs-keyword">for</span> concepts <span class="hljs-keyword">in</span> batch[<span class="hljs-string">"concepts"</span>]]
    target = batch[<span class="hljs-string">"target"</span>]
    <span class="hljs-keyword">return</span> source, target</code></pre>
<p>We then create a function that tokenizes the batches. Depending on your task, you might want to consider adjusting the <code>max_length</code>.</p>
<pre><code class="hljs language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">batch_tokenize</span>(<span class="hljs-params">batch, tokenizer, max_length=<span class="hljs-number">32</span></span>):</span>
    <span class="hljs-string">"""Construct the batch (source, target) and run them through a tokenizer."""</span>
    source, target = construct_input_for_batch(batch)
    res = {
        <span class="hljs-string">"input_ids"</span>: tokenizer(source)[<span class="hljs-string">"input_ids"</span>],
        <span class="hljs-string">"labels"</span>: tokenizer(
            target,
            padding=<span class="hljs-string">"max_length"</span>,
            truncation=<span class="hljs-literal">True</span>,
            max_length=max_length
        )[<span class="hljs-string">"input_ids"</span>],
    }
    <span class="hljs-keyword">return</span> res</code></pre>
<p>All we need to do now to preprocess the dataset is to call <code>batch_tokenize</code> on it. For our example, we are using BART-base as a model and we need to load the corresponding tokenizer.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

MODEL_NAME = <span class="hljs-string">"facebook/bart-base"</span>
MAX_LENGTH = <span class="hljs-number">32</span>

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

train_data_tokenized = data[<span class="hljs-string">'train'</span>].<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> batch: batch_tokenize(batch, tokenizer, max_length=MAX_LENGTH),
    batched=<span class="hljs-literal">True</span>
)
valid_data_tokenized = data[<span class="hljs-string">'validation'</span>].<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> batch: batch_tokenize(batch, tokenizer, max_length=MAX_LENGTH),
    batched=<span class="hljs-literal">True</span>
)</code></pre>
<h2 id="finetuning-a-pretrained-model">Finetuning a pretrained model</h2>
<p>We can now utilize the preprocessed data to finetune a model. To do so, we will use the <a href="https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainingarguments">Trainer API</a> which handles gradient updates, model selection, and evaluation for us.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments</code></pre>
<p>To improve model selection, let's pick the model that has the best test performance on ROUGE-2, a metric that is typically associated with higher fluency. We can do this by constructing a function that returns a function that computes the score and we only have to pass it to our trainer.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric

rouge_scorer = load_metric(<span class="hljs-string">"rouge"</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">rouge_metric_builder</span>(<span class="hljs-params">tokenizer</span>):</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_rouge_metrics</span>(<span class="hljs-params">pred</span>):</span>
        <span class="hljs-string">"""Utility to compute ROUGE during training."""</span>
        labels_ids = pred.label_ids
        pred_ids = pred.predictions
        <span class="hljs-comment"># All special tokens are removed.</span>
        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
        labels_ids[labels_ids == -<span class="hljs-number">100</span>] = tokenizer.pad_token_id
        label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
        <span class="hljs-comment"># Compute the metric.</span>
        rouge_results = rouge_scorer.compute(
            predictions=pred_str,
            references=label_str,
            rouge_types=[<span class="hljs-string">"rouge2"</span>, <span class="hljs-string">"rougeL"</span>],
            use_agregator=<span class="hljs-literal">True</span>,
            use_stemmer=<span class="hljs-literal">False</span>,
        )
        <span class="hljs-keyword">return</span> {
            <span class="hljs-string">"rouge2"</span>: <span class="hljs-built_in">round</span>(rouge_results[<span class="hljs-string">'rouge2'</span>].mid.fmeasure, <span class="hljs-number">4</span>),
            <span class="hljs-string">"rougeL"</span>: <span class="hljs-built_in">round</span>(rouge_results[<span class="hljs-string">'rougeL'</span>].mid.fmeasure, <span class="hljs-number">4</span>),
        }
    <span class="hljs-keyword">return</span> compute_rouge_metrics

rouge_metric_fn = rouge_metric_builder(tokenizer)</code></pre>
<p>We load our model and set some parameters for training and generating.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> torch

DEVICE = <span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>
RANDOM_SEED = <span class="hljs-number">42</span>
BEAM_SIZE = <span class="hljs-number">4</span>

model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
model = model.to(DEVICE)</code></pre>
<p>Fantastic, now all we have to do is set up our trainer class with everything we defined so far and run it!</p>
<pre><code class="hljs language-python">train_args = Seq2SeqTrainingArguments(
    output_dir=<span class="hljs-string">"BART-commongen"</span>,
    evaluation_strategy=<span class="hljs-string">"epoch"</span>, 
    save_strategy=<span class="hljs-string">"epoch"</span>,
    logging_steps=<span class="hljs-number">100</span>,
    <span class="hljs-comment"># optimization args, the trainer uses the Adam optimizer</span>
    <span class="hljs-comment"># and has a linear warmup for the learning rate</span>
    per_device_train_batch_size=<span class="hljs-number">32</span>,
    per_device_eval_batch_size=<span class="hljs-number">32</span>,
    gradient_accumulation_steps=<span class="hljs-number">1</span>,
    learning_rate=<span class="hljs-number">1e-04</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    warmup_steps=<span class="hljs-number">1000</span>,
    <span class="hljs-comment"># misc args</span>
    seed=RANDOM_SEED,
    disable_tqdm=<span class="hljs-literal">False</span>,
    load_best_model_at_end=<span class="hljs-literal">True</span>,
    metric_for_best_model=<span class="hljs-string">"rouge2"</span>,
    <span class="hljs-comment"># generation</span>
    predict_with_generate=<span class="hljs-literal">True</span>,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=train_args,
    train_dataset=train_data_tokenized,
    eval_dataset=valid_data_tokenized,
    tokenizer=tokenizer,
    compute_metrics=rouge_metric_fn,
)

trainer._max_length = MAX_LENGTH
trainer._num_beams = BEAM_SIZE</code></pre>
<p>And finally:</p>
<pre><code class="hljs language-python">>> trainer.train()

Epoch	Training Loss	Validation Loss	Rouge2	    Rougel
<span class="hljs-number">1</span>	<span class="hljs-number">0.953500</span>	<span class="hljs-number">1.113132</span>	<span class="hljs-number">0.122500</span>	<span class="hljs-number">0.322200</span>
<span class="hljs-number">2</span>	<span class="hljs-number">0.825300</span>	<span class="hljs-number">1.132310</span>	<span class="hljs-number">0.133800</span>	<span class="hljs-number">0.324600</span>
<span class="hljs-number">3</span>	<span class="hljs-number">0.709400</span>	<span class="hljs-number">1.133418</span>	<span class="hljs-number">0.129300</span>	<span class="hljs-number">0.324700</span></code></pre>
<p>We now have a model that achieves 12.9 ROUGE-2 which can obviously still be tuned, but it is a great starting point.</p>
<h2 id="generating-and-evaluating-predictions">Generating and evaluating predictions</h2>
<p>Given that we now have our fine-tuned model, we can use it to generate outputs for evaluation. For this, let's build another utility function that handles tokenizing, generating with beam search decoding, and de-tokenizing.</p>
<pre><code class="hljs language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">beam_generate_sentences</span>(<span class="hljs-params">
    batch,
    model,
    tokenizer,
    num_beams=<span class="hljs-number">4</span>,
    max_length=<span class="hljs-number">32</span>,
    device=<span class="hljs-string">'cuda:0'</span>
</span>):</span>
    <span class="hljs-string">"""Generate outputs from a model with beam search decoding."""</span>
    <span class="hljs-comment"># Create batch inputs.</span>
    source, _ = construct_input_for_batch(batch)
    <span class="hljs-comment"># Use the model's tokenizer to create the batch input_ids.</span>
    batch_features = tokenizer(source, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">'pt'</span>)
    <span class="hljs-comment"># Move all inputs to the device.</span>
    batch_features = <span class="hljs-built_in">dict</span>([(k, v.to(device)) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch_features.items()])

    <span class="hljs-comment"># Generate with beam search.</span>
    generated_ids = model.generate(
        **batch_features,
        num_beams=num_beams,
        max_length=max_length,
    )

    <span class="hljs-comment"># Use model tokenizer to decode to text.</span>
    generated_sentences = [
        tokenizer.decode(gen_ids.tolist(), skip_special_tokens=<span class="hljs-literal">True</span>)
        <span class="hljs-keyword">for</span> gen_ids <span class="hljs-keyword">in</span> generated_ids
    ]
    <span class="hljs-keyword">return</span> generated_sentences</code></pre>
<p>We can quickly apply this function across our validation set as a sanity check.</p>
<pre><code class="hljs language-python">valid_output = data[<span class="hljs-string">'validation'</span>].<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> batch: {<span class="hljs-string">'generated'</span>: beam_generate_sentences(
        batch,
        model,
        tokenizer,
        num_beams=BEAM_SIZE,
        max_length=MAX_LENGTH,
        device=DEVICE)
    },
    batched=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">128</span>,
)

<span class="hljs-comment"># Evaluate for ROUGE-2/L</span>
rouge_results = rouge_scorer.compute(
    predictions=valid_output[<span class="hljs-string">"generated"</span>],
    references=valid_output[<span class="hljs-string">"target"</span>],
    rouge_types=[<span class="hljs-string">"rouge2"</span>, <span class="hljs-string">"rougeL"</span>],
    use_agregator=<span class="hljs-literal">True</span>, use_stemmer=<span class="hljs-literal">False</span>,
)

<span class="hljs-string">f"R-2: <span class="hljs-subst">{rouge_results[<span class="hljs-string">'rouge2'</span>].mid.fmeasure:<span class="hljs-number">.3</span>f}</span> R-L: <span class="hljs-subst">{rouge_results[<span class="hljs-string">'rougeL'</span>].mid.fmeasure:<span class="hljs-number">.3</span>f}</span>"</span></code></pre>
<p>As expected, this yields the following output:</p>
<pre><code class="hljs language-python"><span class="hljs-string">'R-2: 0.134 R-L: 0.325'</span></code></pre>
<h2 id="generating-and-submitting-test-predictions">Generating and submitting test predictions</h2>
<p>You can submit your model along with test predictions via our <a href="https://forms.gle/vbTZDMCuqzok8tTA9">submission form</a>.</p>
<h3 id="format-description">Format description</h3>
<p>Please follow this format for your submission file:</p>
<pre><code class="hljs language-json">{
  <span class="hljs-attr">"submission_name"</span>: <span class="hljs-string">"An identifying name of your system"</span>,
  <span class="hljs-attr">"param_count"</span>: <span class="hljs-number">123</span>, # the number of parameters your system has.
  <span class="hljs-attr">"description"</span>: <span class="hljs-string">"An optional brief description of the system that will be shown on the website"</span>,
  <span class="hljs-attr">"tasks"</span>:
    {
      <span class="hljs-attr">"dataset_identifier"</span>: {
        <span class="hljs-attr">"values"</span>: [<span class="hljs-string">"output1"</span>, <span class="hljs-string">"output2"</span>, <span class="hljs-string">"..."</span>], # A list of system outputs
        # Optionally, you can add the keys which are part of an example to ensure that there is no shuffling mistakes.
        <span class="hljs-attr">"keys"</span>: [<span class="hljs-string">"schema_guided_dialog-test-9585"</span>, <span class="hljs-string">"schema_guided_dialog-test-9585"</span>, ...]
        }
    }
}
</code></pre>
<p>In this case, <code>dataset_identifier</code> is the identifier of the dataset followed by an identifier of the set the outputs were created from, for example <code>_validation</code> or <code>_test</code>. That means, the common_gen validation set would have the identifier <code>common_gen_validation</code>.</p>
<p>The <code>keys</code> field can be set to avoid accidental shuffling to impact your metrics. Simply add a list of the <code>gem_id</code> for each output example in the same order as your values.</p>
<h3 id="formatting-your-predictions">Formatting Your Predictions</h3>
<p>For our tutorial, let's say we want to include results for the test (<code>common_gen_test</code>) and challenge set (<code>common_gen_challenge_train_sample</code>) outputs.</p>
<pre><code class="hljs language-python">test_output = data[<span class="hljs-string">'test'</span>].<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> batch: {
        <span class="hljs-string">'generated'</span>: beam_generate_sentences(
        batch,
        model,
        tokenizer,
        num_beams=BEAM_SIZE,
        max_length=MAX_LENGTH,
        device=DEVICE)
    },
    batched=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">128</span>,
)

challenge_train_sample_output = data[<span class="hljs-string">"challenge_train_sample"</span>].<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> batch: {
        <span class="hljs-string">'generated'</span>: beam_generate_sentences(
            batch,
            model,
            tokenizer,
            num_beams=BEAM_SIZE,
            max_length=MAX_LENGTH,
            device=DEVICE)
    },
    batched=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">128</span>,
)
</code></pre>
<p>We add a <code>generated</code> field into the dataset which makes analysis much easier. However, in our submission file we only want the actual values and corresponding IDs. Thus, we filter:</p>
<pre><code class="hljs language-python">valid_formatted = [o[<span class="hljs-string">'generated'</span>] <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> valid_output]
valid_keys = [o[<span class="hljs-string">'gem_id'</span>] <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> data[<span class="hljs-string">'validation'</span>]]

test_formatted = [o[<span class="hljs-string">'generated'</span>] <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> test_output]
test_keys = [o[<span class="hljs-string">'gem_id'</span>] <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> data[<span class="hljs-string">'test'</span>]]

challenge_train_sample_formatted = [o[<span class="hljs-string">'generated'</span>] <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> challenge_train_sample_output]
challenge_train_sample_keys = [o[<span class="hljs-string">'gem_id'</span>] <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> data[<span class="hljs-string">'challenge_train_sample'</span>]]</code></pre>
<p>In our final step, we only have to add the outputs to our larger submission construct.</p>
<pre><code class="hljs language-python">SUBMISSION_NAME = <span class="hljs-string">"An identifying name of your system"</span>
DESCRIPTION = <span class="hljs-string">"An optional brief description of the system that will be shown on the website"</span>

submission_dict = {
    <span class="hljs-string">"submission_name"</span>: SUBMISSION_NAME ,
    <span class="hljs-string">"param_count"</span>: <span class="hljs-built_in">sum</span>(p.numel() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters()),
    <span class="hljs-string">"description"</span>: DESCRIPTION,
    <span class="hljs-string">"tasks"</span>: {
      <span class="hljs-string">"common_gen_validation"</span>: {
          <span class="hljs-string">"values"</span>: valid_formatted, 
          <span class="hljs-string">"keys"</span>: valid_keys
          },
      <span class="hljs-string">"common_gen_test"</span>: {
          <span class="hljs-string">"values"</span>: test_formatted, 
          <span class="hljs-string">"keys"</span>: test_keys
          },
      <span class="hljs-string">"common_gen_challenge_train_sample"</span>: {
          <span class="hljs-string">"values"</span>: challenge_train_sample_formatted, 
          <span class="hljs-string">"keys"</span>: challenge_train_sample_keys
          }
    }
}</code></pre>
<p>This format is scalable to more tasks: you simply need to add more outputs to the <code>tasks</code> subfield.
The last step is to write our submission dictionary to a file.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> json
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'gem_submission.json'</span>, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> f:
    f.write(json.dumps(submission_dict))</code></pre>
<h2 id="evaluating-your-submission-file-with-the-gem-evaluation-framework">Evaluating your submission file with the GEM evaluation framework</h2>
<p>Obviously, we do not want to rely only on ROUGE scores. For this, we developed the GEM evaluation framework.</p>
<p>You can download it by running:</p>
<pre><code class="hljs language-bash">git <span class="hljs-built_in">clone</span> git@github.com:GEM-benchmark/GEM-metrics.git</code></pre>
<p>Install the required packages:</p>
<pre><code class="hljs language-bash"><span class="hljs-built_in">cd</span> GEM-metrics
pip install -r requirements.txt</code></pre>
<p>Assuming that you formatted and saved your outputs correctly, you can now run</p>
<pre><code class="hljs language-bash">python run_metrics.py [-r references.json] [-o outputs.scores.json] gem_submission.json</code></pre>
<p>which will create a json file with your scores per task and challenge set. Please follow the <a href="https://github.com/GEM-benchmark/GEM-metrics">README</a> for more detailed usage information.</p>
</div></article></main><div class="layout_push__1J9g0"></div></div><footer class="layout_footer__127N0 utils_eggshell__Njxsh"><span class="layout_backToHome__1vZsp"><a href="/">← Home</a></span><span>If you have any questions, please join our <a href="https://groups.google.com/g/gem-benchmark" target="_blank" class="utils_accentUnderline__k083p">google group</a> for support.</span></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"taskData":{"contentHtml":"\u003cp\u003eThis tutorial presents a full walk-through on how to get started with GEM, how to load and inspect data, how to finetune a baseline model, and how to generate predictions.\nThroughout this tutorial, we will focus on the CommonGen task, but we will note\nwhat changes to make to use another of the \u003ca href=\"/data_cards\"\u003eGEM datasets\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eYou can also run this tutorial as a notebook \u003ca href=\"https://colab.research.google.com/drive/1iREkGABObpdluTBNAnLhvyABEtdbLokT?usp=sharing\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSUBMITTING\u003c/strong\u003e Our \u003ca href=\"https://forms.gle/vbTZDMCuqzok8tTA9\"\u003esubmission form\u003c/a\u003e is permanently open! Please account for some extra time to write your model card.\u003c/p\u003e\n\u003ch2 id=\"table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#preliminaries\"\u003ePreliminaries\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#loading-the-data\"\u003eLoading the data\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#loading-a-single-example\"\u003eLoading a single example\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#finetuning-a-pretrained-model\"\u003eFinetuning a pretrained model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#generating-and-evaluating-predictions\"\u003eGenerating and evaluating predictions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#generating-and-submitting-test-predictions\"\u003eGenerating and submitting test predictions\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#format-description\"\u003eFormat description\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#formatting-your-predictions\"\u003eFormatting Your Predictions\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#evaluating-your-submission-file-with-the-gem-evaluation-framework\"\u003eEvaluating your submission file with the GEM evaluation framework\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"preliminaries\"\u003ePreliminaries\u003c/h2\u003e\n\u003cp\u003eThis tutorial uses PyTorch and the HuggingFace infrastructure to finetune models.\nYou need to install the following dependencies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003epip install git+https://github.com/huggingface/datasets.git\npip install rouge_score\npip install sentencepiece\npip install transformers\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe recommend you use a GPU machine. You should be able to run all the code inside of a \u003ca href=\"https://colab.research.google.com/\"\u003ecolab notebook for free GPU access\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"loading-the-data\"\u003eLoading the data\u003c/h2\u003e\n\u003cp\u003eWe will be using \u003ca href=\"https://huggingface.co/datasets/gem\"\u003eHuggingFace datasets\u003c/a\u003e, but the GEM datasets are available in \u003ca href=\"https://www.tensorflow.org/datasets\"\u003eTFDS\u003c/a\u003e as well.\u003c/p\u003e\n\u003cp\u003eYou can load and inspect datasets like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003e\u003e \u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_dataset\n\u003e\u003e DATASET_NAME = \u003cspan class=\"hljs-string\"\u003e\"common_gen\"\u003c/span\u003e\n\u003e\u003e data = load_dataset(\u003cspan class=\"hljs-string\"\u003e\"gem\"\u003c/span\u003e, DATASET_NAME)\n\u003e\u003e data\n\nDatasetDict({\n    train: Dataset({\n        features: [\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'gem_parent_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concept_set_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concepts'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'references'\u003c/span\u003e],\n        num_rows: \u003cspan class=\"hljs-number\"\u003e67389\u003c/span\u003e\n    })\n    validation: Dataset({\n        features: [\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'gem_parent_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concept_set_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concepts'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'references'\u003c/span\u003e],\n        num_rows: \u003cspan class=\"hljs-number\"\u003e993\u003c/span\u003e\n    })\n    test: Dataset({\n        features: [\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'gem_parent_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concept_set_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concepts'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'references'\u003c/span\u003e],\n        num_rows: \u003cspan class=\"hljs-number\"\u003e1497\u003c/span\u003e\n    })\n    challenge_train_sample: Dataset({\n        features: [\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'gem_parent_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concept_set_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concepts'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'references'\u003c/span\u003e],\n        num_rows: \u003cspan class=\"hljs-number\"\u003e500\u003c/span\u003e\n    })\n    challenge_validation_sample: Dataset({\n        features: [\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'gem_parent_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concept_set_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concepts'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'references'\u003c/span\u003e],\n        num_rows: \u003cspan class=\"hljs-number\"\u003e500\u003c/span\u003e\n    })\n    challenge_test_scramble: Dataset({\n        features: [\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'gem_parent_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concept_set_id'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'concepts'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'references'\u003c/span\u003e],\n        num_rows: \u003cspan class=\"hljs-number\"\u003e500\u003c/span\u003e\n    })\n})\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can notice that challenge sets created as part of GEM act just like any other data split, which means that you can use them with exactly the same code!\u003c/p\u003e\n\u003cp\u003eGEM supports many other datasets, simply pick one from this list and check out the corresponding \u003ca href=\"/data_cards\"\u003edata cards\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"loading-a-single-example\"\u003eLoading a single example\u003c/h3\u003e\n\u003cp\u003eNow let's look at a single example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003e\u003e data[\u003cspan class=\"hljs-string\"\u003e'train'\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n\n{\n    \u003cspan class=\"hljs-string\"\u003e'concept_set_id'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,\n    \u003cspan class=\"hljs-string\"\u003e'concepts'\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'mountain'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'ski'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'skier'\u003c/span\u003e],\n    \u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'common_gen-train-0'\u003c/span\u003e,\n    \u003cspan class=\"hljs-string\"\u003e'references'\u003c/span\u003e: [],\n    \u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'Skier skis down the mountain'\u003c/span\u003e\n}\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCommonGen is a task that asks for the production of a sentence (\u003ccode\u003etarget\u003c/code\u003e) from a set of concepts (\u003ccode\u003econcepts\u003c/code\u003e). Since one concept set can generate multiple meaningful sentences, the example also includes a unique identifier (\u003ccode\u003econcept_set_id\u003c/code\u003e) so that multiple references can be linked to an input.\u003c/p\u003e\n\u003cp\u003eNext, let's define utility functions that can generate batches of (tokenized) examples which we can use during training.\u003c/p\u003e\n\u003cp\u003eWe create a function that takes a batch from a dataset and constructs the corresponding input string. In our CommonGen example, we concatenate concepts into a single string for each instance.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003econstruct_input_for_batch\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ebatch\u003c/span\u003e):\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"\"\"Construct input strings from a batch.\"\"\"\u003c/span\u003e\n    source = [\u003cspan class=\"hljs-string\"\u003e' '\u003c/span\u003e.join(concepts) \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e concepts \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e batch[\u003cspan class=\"hljs-string\"\u003e\"concepts\"\u003c/span\u003e]]\n    target = batch[\u003cspan class=\"hljs-string\"\u003e\"target\"\u003c/span\u003e]\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e source, target\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe then create a function that tokenizes the batches. Depending on your task, you might want to consider adjusting the \u003ccode\u003emax_length\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003ebatch_tokenize\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ebatch, tokenizer, max_length=\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e\u003c/span\u003e):\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"\"\"Construct the batch (source, target) and run them through a tokenizer.\"\"\"\u003c/span\u003e\n    source, target = construct_input_for_batch(batch)\n    res = {\n        \u003cspan class=\"hljs-string\"\u003e\"input_ids\"\u003c/span\u003e: tokenizer(source)[\u003cspan class=\"hljs-string\"\u003e\"input_ids\"\u003c/span\u003e],\n        \u003cspan class=\"hljs-string\"\u003e\"labels\"\u003c/span\u003e: tokenizer(\n            target,\n            padding=\u003cspan class=\"hljs-string\"\u003e\"max_length\"\u003c/span\u003e,\n            truncation=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n            max_length=max_length\n        )[\u003cspan class=\"hljs-string\"\u003e\"input_ids\"\u003c/span\u003e],\n    }\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e res\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAll we need to do now to preprocess the dataset is to call \u003ccode\u003ebatch_tokenize\u003c/code\u003e on it. For our example, we are using BART-base as a model and we need to load the corresponding tokenizer.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e AutoTokenizer\n\nMODEL_NAME = \u003cspan class=\"hljs-string\"\u003e\"facebook/bart-base\"\u003c/span\u003e\nMAX_LENGTH = \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ntrain_data_tokenized = data[\u003cspan class=\"hljs-string\"\u003e'train'\u003c/span\u003e].\u003cspan class=\"hljs-built_in\"\u003emap\u003c/span\u003e(\n    \u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e batch: batch_tokenize(batch, tokenizer, max_length=MAX_LENGTH),\n    batched=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e\n)\nvalid_data_tokenized = data[\u003cspan class=\"hljs-string\"\u003e'validation'\u003c/span\u003e].\u003cspan class=\"hljs-built_in\"\u003emap\u003c/span\u003e(\n    \u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e batch: batch_tokenize(batch, tokenizer, max_length=MAX_LENGTH),\n    batched=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e\n)\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"finetuning-a-pretrained-model\"\u003eFinetuning a pretrained model\u003c/h2\u003e\n\u003cp\u003eWe can now utilize the preprocessed data to finetune a model. To do so, we will use the \u003ca href=\"https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainingarguments\"\u003eTrainer API\u003c/a\u003e which handles gradient updates, model selection, and evaluation for us.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo improve model selection, let's pick the model that has the best test performance on ROUGE-2, a metric that is typically associated with higher fluency. We can do this by constructing a function that returns a function that computes the score and we only have to pass it to our trainer.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_metric\n\nrouge_scorer = load_metric(\u003cspan class=\"hljs-string\"\u003e\"rouge\"\u003c/span\u003e)\n\n\u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003erouge_metric_builder\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003etokenizer\u003c/span\u003e):\u003c/span\u003e\n    \u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003ecompute_rouge_metrics\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003epred\u003c/span\u003e):\u003c/span\u003e\n        \u003cspan class=\"hljs-string\"\u003e\"\"\"Utility to compute ROUGE during training.\"\"\"\u003c/span\u003e\n        labels_ids = pred.label_ids\n        pred_ids = pred.predictions\n        \u003cspan class=\"hljs-comment\"\u003e# All special tokens are removed.\u003c/span\u003e\n        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n        labels_ids[labels_ids == -\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e] = tokenizer.pad_token_id\n        label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n        \u003cspan class=\"hljs-comment\"\u003e# Compute the metric.\u003c/span\u003e\n        rouge_results = rouge_scorer.compute(\n            predictions=pred_str,\n            references=label_str,\n            rouge_types=[\u003cspan class=\"hljs-string\"\u003e\"rouge2\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"rougeL\"\u003c/span\u003e],\n            use_agregator=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n            use_stemmer=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e,\n        )\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e {\n            \u003cspan class=\"hljs-string\"\u003e\"rouge2\"\u003c/span\u003e: \u003cspan class=\"hljs-built_in\"\u003eround\u003c/span\u003e(rouge_results[\u003cspan class=\"hljs-string\"\u003e'rouge2'\u003c/span\u003e].mid.fmeasure, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e),\n            \u003cspan class=\"hljs-string\"\u003e\"rougeL\"\u003c/span\u003e: \u003cspan class=\"hljs-built_in\"\u003eround\u003c/span\u003e(rouge_results[\u003cspan class=\"hljs-string\"\u003e'rougeL'\u003c/span\u003e].mid.fmeasure, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e),\n        }\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e compute_rouge_metrics\n\nrouge_metric_fn = rouge_metric_builder(tokenizer)\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe load our model and set some parameters for training and generating.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch\n\nDEVICE = \u003cspan class=\"hljs-string\"\u003e\"cuda:0\"\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e torch.cuda.is_available() \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"cpu\"\u003c/span\u003e\nRANDOM_SEED = \u003cspan class=\"hljs-number\"\u003e42\u003c/span\u003e\nBEAM_SIZE = \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\nmodel = model.to(DEVICE)\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFantastic, now all we have to do is set up our trainer class with everything we defined so far and run it!\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003etrain_args = Seq2SeqTrainingArguments(\n    output_dir=\u003cspan class=\"hljs-string\"\u003e\"BART-commongen\"\u003c/span\u003e,\n    evaluation_strategy=\u003cspan class=\"hljs-string\"\u003e\"epoch\"\u003c/span\u003e, \n    save_strategy=\u003cspan class=\"hljs-string\"\u003e\"epoch\"\u003c/span\u003e,\n    logging_steps=\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e,\n    \u003cspan class=\"hljs-comment\"\u003e# optimization args, the trainer uses the Adam optimizer\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e# and has a linear warmup for the learning rate\u003c/span\u003e\n    per_device_train_batch_size=\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e,\n    per_device_eval_batch_size=\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e,\n    gradient_accumulation_steps=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\n    learning_rate=\u003cspan class=\"hljs-number\"\u003e1e-04\u003c/span\u003e,\n    num_train_epochs=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e,\n    warmup_steps=\u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e,\n    \u003cspan class=\"hljs-comment\"\u003e# misc args\u003c/span\u003e\n    seed=RANDOM_SEED,\n    disable_tqdm=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e,\n    load_best_model_at_end=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n    metric_for_best_model=\u003cspan class=\"hljs-string\"\u003e\"rouge2\"\u003c/span\u003e,\n    \u003cspan class=\"hljs-comment\"\u003e# generation\u003c/span\u003e\n    predict_with_generate=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=train_args,\n    train_dataset=train_data_tokenized,\n    eval_dataset=valid_data_tokenized,\n    tokenizer=tokenizer,\n    compute_metrics=rouge_metric_fn,\n)\n\ntrainer._max_length = MAX_LENGTH\ntrainer._num_beams = BEAM_SIZE\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd finally:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003e\u003e trainer.train()\n\nEpoch\tTraining Loss\tValidation Loss\tRouge2\t    Rougel\n\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.953500\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e1.113132\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.122500\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.322200\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.825300\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e1.132310\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.133800\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.324600\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.709400\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e1.133418\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.129300\u003c/span\u003e\t\u003cspan class=\"hljs-number\"\u003e0.324700\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe now have a model that achieves 12.9 ROUGE-2 which can obviously still be tuned, but it is a great starting point.\u003c/p\u003e\n\u003ch2 id=\"generating-and-evaluating-predictions\"\u003eGenerating and evaluating predictions\u003c/h2\u003e\n\u003cp\u003eGiven that we now have our fine-tuned model, we can use it to generate outputs for evaluation. For this, let's build another utility function that handles tokenizing, generating with beam search decoding, and de-tokenizing.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003ebeam_generate_sentences\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\n    batch,\n    model,\n    tokenizer,\n    num_beams=\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e,\n    max_length=\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e,\n    device=\u003cspan class=\"hljs-string\"\u003e'cuda:0'\u003c/span\u003e\n\u003c/span\u003e):\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"\"\"Generate outputs from a model with beam search decoding.\"\"\"\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e# Create batch inputs.\u003c/span\u003e\n    source, _ = construct_input_for_batch(batch)\n    \u003cspan class=\"hljs-comment\"\u003e# Use the model's tokenizer to create the batch input_ids.\u003c/span\u003e\n    batch_features = tokenizer(source, padding=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e, return_tensors=\u003cspan class=\"hljs-string\"\u003e'pt'\u003c/span\u003e)\n    \u003cspan class=\"hljs-comment\"\u003e# Move all inputs to the device.\u003c/span\u003e\n    batch_features = \u003cspan class=\"hljs-built_in\"\u003edict\u003c/span\u003e([(k, v.to(device)) \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e k, v \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e batch_features.items()])\n\n    \u003cspan class=\"hljs-comment\"\u003e# Generate with beam search.\u003c/span\u003e\n    generated_ids = model.generate(\n        **batch_features,\n        num_beams=num_beams,\n        max_length=max_length,\n    )\n\n    \u003cspan class=\"hljs-comment\"\u003e# Use model tokenizer to decode to text.\u003c/span\u003e\n    generated_sentences = [\n        tokenizer.decode(gen_ids.tolist(), skip_special_tokens=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e gen_ids \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e generated_ids\n    ]\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e generated_sentences\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe can quickly apply this function across our validation set as a sanity check.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003evalid_output = data[\u003cspan class=\"hljs-string\"\u003e'validation'\u003c/span\u003e].\u003cspan class=\"hljs-built_in\"\u003emap\u003c/span\u003e(\n    \u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e batch: {\u003cspan class=\"hljs-string\"\u003e'generated'\u003c/span\u003e: beam_generate_sentences(\n        batch,\n        model,\n        tokenizer,\n        num_beams=BEAM_SIZE,\n        max_length=MAX_LENGTH,\n        device=DEVICE)\n    },\n    batched=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n    batch_size=\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e,\n)\n\n\u003cspan class=\"hljs-comment\"\u003e# Evaluate for ROUGE-2/L\u003c/span\u003e\nrouge_results = rouge_scorer.compute(\n    predictions=valid_output[\u003cspan class=\"hljs-string\"\u003e\"generated\"\u003c/span\u003e],\n    references=valid_output[\u003cspan class=\"hljs-string\"\u003e\"target\"\u003c/span\u003e],\n    rouge_types=[\u003cspan class=\"hljs-string\"\u003e\"rouge2\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"rougeL\"\u003c/span\u003e],\n    use_agregator=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e, use_stemmer=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e,\n)\n\n\u003cspan class=\"hljs-string\"\u003ef\"R-2: \u003cspan class=\"hljs-subst\"\u003e{rouge_results[\u003cspan class=\"hljs-string\"\u003e'rouge2'\u003c/span\u003e].mid.fmeasure:\u003cspan class=\"hljs-number\"\u003e.3\u003c/span\u003ef}\u003c/span\u003e R-L: \u003cspan class=\"hljs-subst\"\u003e{rouge_results[\u003cspan class=\"hljs-string\"\u003e'rougeL'\u003c/span\u003e].mid.fmeasure:\u003cspan class=\"hljs-number\"\u003e.3\u003c/span\u003ef}\u003c/span\u003e\"\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAs expected, this yields the following output:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-string\"\u003e'R-2: 0.134 R-L: 0.325'\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"generating-and-submitting-test-predictions\"\u003eGenerating and submitting test predictions\u003c/h2\u003e\n\u003cp\u003eYou can submit your model along with test predictions via our \u003ca href=\"https://forms.gle/vbTZDMCuqzok8tTA9\"\u003esubmission form\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"format-description\"\u003eFormat description\u003c/h3\u003e\n\u003cp\u003ePlease follow this format for your submission file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-json\"\u003e{\n  \u003cspan class=\"hljs-attr\"\u003e\"submission_name\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"An identifying name of your system\"\u003c/span\u003e,\n  \u003cspan class=\"hljs-attr\"\u003e\"param_count\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e123\u003c/span\u003e, # the number of parameters your system has.\n  \u003cspan class=\"hljs-attr\"\u003e\"description\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"An optional brief description of the system that will be shown on the website\"\u003c/span\u003e,\n  \u003cspan class=\"hljs-attr\"\u003e\"tasks\"\u003c/span\u003e:\n    {\n      \u003cspan class=\"hljs-attr\"\u003e\"dataset_identifier\"\u003c/span\u003e: {\n        \u003cspan class=\"hljs-attr\"\u003e\"values\"\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e\"output1\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"output2\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"...\"\u003c/span\u003e], # A list of system outputs\n        # Optionally, you can add the keys which are part of an example to ensure that there is no shuffling mistakes.\n        \u003cspan class=\"hljs-attr\"\u003e\"keys\"\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e\"schema_guided_dialog-test-9585\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"schema_guided_dialog-test-9585\"\u003c/span\u003e, ...]\n        }\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn this case, \u003ccode\u003edataset_identifier\u003c/code\u003e is the identifier of the dataset followed by an identifier of the set the outputs were created from, for example \u003ccode\u003e_validation\u003c/code\u003e or \u003ccode\u003e_test\u003c/code\u003e. That means, the common_gen validation set would have the identifier \u003ccode\u003ecommon_gen_validation\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003ekeys\u003c/code\u003e field can be set to avoid accidental shuffling to impact your metrics. Simply add a list of the \u003ccode\u003egem_id\u003c/code\u003e for each output example in the same order as your values.\u003c/p\u003e\n\u003ch3 id=\"formatting-your-predictions\"\u003eFormatting Your Predictions\u003c/h3\u003e\n\u003cp\u003eFor our tutorial, let's say we want to include results for the test (\u003ccode\u003ecommon_gen_test\u003c/code\u003e) and challenge set (\u003ccode\u003ecommon_gen_challenge_train_sample\u003c/code\u003e) outputs.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003etest_output = data[\u003cspan class=\"hljs-string\"\u003e'test'\u003c/span\u003e].\u003cspan class=\"hljs-built_in\"\u003emap\u003c/span\u003e(\n    \u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e batch: {\n        \u003cspan class=\"hljs-string\"\u003e'generated'\u003c/span\u003e: beam_generate_sentences(\n        batch,\n        model,\n        tokenizer,\n        num_beams=BEAM_SIZE,\n        max_length=MAX_LENGTH,\n        device=DEVICE)\n    },\n    batched=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n    batch_size=\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e,\n)\n\nchallenge_train_sample_output = data[\u003cspan class=\"hljs-string\"\u003e\"challenge_train_sample\"\u003c/span\u003e].\u003cspan class=\"hljs-built_in\"\u003emap\u003c/span\u003e(\n    \u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e batch: {\n        \u003cspan class=\"hljs-string\"\u003e'generated'\u003c/span\u003e: beam_generate_sentences(\n            batch,\n            model,\n            tokenizer,\n            num_beams=BEAM_SIZE,\n            max_length=MAX_LENGTH,\n            device=DEVICE)\n    },\n    batched=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n    batch_size=\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e,\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe add a \u003ccode\u003egenerated\u003c/code\u003e field into the dataset which makes analysis much easier. However, in our submission file we only want the actual values and corresponding IDs. Thus, we filter:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003evalid_formatted = [o[\u003cspan class=\"hljs-string\"\u003e'generated'\u003c/span\u003e] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e o \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e valid_output]\nvalid_keys = [o[\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e o \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e data[\u003cspan class=\"hljs-string\"\u003e'validation'\u003c/span\u003e]]\n\ntest_formatted = [o[\u003cspan class=\"hljs-string\"\u003e'generated'\u003c/span\u003e] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e o \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e test_output]\ntest_keys = [o[\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e o \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e data[\u003cspan class=\"hljs-string\"\u003e'test'\u003c/span\u003e]]\n\nchallenge_train_sample_formatted = [o[\u003cspan class=\"hljs-string\"\u003e'generated'\u003c/span\u003e] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e o \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e challenge_train_sample_output]\nchallenge_train_sample_keys = [o[\u003cspan class=\"hljs-string\"\u003e'gem_id'\u003c/span\u003e] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e o \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e data[\u003cspan class=\"hljs-string\"\u003e'challenge_train_sample'\u003c/span\u003e]]\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn our final step, we only have to add the outputs to our larger submission construct.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003eSUBMISSION_NAME = \u003cspan class=\"hljs-string\"\u003e\"An identifying name of your system\"\u003c/span\u003e\nDESCRIPTION = \u003cspan class=\"hljs-string\"\u003e\"An optional brief description of the system that will be shown on the website\"\u003c/span\u003e\n\nsubmission_dict = {\n    \u003cspan class=\"hljs-string\"\u003e\"submission_name\"\u003c/span\u003e: SUBMISSION_NAME ,\n    \u003cspan class=\"hljs-string\"\u003e\"param_count\"\u003c/span\u003e: \u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(p.numel() \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e p \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e model.parameters()),\n    \u003cspan class=\"hljs-string\"\u003e\"description\"\u003c/span\u003e: DESCRIPTION,\n    \u003cspan class=\"hljs-string\"\u003e\"tasks\"\u003c/span\u003e: {\n      \u003cspan class=\"hljs-string\"\u003e\"common_gen_validation\"\u003c/span\u003e: {\n          \u003cspan class=\"hljs-string\"\u003e\"values\"\u003c/span\u003e: valid_formatted, \n          \u003cspan class=\"hljs-string\"\u003e\"keys\"\u003c/span\u003e: valid_keys\n          },\n      \u003cspan class=\"hljs-string\"\u003e\"common_gen_test\"\u003c/span\u003e: {\n          \u003cspan class=\"hljs-string\"\u003e\"values\"\u003c/span\u003e: test_formatted, \n          \u003cspan class=\"hljs-string\"\u003e\"keys\"\u003c/span\u003e: test_keys\n          },\n      \u003cspan class=\"hljs-string\"\u003e\"common_gen_challenge_train_sample\"\u003c/span\u003e: {\n          \u003cspan class=\"hljs-string\"\u003e\"values\"\u003c/span\u003e: challenge_train_sample_formatted, \n          \u003cspan class=\"hljs-string\"\u003e\"keys\"\u003c/span\u003e: challenge_train_sample_keys\n          }\n    }\n}\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis format is scalable to more tasks: you simply need to add more outputs to the \u003ccode\u003etasks\u003c/code\u003e subfield.\nThe last step is to write our submission dictionary to a file.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e json\n\u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eopen\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'gem_submission.json'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'w'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e f:\n    f.write(json.dumps(submission_dict))\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"evaluating-your-submission-file-with-the-gem-evaluation-framework\"\u003eEvaluating your submission file with the GEM evaluation framework\u003c/h2\u003e\n\u003cp\u003eObviously, we do not want to rely only on ROUGE scores. For this, we developed the GEM evaluation framework.\u003c/p\u003e\n\u003cp\u003eYou can download it by running:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003egit \u003cspan class=\"hljs-built_in\"\u003eclone\u003c/span\u003e git@github.com:GEM-benchmark/GEM-metrics.git\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eInstall the required packages:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-built_in\"\u003ecd\u003c/span\u003e GEM-metrics\npip install -r requirements.txt\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAssuming that you formatted and saved your outputs correctly, you can now run\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003epython run_metrics.py [-r references.json] [-o outputs.scores.json] gem_submission.json\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhich will create a json file with your scores per task and challenge set. Please follow the \u003ca href=\"https://github.com/GEM-benchmark/GEM-metrics\"\u003eREADME\u003c/a\u003e for more detailed usage information.\u003c/p\u003e\n","title":"From pretrained model to submission","type":"Modeling","background":"This tutorial shows the entire pipeline from loading the data, creating a model, to formatting the submission file from predictions."}},"__N_SSG":true},"page":"/tutorials/[id]","query":{"id":"modeling"},"buildId":"Nx8QlaATgB-argoYvzlf6","nextExport":false,"isFallback":false,"gsp":true,"head":[["meta",{"name":"viewport","content":"width=device-width"}],["meta",{"charSet":"utf-8"}],["link",{"rel":"icon","href":"/favicon.ico"}],["meta",{"name":"description","content":"Benchmark natural language generation systems with GEM."}],["meta",{"property":"og:image","content":"https://og-image.now.sh/**GEM**%20Benchmark.png?theme=light\u0026md=1\u0026fontSize=100px\u0026images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fvercel-triangle-black.svg"}],["meta",{"name":"og:title","content":"GEM"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["title",{"children":"GEM From pretrained model to submission"}]]}</script><script nomodule="" src="/_next/static/chunks/polyfills-e69cc13a7e89296a69e4.js"></script><script src="/_next/static/chunks/main-47bc8f80085b54a800da.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.baa41d4dbf5d52db897c.js" async=""></script><script src="/_next/static/chunks/e70fad557dfa42f32a11d0d2c99fe8f6e8d1fa86.4a36a385313236c59b19.js" async=""></script><script src="/_next/static/chunks/pages/_app-a9ae7a6d1de4e51a7ab6.js" async=""></script><script src="/_next/static/chunks/cb1608f2.c3a9f0eb95374ca4919a.js" async=""></script><script src="/_next/static/chunks/451c6be158cef50d8cc28b919cf08d1e5b9ff3fc.f0ec181e43727e8a893e.js" async=""></script><script src="/_next/static/chunks/pages/tutorials/%5Bid%5D-c1ef01944be3b5e528b1.js" async=""></script><script src="/_next/static/Nx8QlaATgB-argoYvzlf6/_buildManifest.js" async=""></script><script src="/_next/static/Nx8QlaATgB-argoYvzlf6/_ssgManifest.js" async=""></script></body></html>